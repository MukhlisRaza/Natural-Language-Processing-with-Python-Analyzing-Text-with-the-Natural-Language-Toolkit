{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Chapter 3\n",
    "\n",
    "## Processing Raw Text\n",
    "\n",
    "*The html version of this chapter in the NLTK book is available [here](https://www.nltk.org/book/ch03.html#exercises \"Ch03 Exercises\").*\n",
    "\n",
    "### 8   Exercises\n",
    "\n",
    "###### 1. \n",
    "\n",
    "☼ Define a string `s = 'colorless'`. Write a Python statement that changes this to \"colourless\" using only the slice and concatenation operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'colourless'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'colorless'\n",
    "s = s[:4] + 'u' + s[4:]\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.\n",
    "\n",
    "☼ We can use the slice notation to remove morphological endings on words. For example, `'dogs'[:-1]` removes the last character of `dogs`, leaving `dog`. Use slice notation to remove the affixes from these words (we've inserted a hyphen to indicate the affix boundary, but omit this from your strings): `dish-es`, `run-ning`, `nation-ality`, `un-do`, `pre-heat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dish', 'run', 'nation', 'un', 'pre']\n"
     ]
    }
   ],
   "source": [
    "affixed = [('dishes', 2), \n",
    "           ('running', 4),\n",
    "           ('nationality', 5),\n",
    "           ('undo', 2),\n",
    "           ('preheat', 4)]\n",
    "\n",
    "print([s[:-a] for s, a in affixed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.\n",
    "\n",
    "☼ We saw how we can generate an `IndexError` by indexing beyond the end of a string. Is it possible to construct an index that goes too far to the left, before the start of the string?\n",
    "\n",
    "*Yes.  I'm not going to run the code in my notebook, because then the cells below this one wouldn't run.*\n",
    "\n",
    "```\n",
    ">>>trial = \"trial\"\n",
    ">>>for i in range(1, len(trial) + 2):\n",
    ">>>    print(trial[-i])\n",
    "    \n",
    "l\n",
    "a\n",
    "i\n",
    "r\n",
    "t\n",
    "---------------------------------------------------------------------------\n",
    "IndexError                                Traceback (most recent call last)\n",
    "<ipython-input-21-98077138b076> in <module>\n",
    "      1 trial = \"trial\"\n",
    "      2 for i in range(1, len(trial) + 2):\n",
    "----> 3     print(trial[-i])\n",
    "\n",
    "IndexError: string index out of range\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. \n",
    "\n",
    "☼ We can specify a \"step\" size for the slice. The following returns every second character within the slice: `monty[6:11:2]`. It also works in the reverse direction: `monty[10:5:-2]` Try these for yourself, then experiment with different step values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tittitie tírýhkeee řltl řstittitie tírýhsřc.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A Czech tongue twister:\n",
    "tt = \"Třistatřiatřicet stříbrných křepelek přeletělo přes třistatřiatřicet stříbrných střech.\"\n",
    "\n",
    "# Every other letter\n",
    "tt[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.cřshýrít eitittitsř ltlř eeekhýrít eitittiT'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every other letter from the end\n",
    "tt[::-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tstaittbý el etoř iaiřesínhtc'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every third letter\n",
    "tt[::3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You get the point...*\n",
    "\n",
    "##### 5. \n",
    "\n",
    "☼ What happens if you ask the interpreter to evaluate `monty[::-1]`? Explain why this is a reasonable result.\n",
    "\n",
    "*It prints the word backwards.  It's simply printing from the end by steps of -1:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'murder'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"redrum\"[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.\n",
    "\n",
    "☼ Describe the class of strings matched by the following regular expressions.\n",
    "\n",
    "a. `[a-zA-Z]+`\n",
    "\n",
    "b. `[A-Z][a-z]*`\n",
    "\n",
    "c. `p[aeiou]{,2}t`\n",
    "\n",
    "d. `\\d+(\\.\\d+)?`\n",
    "\n",
    "e. `([^aeiou][aeiou][^aeiou])*`\n",
    "\n",
    "f. `\\w+|[^\\w\\s]+`\n",
    "\n",
    "Test your answers using `nltk.re_show()`.\n",
    "\n",
    "*__a.__ `[a-zA-Z]+` will match anything alphabetical:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{cAMELCASE} 6186258313 {hybr}1{d}\n"
     ]
    }
   ],
   "source": [
    "import nltk, re\n",
    "\n",
    "nltk.re_show(r'[a-zA-Z]+', \"cAMELCASE 6186258313 hybr1d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>__b.__ `[A-Z][a-z]*` will match words beginning with uppercase letters, or any uppercase letters in other positions:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{I} think words beginning with {Uppercase} {Letters} will be matched, or any uppercase letters found in o{T}{H}{E}{R} positions.\n"
     ]
    }
   ],
   "source": [
    "test = 'I think words beginning with Uppercase Letters will be matched, ' \\\n",
    "       'or any uppercase letters found in oTHER positions.'\n",
    "\n",
    "nltk.re_show(r'[A-Z][a-z]*', test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__c.__ `p[aeiou]{,2}t` will match all words with __p__, up to two vowels, and a letter __t__.  This is a lot of words: In the wordlist we've been using in this chapter, this RegExp will return nearly 7,000 words, since any word with __pt__ will be a match.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6978"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist = [w.lower() for w in nltk.corpus.words.words('en')]\n",
    "len([w for w in wordlist if re.search(r'p[aeiou]{,2}t', w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abaptiston', 'abepithymia', 'ableptical', 'ableptically', 'abrupt', 'abruptedly', 'abruption', 'abruptly', 'abruptness', 'absorpt', 'absorptance', 'absorptiometer', 'absorptiometric', 'absorption', 'absorptive', 'absorptively', 'absorptiveness', 'absorptivity', 'absumption', 'acalypterae']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in wordlist if re.search(r'p[aeiou]{,2}t', w)][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If we add the `^` and <code>\\$</code> operators, we'll instead end up with all 3-letter words beginning and ending with __p__ and __t__ with one vowel in the middle, or all 4-letter words beginning and ending with __p__ and __t__ with two vowels in the middle:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pat', 'pat', 'paut', 'peat', 'pet', 'piet', 'piet', 'pit', 'poet', 'poot', 'pot', 'pout', 'put']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in wordlist if re.search(r'^p[aeiou]{,2}t$', w)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__d.__ `\\d+(\\.\\d+)?` will match any numbers and decimal points, no matter how many numbers are to the left/right of the decimal.  It will not match dashes, dollar signs, or any other symbol associated with number.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1234}\n",
      "{12.34}\n",
      "example {123.4} in a string\n",
      "{1}-{234}\n",
      "{12},{4}\n",
      "${12.34}\n"
     ]
    }
   ],
   "source": [
    "test = ['1234', '12.34', 'example 123.4 in a string', '1-234', '12,4', '$12.34']\n",
    "for t in test:\n",
    "    nltk.re_show(r'\\d+(\\.\\d+)?', t) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If we use two decimals, the second is ignored:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.23}.{4}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'\\d+(\\.\\d+)?', '1.23.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can alter that by changing the `?` to a `+`:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.23.4}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'\\d+(\\.\\d+)+', '1.23.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>__e.__ `([^aeiou][aeiou][^aeiou])*` will match any non-vowel\\vowel\\non-vowel combination, no matter how many times it's repeated.  White spaces are considered non-vowels, so a string such as `to ` would match.  `nltk.re_show()` behaves quite strangely with this RegExp - a string like `\"baab\"` would return `{}b{}a{}a{}b{}`.  However, I have evaluated this RegExp with online evaluators (such as [this one](https://regexr.com/ \"regexr.com\"), and there the responsive is as expected:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{babbabbabbab}{}a{pap}{}a{}\n"
     ]
    }
   ],
   "source": [
    "string = \"babbabbab\" \\\n",
    "         \"babapapa\"\n",
    "nltk.re_show(r'([^aeiou][aeiou][^aeiou])*', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}b{}a{}a{}b{}\n"
     ]
    }
   ],
   "source": [
    "string = \"baab\"\n",
    "nltk.re_show(r'([^aeiou][aeiou][^aeiou])*', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__f.__ `\\w+|[^\\w\\s]+` will match either any alphanumeric string of any length, or a string of any length that does not contain alphanumeric characters or whitespace - i.e., all punctuation and any other non-whitespace/non-alphanumeric characters:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{This} {RegExp} {needs} {a} {fairly} {long} {string} {to} {show} {what} {it} {can} {%#$^%&*} {do}{.}\n"
     ]
    }
   ],
   "source": [
    "string = \"This RegExp needs a fairly long string to show what it can %#$^%&* do.\"\n",
    "nltk.re_show(r'\\w+|[^\\w\\s]+', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.\n",
    "\n",
    "*☼ Write regular expressions to match the following classes of strings:*\n",
    "\n",
    " + *__a.__ A single determiner (assume that __a__, __an__, and __the__ are the only determiners).*\n",
    " + <i>__b.__ An arithmetic expression using integers, addition, and multiplication, such as `2*3+8`.</i>\n",
    " \n",
    "*__a.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think {a} relevant string like {the} one here is {an} example of what we need.\n"
     ]
    }
   ],
   "source": [
    "string = \"I think a relevant string like the one here is an example of what we need.\"\n",
    "nltk.re_show(r'\\b[Aa]n?\\b|\\b[Tt]he\\b', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__b.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2 * 3 + 8}\n"
     ]
    }
   ],
   "source": [
    "string = \"2 * 3 + 8\"\n",
    "nltk.re_show(r'(\\d|[+*= ])+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{11 + 4 * 2}\n"
     ]
    }
   ],
   "source": [
    "string = \"11 + 4 * 2\"\n",
    "nltk.re_show(r'(\\d|[+*= ])+', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.\n",
    "\n",
    "\n",
    "☼ Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML markup removed. Use `from urllib import request`  and then `request.urlopen('http://nltk.org/').read().decode('utf8')` to access the contents of the URL.\n",
    "\n",
    "*This chapter of the NLTK book dealt with removing HTML tags, but didn't really touch on removing the style & scripts tags that are present in most pages today. [This Stack Overflow discussion](https://stackoverflow.com/questions/30565404/remove-all-style-scripts-and-html-tags-from-an-html-page#answers \"Removing Style, Scripts, and HTML tags\") has a good discussion on how to use BeautifulSoup to do that, specifically with the `extract` and `stripped_strings` methods.  The code below borrows heavily from [this answer in the above Stack Overflow discussion](https://stackoverflow.com/a/30565597 \"Removing Style, Scripts, and HTML tags - answer\"):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "from unicodedata import normalize\n",
    "\n",
    "def return_URL_contents(url):\n",
    "    html = request.urlopen(url).read().decode('utf8')\n",
    "    raw = BeautifulSoup(html, 'html.parser')\n",
    "    for r in raw(['script', 'style']):\n",
    "        r.extract() # remove tags\n",
    "    \n",
    "    text = ' '.join(raw.stripped_strings) # retrieve tag content\n",
    "    \n",
    "    return normalize('NFKD', text) # normalize escape sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What Virtual Reality Can Teach a Driverless Car - The New York Times Sections SEARCH Skip to content Skip to site index Business Log In Log In Today’s Paper Business | What Virtual Reality Can Teach a Driverless Car Subscribe Log In Advertisement Supported by What Virtual Reality Can Teach a Driverless Car By Cade Metz Oct. 29, 2017 SAN FRANCISCO — As the computers that operate driverless cars digest the rules of the road, some engineers think it might be nice if they can learn from mistakes made in virtual reality rather than on real streets. Companies like Toyota, Uber and Waymo have discussed at length how they are testing autonomous vehicles on the streets of Mountain View, Calif., Phoenix and other cities. What is not as well known is that they are also testing vehicles inside computer simulations of these same cities. Virtual cars, equipped with the same software as the real thing, spend thousands of hours driving their digital worlds. Think of it as a way of identifying flaws in the way the cars operate without endangering real people. If a car makes a mistake on a simulated drive, engineers can tweak its software accordingly, laying down new rules of behavior. On Monday, Waymo, the autonomous car company that spun out of Google, is expected to show off its simulator tests when it takes a group of reporters to its secretive testing center in California’s Central Valley. Researchers are also developing methods that would allow cars to actually learn new behavior from these simulations, gathering skills more quickly than human engineers could ever lay them down with explicit software code. “Simulation is a tremendous thing,” said Gill Pratt, chief executive of the Toyota Research Institute, one of the artificial intelligence labs exploring this kind of virtual training for autonomous vehicles and other robotics. Image In virtual reality simulations, driverless cars operated by Waymo can practice driving the same intersection, in the same driving conditions, tho'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.nytimes.com/2017/10/29/business/virtual-reality-driverless-cars.html?module=inline\"\n",
    "\n",
    "return_URL_contents(url)[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Guido van Rossum - Wikipedia Guido van Rossum From Wikipedia, the free encyclopedia Jump to navigation Jump to search Dutch programmer and creator of Python In this Dutch name , the family name is van Rossum , not Rossum . Guido van Rossum Guido van Rossum at the Dropbox headquarters in 2014 Born ( 1956-01-31 ) 31 January 1956 (age 63) [1] Haarlem , Netherlands [2] [3] Residence Belmont, California , U.S. Nationality Dutch Alma mater University of Amsterdam Occupation Computer programmer, author Employer Dropbox [4] Known for Creating the Python programming language Spouse(s) Kim Knapp ( m. 2000) Children 1 [5] Awards Award for the Advancement of Free Software (2001) Website gvanrossum .github .io Guido van Rossum ( Dutch: [ˈɣido vɑn ˈrɔsʏm, -səm] ; born 31 January 1956) is a Dutch programmer best known as the author of the Python programming language , for which he was the \" Benevolent dictator for life \" (BDFL) until he stepped down from the position in July 2018. [6] [7] He is currently a member of the Python Steering Council. [8] Contents 1 Life and education 2 Work 2.1 Python 2.2 Computer Programming for Everybody 2.3 Mondrian 2.4 Dropbox 3 Awards 4 References 5 External links Life and education [ edit ] Van Rossum was born and raised in the Netherlands , where he received a master\\'s degree in mathematics and computer science from the University of Amsterdam in 1982. He has a brother, Just van Rossum, who is a type designer and programmer who designed the typeface used in the \"Python Powered\" logo. [ citation needed ] Guido lives in Belmont , California, with his wife, Kim Knapp, [9] and their son. [10] [11] [12] According to his home page and Dutch naming conventions , the \" van \" in his name is capitalized when he is referred to by surname alone, but not when using his first and last name together. [13] Work [ edit ] While working at the Centrum Wiskunde & Informatica (CWI), Van Rossum wrote and contributed a glob() routine to BSD Unix in 1986 [14] [15] and h'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Guido_van_Rossum\"\n",
    "\n",
    "return_URL_contents(url)[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. \n",
    "\n",
    "☼ Save some text into a file `corpus.txt`. Define a function `load(f)` that reads from the file named in its sole argument, and returns a string containing the text of the file.\n",
    "\n",
    " + a. Use `nltk.regexp_tokenize()` to create a tokenizer that tokenizes the various kinds of punctuation in this text. Use one multi-line regular expression, with inline comments, using the verbose flag `(?x)`.\n",
    " \n",
    " + b. Use `nltk.regexp_tokenize()` to create a tokenizer that tokenizes the following kinds of expression: monetary amounts; dates; names of people and organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\chapter03\"\n",
    "\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.nytimes.com/2017/10/22/technology/artificial-intelligence-experts-salaries.html?action=click&module=RelatedCoverage&pgtype=Article&region=Footer'\n",
    "\n",
    "text = return_URL_contents(url)\n",
    "\n",
    "with open('corpus.txt', 'w', encoding = \"utf-8\") as f:\n",
    "    f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(f):\n",
    "    text = open(f, encoding = \"utf-8\")\n",
    "    raw = text.read()\n",
    "    \n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt = load('corpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__a.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '.', '.', '.', '.', '.', ',', '.', '.', ',', ':', '.', '.', '.', ',', '.', ',', '.', '.', '.', ',', '.', '.', ',', ',', ',', ',', '.', '.', '.', '.', '.', ',', '.', '.', '.', '.', ',', ',', ',', ',', '.', ',', ',', '.', ',', '.', '.', '.', '.', ',', ',', '.', '.', '.', '.', '.', '.', ',', '.', ',', ',', '.', '.', '.', '.', ',', ',', ',', ',', '.', ',', ',', ',', ',', '.', '.', '.', '.', ',', ',', ',', '.', ',', ',', '.', ',', '.', ',', ',', ',', '.', '.', '.', ',', ',', '.', ',', '.', ',', ',', '.', ',', '.', ',', ',', ',', '.', '.', '.', ',', '.', ',', '.', '.', '.', '.', ',', '.', '.', '.', ',', '.', ',', ',', '.', '.', '(', ',', ',', ')', '.', ',', '.', ',', ',', '.', '.', ',', '.', '.', '.', ',', '.', '.', '.', ',', ',', '.', ',', '.', '.', ',', '.', ',', '.', '.', ',', '.', ',', ',', ',', ',', '.', '.', '.', ',', ',', '.', '.', ',', '.', ',', '.', ',', '.', '.', ',', ',', '.', '.', ',', '.', '.', '.', ',', ',', ':', '.', '.', '.', '.', '.', '.', '.', '.', \"'\", \"'\", ':', \"'\", ':', '.', '.', '.', '.', '.', \"'\", \"'\", ':', \"'\", ':', '.', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'''(?x)\n",
    "    [][.,;\"'?!():_-`] # finds punctuation\n",
    "'''\n",
    "\n",
    "print(nltk.regexp_tokenize(nyt, pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__b.__ Using regular expressions to extract information such as proper names - which can take numerous forms - is wrought with problems, and the regular expressions below are far from perfect.  It could very well be that the point of this exercise was to demonstrate just how difficult this approach is.*\n",
    "\n",
    "*Proper names are almost always capitalized; but so are many other words, and this approach alone is practically guaranteed to generate false positives/negatives.  I tried to limit the number of false positives by only examining sequences of two or more strings that began with uppercase letters; but that would eliminate any companies with a one-word name, as well as any references to a person using only his/her first/last name.  Another issue is that this returns strings where multiple words are in uppercase, such as titles.*\n",
    "\n",
    "*Monetary values are more straightforward, but I noticed that large amounts are often written out, so I created a special use case for this.  I didn't bother with all the different currency symbols for commonly-used currencies and instead made do with using the dollar sign.*\n",
    "\n",
    "*Finally, dates have a variety of formats around the world, but for the purpose of this exercise I only focussed on those formats in common use in the U.S., i.e. numerical dates (__10/16/19__ or __10/16/2019__) or literal dates (__Oct. 16, 2019__ or __October 16, 2019__.)*\n",
    "\n",
    "*While researching this question, I came across several methods that looked interesting, but which I did not pursue because of time limitations.  One was finding first names in a text by using the `names` corpus in NLTK.  It's easy to see how this would work; but I didn't explore this, because it seems the focus of this unit is regular expressions.  An additional issue would be the lack of corpora for last and company names.  Another method that looked interesting was using Python's `datetime` module to deal with dates.  But, as was the case above, time limitations prevented me from trying this.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tech Giants Are Paying Huge Salaries', 'Scarce A.', 'I. Talent', 'The New York Times Technology', 'Tech Giants Are Paying Huge Salaries', 'Scarce A.', 'I. Talent Subscribe Log In Image Credit Credit Christina Chung Sections Skip', 'Tech Giants Are Paying Huge Salaries', 'Scarce A.', 'I. Talent Nearly', 'Credit Credit Christina Chung Supported', 'By Cade Metz Oct', '22', '2017', 'Silicon Valley', 'I. Tech', 'Typical A.', '$300,000', '$500,000', 'Anthony Levandowski', '2007', '$120 million', 'Image Luke Zettlemoyer', 'Allen Institute', 'Artificial Intelligence', 'Credit Kyle Johnson', 'The New York Times Salaries', 'National Football League', 'Christopher Fernandez', 'Silicon Valley', '10,000', 'Andrew Moore', 'Carnegie Mellon University', '$650 million', '2014', '50', '400', '$138 million', '$345,000', 'Jessica Cataneo', '1950', '2013', 'Amazon Echo', '40', 'Carnegie Mellon', '2015', 'Stanford University', '20', 'Oren Etzioni', 'Allen Institute', 'Artificial Intelligence', 'Luke Zettlemoyer', '$180,000', 'Allen Institute', 'Google Brain', 'United States', 'Eastern Europe', 'Chris Nicholson', 'San Francisco', 'United States', 'Yoshua Bengio', 'Follow Cade Metz', '1', 'New York', 'L. Salaries', 'I. Talent', 'Order Reprints', 'Subscribe Advertisement Site Index Go', 'Home Page', '2020', 'New York', 'Pop Culture', 'Reader Center Wirecutter Live Events The Learning Network', '2020', 'New York', 'Pop Culture', 'Reader Center Wirecutter Live Events The Learning Network', 'Crossword Cooking', 'Site Information Navigation', '2019', 'The New York Times Company Contact Us Work', 'Brand Studio Your Ad Choices Privacy Terms', 'Service Terms', 'Sale Site Map Help Subscriptions']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'''(?x)\n",
    "          \n",
    "          (?:[A-Z])(?:[a-z]+|\\.)(?:\\s+[A-Z](?:[a-z]+|\\.))*(?:\\s+[A-Z])(?:[a-z]+|\\.)\n",
    "                                         # proper names\n",
    "          | \\$\\d+\\s\\b[tr|b|m]illion\\b    # literal monetary amounts\n",
    "          | \\$?\\d+(?:[,\\.]\\d+)?          # numerical monetary amounts\n",
    "          | \\d{2}\\[\\\\]\\d{2}\\-\\\\]\\d{4}    # numerical dates (U.S. format)\n",
    "          | [A-Z][a-z.]*\\s\\d{2}\\,\\s\\d{2, 4} # literal dates (U.S. format)\n",
    "\n",
    "          \n",
    "        '''\n",
    "\n",
    "print(nltk.regexp_tokenize(nyt, pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  10.\n",
    "\n",
    "☼ Rewrite the following loop as a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n"
     ]
    }
   ],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "result = []\n",
    "for word in sent:\n",
    "    word_len = (word, len(word))\n",
    "    result.append(word_len)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n"
     ]
    }
   ],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "result = [(word, len(word)) for word in sent]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11.\n",
    "\n",
    "☼ Define a string `raw` containing a sentence of your own choosing. Now, split `raw` on some character other than space, such as '`s`'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How much ', ' would a ', 'chuck chuck if a ', 'chuck could chuck ', '?']"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\"\n",
    "raw.split('wood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12.\n",
    "\n",
    "☼ Write a `for` loop to print out the characters of a string, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "o\n",
      "m\n",
      "p\n",
      "a\n",
      "r\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "s\n",
      "o\n",
      "m\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n"
     ]
    }
   ],
   "source": [
    "string = \"Compared to some of the previous exercises, this seems comically easy.\"\n",
    "\n",
    "for s in string[:20]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 13.\n",
    "\n",
    "☼ What is the difference between calling `split` on a string with no argument or with `' '` as the argument, e.g. `sent.split()` versus `sent.split(' ')`? What happens when the string being split contains tab characters, consecutive space characters, or a sequence of tabs and spaces? \n",
    "\n",
    "*`sent.split()` splits all whitespace identically.*\n",
    "\n",
    "*`sent.split(' ')` splits all whitespace literally.  I.e., tabs will be represented as `\\t`, and each individaul whitespace will be spilt into its own string.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With `sent.split()`:\n",
      "['This', 'string', 'is', 'a', 'pretty', 'simple', 'string.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This', 'string', 'is', 'a', 'pretty', 'simple', 'string.']\n",
      "\n",
      "With `sent.split()`:\n",
      "['This', 'strings', 'has', 'tabs.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This\\tstrings\\thas\\ttabs.']\n",
      "\n",
      "With `sent.split()`:\n",
      "['This', 'string', 'has', 'lots', 'of', 'space.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This', '', '', '', '', '', '', '', 'string', '', '', '', '', '', '', '', '', '', 'has', '', '', '', '', '', 'lots', '', '', '', 'of', '', '', '', '', 'space.']\n",
      "\n",
      "With `sent.split()`:\n",
      "['This', 'string', 'has', 'tabs', 'and', 'spaces.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This\\tstring', '', '', '', '', '', '', '', '', 'has\\ttabs', '', '', '', '', '', '', 'and\\tspaces.']\n"
     ]
    }
   ],
   "source": [
    "s1 = \"This string is a pretty simple string.\"\n",
    "s2 = \"This\\tstrings\\thas\\ttabs.\"\n",
    "s3 = \"This        string          has      lots    of     space.\"\n",
    "s4 = \"This\\tstring         has\\ttabs       and\\tspaces.\"\n",
    "\n",
    "Ss = [s1, s2, s3, s4]\n",
    "\n",
    "for s in Ss:\n",
    "    print(\"\\nWith `sent.split()`:\")\n",
    "    print(s.split())\n",
    "    print(\"\\nWith `sent.split(' ')`:\")\n",
    "    print(s.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. \n",
    "\n",
    "☼ Create a variable `words` containing a list of words. Experiment with `words.sort()` and `sorted(words)`. What is the difference?\n",
    "\n",
    "*`words.sort()` doesn't return a value, but it alters the ordering of the list, so that whenever I call the list again, the returned list will be the ordered one, and not the one I originally stored.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wörter', 'focail', 'mots', 'ord', 'ord', 'palabras', 'palavras', 'parole', 'sanat', 'slova', 'szavak', 'słowa', 'từ ngữ', 'woorden', 'words', 'words', 'λόγια', 'ווערטער']\n"
     ]
    }
   ],
   "source": [
    "words = [\"slova\", \"ord\", \"Wörter\", \"λόγια\", \"words\", \"palabras\", \"sanat\", \n",
    "         \"mots\", \"focail\", \"szavak\", \"parole\", \"words\", \"woorden\", \"ord\", \n",
    "         \"słowa\", \"palavras\", \"từ ngữ\", \"ווערטער\"]\n",
    "\n",
    "words.sort()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wörter', 'focail', 'mots', 'ord', 'ord', 'palabras', 'palavras', 'parole', 'sanat', 'slova', 'szavak', 'słowa', 'từ ngữ', 'woorden', 'words', 'words', 'λόγια', 'ווערטער']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`sorted(words)` will return a result, but the ordering of the original list will not be changed:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wörter', 'focail', 'mots', 'ord', 'ord', 'palabras', 'palavras', 'parole', 'sanat', 'slova', 'szavak', 'słowa', 'từ ngữ', 'woorden', 'words', 'words', 'λόγια', 'ווערטער']\n"
     ]
    }
   ],
   "source": [
    "words = [\"slova\", \"ord\", \"Wörter\", \"λόγια\", \"words\", \"palabras\", \"sanat\", \n",
    "         \"mots\", \"focail\", \"szavak\", \"parole\", \"words\", \"woorden\", \"ord\", \n",
    "         \"słowa\", \"palavras\", \"từ ngữ\", \"ווערטער\"]\n",
    "\n",
    "print(sorted(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['slova', 'ord', 'Wörter', 'λόγια', 'words', 'palabras', 'sanat', 'mots', 'focail', 'szavak', 'parole', 'words', 'woorden', 'ord', 'słowa', 'palavras', 'từ ngữ', 'ווערטער']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. \n",
    "\n",
    "☼ Explore the difference between strings and integers by typing the following at a Python prompt: `\"3\" * 7` and `3 * 7`. Try converting between strings and integers using `int(\"3\")` and `str(3)`.\n",
    "\n",
    "*Multiplying a string $x$ by an integer $y$ will just cause $x$ to be printed to the console $y$ times:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3333333'"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"3\" * 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>`3 * 7` will just give us the product:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 * 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can convert strings to integers with `int()` and vice-versa with `str()`:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(\"3\") * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3333333'"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(3) * 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16. \n",
    "\n",
    "☼ Use a text editor to create a file called `prog.py` containing the single line `monty = 'Monty Python'`. Next, start up a new session with the Python interpreter, and enter the expression `monty` at the prompt. You will get an error from the interpreter. Now, try the following (note that you have to leave off the `.py` part of the filename):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Errors in my notebook prevent the remaining cells from running, so this is being saved as markdown:*\n",
    "\n",
    "```\n",
    "monty\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "NameError                                 Traceback (most recent call last)\n",
    "<ipython-input-308-d4cc90107335> in <module>\n",
    "----> 1 monty\n",
    "\n",
    "NameError: name 'monty' is not defined\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python'"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prog import monty\n",
    "monty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This time, Python should return with a value. You can also try `import prog`, in which case Python should be able to evaluate the expression `prog.monty` at the prompt.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python'"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import prog\n",
    "\n",
    "prog.monty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17. \n",
    "\n",
    "☼ What happens when the formatting strings `%6s` and `%-6s` are used to display strings that are longer than six characters?\n",
    "\n",
    "*This looks to be a legacy question from an older version of the book, since this is the older method of formatting in Python.  As the question is written, `%6s` won't do anything to a longer string:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'another test'"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"another test\"\n",
    "\n",
    "\"%6s\" % (test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`%-6s` will add padding to a string shorter than six characters:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey   '"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"%-6s\" % (\"hey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I suspect the authors may have left out a decimal.  `%.6s` has a quite different effect:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anothe'"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"%.6s\" % (test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. \n",
    "\n",
    "◑ Read in some text from a corpus, tokenize it, and print the list of all *wh*-word types that occur. (*wh*-words in English are used in questions, relative clauses and exclamations: *who*, *which*, *what*, and so on.) Print them in order. Are any words duplicated in this list, because of the presence of case distinctions or punctuation?\n",
    "\n",
    "*This question is a little difficult to follow.  Most of the corpora we're using have texts that have already been tokenized, so the first part of this question seems a bit redundant.  However, just to play along,  I'll use the raw text version of one of the Project Gutenberg texts.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "raw = gutenberg.raw('bryant-stories.txt')\n",
    "\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "tokens = sorted(set(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The next part of the question also seems a little hard to follow: The __wh-__words are a closed set (__what__, __when__, __where__, __which__, __who__, __whose__, and __why__), and if I explicitly define them, I won't find any exceptions.  The only possible way around this is to search for all words that begin with __wh__.  But if I do so, most of the hits will be false positives, because most words that start with __wh-__ are outside of this set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Whale', 'What', 'When', 'Whenever', 'Where', 'Whether', 'Whiff', 'While', 'Whirling', 'White', 'Who', 'Whose', 'Why', 'what', 'whatever', 'wheat', 'wheelbarrow', 'wheeled', 'when', 'whence', 'whenever', 'where', 'wherein', 'wherever', 'whether', 'which', 'while', 'whimpering', 'whin', 'whinny', 'whipped', 'whirlpool', 'whiruled', 'whisk', 'whisked', 'whisper', 'whisper_', 'whispered', 'whispering', 'whispers', 'whistle', 'whistled', 'white', 'white-haired', 'white-robed', 'whither', 'who', 'whole', 'wholly', 'whom', 'whose', 'why']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in tokens if re.search('^[Ww]h', w)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As expected, most of the results are false positives.  There are also  versions of the __wh-__ words starting with both upper- and lowercase letters; as well as 'whom', the accusative form of 'who'.  But there are also a number of words that could be considered __wh-__ words that I wouldn't have thought of searching for, such as words ending with '-ever' (e.g., 'whatever', 'whenever', etc...); variant forms of 'where' (i.e., 'whence' and 'whither'), as well as 'whether' and 'wherein'.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. \n",
    "\n",
    "◑ Create a file consisting of words and (made up) frequencies, where each line consists of a word, the space character, and a positive integer, e.g. `fuzzy 53`. Read the file into a Python list using `open(filename).readlines()`. Next, break each line into its two fields using `split()`, and convert the number into an integer using `int()`. The result should be a list of the form: `[['fuzzy', 53], ...]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy = open('fuzzy.txt', encoding = \"utf-8\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fuzzy', 53],\n",
       " ['wuzzy', 92],\n",
       " ['was', 128],\n",
       " ['a', 4897],\n",
       " ['bear', 23],\n",
       " ['had', 47],\n",
       " ['no', 93],\n",
       " ['hair', 23],\n",
       " [\"wasn't\", 78],\n",
       " ['he', 77]]"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[word, int(value)] for word, value in (f.split() for f in fuzzy)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20. \n",
    "\n",
    "◑ Write code to access a favorite webpage and extract some text from it. For example, access a weather site and extract the forecast top temperature for your town or city today.\n",
    "\n",
    "*This is not so straightforward.  Most websites today do not have a static link to weather values, and it's almost certainly easier to use an API.  [OpenWeather](https://openweathermap.org/api \"OpenWeather\") offers such an API, though it requires registration, which takes a few hours to process.  The results will be in JSON, which needs to be parsed, and the temperature will be in Kelvin, which will need to be converted:*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# needs to be deleted before posting to GH\n",
    "key = '10f1049e5862ee388ad748981accb4c1'\n",
    "\n",
    "def k_to_c(temp):\n",
    "    \"\"\"Converts Kelvin to Celsius.\"\"\"\n",
    "    return temp - 273.15\n",
    "\n",
    "def k_to_f(temp):\n",
    "    \"\"\"Converts Kelvin to Fahrenheit.\"\"\"\n",
    "    return (temp - 273.15) * 1.8 + 32\n",
    "\n",
    "def get_temp(city):\n",
    "    \"\"\"\n",
    "    Gets current, high, and low temperatures for a given city.\n",
    "    \"\"\"\n",
    "    r = requests.get('http://api.openweathermap.org/data/2.5/weather?q=' + city + '&APPID=' + key)\n",
    "    \n",
    "    if r.json()['cod'] == '404':\n",
    "        print(\"Sorry, we don't know where that city is.\")\n",
    "    else:\n",
    "        current_k = r.json()['main']['temp']\n",
    "        min_k = r.json()['main']['temp_min']\n",
    "        max_k = r.json()['main']['temp_max']\n",
    "        print(\"The current temperature is {:.1f} C°/{:.1f} F°.\".format(k_to_c(current_k), k_to_f(current_k)))\n",
    "        print(\"Today's high temperature is {:.1f} C°/{:.1f} F°.\".format(k_to_c(max_k), k_to_f(max_k)))\n",
    "        print(\"Today's low temperature is {:.1f} C°/{:.1f} F°.\".format(k_to_c(min_k), k_to_f(min_k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature is 17.8 C°/64.0 F°.\n",
      "Today's high temperature is 20.6 C°/69.0 F°.\n",
      "Today's low temperature is 14.0 C°/57.2 F°.\n"
     ]
    }
   ],
   "source": [
    "get_temp('Hiroshima')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature is 18.7 C°/65.6 F°.\n",
      "Today's high temperature is 20.6 C°/69.0 F°.\n",
      "Today's low temperature is 16.0 C°/60.8 F°.\n"
     ]
    }
   ],
   "source": [
    "get_temp('Kure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, we don't know where that city is.\n"
     ]
    }
   ],
   "source": [
    "get_temp('aaaa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21.\n",
    "\n",
    "◑ Write a function `unknown()` that takes a URL as its argument, and returns a list of unknown words that occur on that webpage. In order to do this, extract all substrings consisting of lowercase letters (using `re.findall()`) and remove any items from this set that occur in the Words Corpus (`nltk.corpus.words`). Try to categorize these words manually and discuss your findings.\n",
    "\n",
    "*If we literally do what the instructions tell us, we'll get quite a large list, as inflected word forms (-ing, -ed, -s, etc...) are mostly absent from the Words Corpus.  So I've added parameters to `unknown` that will let us exclude these common words.  This may lead to a small number of false negatives; but I feel this is an acceptable cost, given the very high number of false positives.  It might be advisable to run the function twice, once without the endings excluded and once with, so that the user can inspect for him-/herself which words are being excluded.*\n",
    "\n",
    "*Additionally, irregular verbs are absent in the wordlist, so I added these manually.*\n",
    "\n",
    "*After adjusting which words are excluded, we'll see that most of the words are fairly new words ('podcast', 'app', ...) which haven't had a chance to be added to the corpus.  We also see compound words ('counterproductive', 'rollout', ...) whose constituent parts are part of the corpus.  We will also see the occasional lower-case last name: it's common for authors to post their Twitter handles next to their byline, and these handles are usually lowercase.  Upper-case names were pruned in the function.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = [w.lower() for w in nltk.corpus.words.words('en')]\n",
    "\n",
    "# irregular verbs\n",
    "verbs = ['ate', 'beat', 'beaten', 'became', 'become', 'began', 'begun', 'bent', \n",
    "         'bet', 'bid', 'bit', 'bitten', 'blew', 'blown', 'bought', 'broke', 'broken', \n",
    "         'brought', 'built', 'burnt', 'came', 'caught', 'chose', 'chosen', 'come', \n",
    "         'cost', 'cut', 'did', 'dived', 'done', 'dove', 'drank', 'drawn', 'dreamt', \n",
    "         'drew', 'driven', 'drove', 'drunk', 'dug', 'eaten', 'fallen', 'fell', 'felt', \n",
    "         'flew', 'flown', 'forgave', 'forgiven', 'forgot', 'forgotten', 'fought', 'found', \n",
    "         'froze', 'frozen', 'gave', 'given', 'gone', 'got', 'gotten', 'grew', 'grown', \n",
    "         'had', 'heard', 'held', 'hid', 'hidden', 'hit', 'hung', 'hurt', 'kept', 'knew', \n",
    "         'known', 'laid', 'lain', 'lay', 'led', 'left', 'lent', 'let', 'lost', 'made', \n",
    "         'meant', 'met', 'paid', 'put', 'ran', 'rang', 'read', 'ridden', 'risen', 'rode', \n",
    "         'rose', 'run', 'rung', 'said', 'sang', 'sat', 'saw', 'seen', 'sent', 'showed', \n",
    "         'shown', 'shut', 'slept', 'sold', 'spent', 'spoke', 'spoken', 'stood', 'sung', \n",
    "         'swam', 'swum', 'taken', 'taught', 'thought', 'threw', 'thrown', 'told', 'took', \n",
    "         'tore', 'torn', 'understood', 'went', 'woke', 'woken', 'won', 'wore', 'worn', \n",
    "         'written', 'wrote']\n",
    "\n",
    "wordlist += verbs\n",
    "\n",
    "\n",
    "\n",
    "def unknown(url, es = False, s = False, ed = False, ing = False, n = False, er = False):\n",
    "    \n",
    "    # get text\n",
    "    raw = return_URL_contents(url)\n",
    "    \n",
    "    # get lower-case words\n",
    "    raw_lower = re.findall(r'\\b[a-z]+\\b', raw)\n",
    "    \n",
    "    # find unknown words and eliminate duplicates\n",
    "    unknown = sorted(set([w for w in raw_lower if w not in wordlist]))\n",
    "    \n",
    "    # find common words that are not in wordlist because of \n",
    "    # morphological changes\n",
    "    exclude = []\n",
    "    \n",
    "    # words with -es plurals\n",
    "    if es:\n",
    "        es = [i for i in unknown if i[-2:] == 'es' and i[:-2] in wordlist] \n",
    "        exclude += es\n",
    "        # -y becomes -ies\n",
    "        ies = [i for i in unknown if i[-3:] == 'ies' and i[:-3] + 'y' in wordlist]\n",
    "        exclude += ies\n",
    "        \n",
    "    # regular plurals\n",
    "    if s:\n",
    "        s = [i for i in unknown if i[-1] == 's' and i[:-1] in wordlist]\n",
    "        exclude += s\n",
    "        \n",
    "    # regular past tense forms\n",
    "    if ed:\n",
    "        # verbs with final -e\n",
    "        d = [i for i in unknown if i[-1:] == 'd' and i[:-1] in wordlist]\n",
    "        exclude += d\n",
    "        # regular verbs\n",
    "        ed = [i for i in unknown if i[-2:] == 'ed' and i[:-2] in wordlist]\n",
    "        exclude += ed\n",
    "        # verbs that double final consonant\n",
    "        dd = [i for i in unknown if i[-2:] == 'ed' and i[:-3] in wordlist]\n",
    "        exclude += dd\n",
    "        \n",
    "    # regular gerunds\n",
    "    if ing:\n",
    "        # verbs with final -e\n",
    "        ng = [i for i in unknown if i[-3:] == 'ing' and i[:-3] + 'e' in wordlist]\n",
    "        exclude += ng\n",
    "        # regular verbs\n",
    "        ing = [i for i in unknown if i[-3:] == 'ing' and i[:-3] in wordlist]\n",
    "        exclude += ing\n",
    "        # verbs that double final consonat\n",
    "        nng = [i for i in unknown if i[-3:] == 'ing' and i[:-4] in wordlist]\n",
    "        exclude += nng\n",
    "        \n",
    "    if n:\n",
    "        # negative contractions without final -'t\n",
    "        n = [i for i in unknown if i[-1:] == 'n' and i[:-1] in wordlist]\n",
    "        exclude += n\n",
    "        \n",
    "    if er:\n",
    "        # comparative forms\n",
    "        er = [i for i in unknown if i[-2:] == 'er' and i[:-2] in wordlist]\n",
    "        exclude += er\n",
    "        # comparative forms with final -y\n",
    "        ier = [i for i in unknown if i[-3:] == 'ier' and i[:-3] + 'y' in wordlist]\n",
    "        exclude += ier\n",
    "        # comparative forms with final -e\n",
    "        r = [i for i in unknown if i[-2:] == 'er' and i[:-1] in wordlist]\n",
    "        exclude += r\n",
    "        # superlative forms\n",
    "        est = [i for i in unknown if i[-3:] == 'est' and i[:-3] in wordlist]\n",
    "        exclude += est\n",
    "        # superlative forms with final -y\n",
    "        st = [i for i in unknown if i[-3:] == 'est' and i[:-2] in wordlist]\n",
    "        exclude += st\n",
    "        # superlative forms with final -e\n",
    "        iest = [i for i in unknown if i[-4:] == 'iest' and i[:-4] + 'y' in wordlist]\n",
    "        exclude += iest\n",
    "        \n",
    "    # return only those unknown words that have not been excluded\n",
    "    # by the above list comprehensions\n",
    "    return [i for i in unknown if i not in exclude]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actions', 'agencies', 'answers', 'app', 'attacking', 'attempts', 'banned', 'biohacker', 'breaks', 'bringing', 'businesses', 'called', 'candidates', 'changing', 'companies', 'compares', 'comparing', 'competitors', 'couldn', 'criticized', 'debated', 'details', 'diagnosing', 'didn', 'discussed', 'doesn', 'doors', 'employees', 'enemies', 'enjoyed', 'executives', 'explained', 'explodes', 'explores', 'frontrunner', 'fundraising', 'giants', 'harshest', 'has', 'helped', 'hosted', 'ignoring', 'including', 'indicated', 'infused', 'joined', 'laws', 'lists', 'lives', 'mailing', 'mainstream', 'members', 'millennials', 'minutes', 'missed', 'monopolies', 'numbers', 'offered', 'okay', 'options', 'playing', 'podcast', 'politicians', 'practices', 'pressed', 'proposals', 'pushed', 'represents', 'required', 'rights', 'rules', 'sectors', 'sees', 'sharing', 'shifted', 'showcased', 'signing', 'simmering', 'specifics', 'stories', 'streamers', 'techlash', 'teddyschleifer', 'themes', 'things', 'topics', 'tougher', 'users', 'using', 'wants', 'wasn', 'whacks', 'years']\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.vox.com/recode/2019/10/16/20916712/cnn-democratic-presidential-debate-big-tech-silicon-valley-warren-harris'\n",
    "\n",
    "print(unknown(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['app', 'biohacker', 'frontrunner', 'fundraising', 'mainstream', 'okay', 'podcast', 'techlash', 'teddyschleifer']\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.vox.com/recode/2019/10/16/20916712/cnn-democratic-presidential-debate-big-tech-silicon-valley-warren-harris'\n",
    "\n",
    "print(unknown(url, es = True, s = True, ed = True, ing = True, n = True, er = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abandoning', 'acquiescing', 'adding', 'advocates', 'aligned', 'angels', 'announced', 'appearances', 'applications', 'areas', 'arts', 'asserted', 'attacking', 'attempted', 'automobiles', 'berated', 'books', 'briefed', 'captured', 'citing', 'closest', 'closing', 'columnists', 'comments', 'communists', 'compared', 'complaining', 'concerns', 'condemns', 'continues', 'contributed', 'controlled', 'corrections', 'counterproductive', 'countries', 'created', 'criticized', 'dated', 'decades', 'defended', 'defends', 'denied', 'denounced', 'deployed', 'described', 'detained', 'deterring', 'dismissed', 'dismissing', 'earlier', 'edited', 'editorials', 'elected', 'email', 'emerged', 'emphasized', 'events', 'expressing', 'fears', 'feels', 'fighters', 'focusing', 'followed', 'forces', 'friends', 'gained', 'hands', 'has', 'having', 'heated', 'horses', 'hours', 'implying', 'including', 'insisted', 'interests', 'interjected', 'interpreted', 'invading', 'isn', 'issues', 'jeopardizing', 'jobs', 'journeys', 'launched', 'leaders', 'lebanon', 'letters', 'listings', 'looks', 'managing', 'matters', 'meltdown', 'mercenaries', 'minimized', 'mishandled', 'misquoted', 'movies', 'multimedia', 'near', 'newsletters', 'obituaries', 'obtained', 'officials', 'ol', 'op', 'opened', 'operations', 'opponents', 'others', 'outposts', 'overrated', 'parenting', 'parties', 'penalties', 'permitting', 'peterbakernyt', 'points', 'policies', 'positions', 'preparing', 'presidents', 'prisoners', 'promoting', 'proposed', 'pulled', 'pulling', 'pushed', 'rebuked', 'rebutted', 'reemerge', 'referring', 'relayed', 'remarks', 'reported', 'reporters', 'reporting', 'reports', 'rollout', 'scratched', 'seemed', 'served', 'services', 'sharpest', 'shouldn', 'soldiers', 'statements', 'states', 'stronger', 'subscriptions', 'suggested', 'talks', 'things', 'ties', 'tools', 'trips', 'troops', 'visited', 'walked', 'wants', 'wars', 'wasn', 'years']\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.nytimes.com/2019/10/16/world/middleeast/trump-erdogan-turkey-syria-kurds.html?action=click&module=Top%20Stories&pgtype=Homepage\"\n",
    "\n",
    "print(unknown(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['counterproductive', 'email', 'lebanon', 'meltdown', 'multimedia', 'near', 'ol', 'op', 'peterbakernyt', 'reemerge', 'rollout']\n"
     ]
    }
   ],
   "source": [
    "print(unknown(url, es = True, s = True, ed = True, ing = True, n = True, er = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['allenskratch', 'cyclo', 'll', 'longtime', 'maglia', 'plc', 'taylorphinney', 'triallist', 'unsubscribe', 'vibe']\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.cyclingnews.com/news/taylor-phinney-set-to-retire/\"\n",
    "print(unknown(url, es = True, s = True, ed = True, ing = True, n = True, er = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22. \n",
    "\n",
    "◑ Examine the results of processing the URL `http://news.bbc.co.uk/` using the regular expressions suggested above. You will see that there is still a fair amount of non-textual data there, particularly Javascript commands. You may also find that sentence breaks have not been properly preserved. Define further regular expressions that improve the extraction of text from this web page.\n",
    "\n",
    "*I find this question really poorly written.  We're supposed to use the 'regular expressions suggested above', but which ones?  The chapter is full of them!*\n",
    "\n",
    "*I've already made several functions that do fairly good jobs of extracting text and removing Javascript commands.  I don't really feel like breaking one of these functions just for the sake of practice...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Home - BBC News Homepage Accessibility links Skip to content Accessibility Help BBC Account Notifications Home News Sport Weather iPlayer Sounds CBBC CBeebies Food Bitesize Arts Taster Local TV Radio Three Menu Search Search the BBC Search the BBC BBC News News Navigation Sections Home Home selected Video World Asia UK Business Tech Science Stories Entertainment & Arts Health World News TV In Pictures Reality Check Newsbeat Special Reports Explainers Long Reads Have Your Say More More sections Home Home selected Video World World Home Africa Australia Europe Latin America Middle East US & Canada Asia Asia Home China India UK UK Home England N. Ireland Scotland Wales Politics Local News Business Business Home Market Data Global Trade Companies Entrepreneurship Technology of Business Business of Sport Global Education Economy Global Car Industry Tech Science Stories Entertainment & Arts Health World News TV In Pictures Reality Check Newsbeat Special Reports Explainers Long Reads Have Your Say BBC News Home Breaking Breaking news Close breaking news Latest Stories Most Read Skip to most read Latest Stories Most Read Top Stories Turkey-Syria 'not our border', says Trump His remarks come as the US comes under fire at home and abroad for withdrawing its troops from Syria. 3h 3 hours ago Middle East Turkey-Syria 'not our border', says Trump His remarks come as the US comes under fire at home and abroad for withdrawing its troops from Syria. 3h 3 hours ago Middle East Related content Video Trump exchange with reporter over troop withdrawal Viewpoint: Syria could cost Trump re-election Video Turkey presses on with Syria military offensive 'No Brexit deal tonight', UK source says UK and EU officials are continuing technical talks ahead of Thursday's crunch EU council meeting. 2h 2 hours ago UK Politics Hong Kong protest leader violently attacked Photographs show Jimmy Sham of the Civil Human Rights Front lying in the street, covered in blood. 8h 8 hours ago China Hundreds arr\""
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.bbc.com/news\"\n",
    "return_URL_contents(url)[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23. \n",
    "◑ Are you able to write a regular expression to tokenize text in such a way that the word *don't* is tokenized into *do* and *n't*? Explain why this regular expression won't work: `«n't|\\w+»`.\n",
    "\n",
    "*The short answer is that I'm not really sure.  I understand that this book is (was) designed so that it could be used in the classroom, and therefore the answers were not included; but I believe a large percentage - perhaps even the majority - of users are students engaging in self-study, and for these people (myself included), explanations would be greatly appreciated.*\n",
    "\n",
    "<i>My guess is that matches are greedy, and the regular expression evaluator will try to return the largest match that it can.  To turn this off, we need to add <code>(.*?)</code>.  Also, we need to add parentheses around `n't` to specify it as a capturing group.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('do', \"n't\")]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"(.*?)(n't)|\\w+\", \"don't\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24. \n",
    "\n",
    "◑ Try to write code to convert text into *hAck3r*, using regular expressions and substitution, where `e` → `3`, `i` → `1`, `o` → `0`, `l` → `|`, `s` → `5`, `.` → `5w33t!`, `ate` → `8`. Normalize the text to lowercase before converting it. Add more substitutions of your own. Now try to map `s` to two different values: `$` for word-initial `s`, and `5` for word-internal `s`.\n",
    "\n",
    "*This seems like a perfect place to `re.sub()`, which, typically, is not introduced until later in the exercise set...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H3||0 5uck3r55w33t!  I 8 y0ur |unch5w33t!  It wa5 d3|15h5w33t!'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"Hello suckers.  I ate your lunch.  It was delish.\"\n",
    "\n",
    "test = test.lower()\n",
    "\n",
    "org = ['ate', 'e', 'i', 'o', 'l', 's', '\\.']\n",
    "sub = ['8', '3', '1', '0', '|', '5', '5w33t!']\n",
    "\n",
    "for i in range(len(org)):\n",
    "    test = re.sub(org[i], sub[i], test)\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adding my own substitutions:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%3+3r %1%3r %1ck3d a %3ck 0f %1ck|3d %3%%3r55w33t!'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"Peter Piper picked a peck of pickled peppers.\"\n",
    "\n",
    "test = test.lower()\n",
    "\n",
    "org = ['e', 'i', 'o', 'l', 's', 't', 'p', '\\.']\n",
    "sub = ['3', '1', '0', '|', '5', '+', '%', '5w33t!']\n",
    "\n",
    "for i in range(len(org)):\n",
    "    test = re.sub(org[i], sub[i], test)\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Differentiating between initial $s$ and medial $s$:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$u513 |1v35 1n m1551551pp15w33t!'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"Susie lives in Mississippi.\"\n",
    "\n",
    "test = test.lower()\n",
    "\n",
    "org = ['ate', 'e', 'i', 'o', 'l', 't', r'\\bs', 's', '\\.']\n",
    "sub = ['8', '3', '1', '0', '|', '+', '$', '5', '5w33t!']\n",
    "\n",
    "for i in range(len(org)):\n",
    "    test = re.sub(org[i], sub[i], test)\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25. \n",
    "\n",
    "◑ Pig Latin is a simple transformation of English text. Each word of the text is converted as follows: move any consonant (or consonant cluster) that appears at the start of the word to the end, then append *ay*, e.g. *string* → *ingstray*, *idle* → *idleay*. http://en.wikipedia.org/wiki/Pig_Latin\n",
    "\n",
    " * a. Write a function to convert a word to Pig Latin.\n",
    " * b. Write code that converts text, instead of individual words.\n",
    " * c. Extend it further to preserve capitalization, to keep qu together (i.e. so that `quiet` becomes `ietquay`), and to detect when `y` is used as a consonant (e.g. `yellow`) vs a vowel (e.g. `style`).\n",
    " \n",
    "*Instead of going through the instructions in order, I think it might be easier to handle all of the exceptions from the beginning.  I'm also not going to include all the intermediate code, so what follows is my final answer to all parts of this question:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pig_latin(word):\n",
    "    \"\"\"\n",
    "    Returns pig latin version of word.\n",
    "    \"\"\"\n",
    "    \n",
    "    # replace 'dumb' quotes\n",
    "    word = re.sub(\"’\", \"'\", word)\n",
    "        \n",
    "    # won't work on non-alphabetic strings\n",
    "    if not word.isalpha():\n",
    "        if \"'\" not in word:\n",
    "            return word\n",
    "    \n",
    "    # Return uppercase word if original is in uppercase\n",
    "    caps = False\n",
    "    if word[0].isupper():\n",
    "        caps = True\n",
    "    word = word.lower()\n",
    "    \n",
    "    # word starts with vowel\n",
    "    if word[0] in 'AEIOUaeiou':\n",
    "        pl = word + 'ay'\n",
    "        \n",
    "    # some tokenizers will produce non-words    \n",
    "    elif len(word) == 1:\n",
    "        return word\n",
    "    \n",
    "    # word begins with 'y' - treated as a consonant\n",
    "    # otherwise 'y' is a vowel, or first vowel is not 'y'\n",
    "    elif word[0] == 'y':\n",
    "        pl = word[1:] + 'yay'\n",
    "    \n",
    "    # word begins with 'qu'\n",
    "    elif word[:2] == \"qu\":\n",
    "        pl = word[2:] + \"quay\"\n",
    "    \n",
    "    # all other cases\n",
    "    else:\n",
    "        start, end = re.findall(r'\\b^[^aeiouy]*|[aeiouy]{1}\\S*', word)\n",
    "        pl = end + start + 'ay'\n",
    "    \n",
    "    # restore word to uppercase if necessary\n",
    "    if caps == True:\n",
    "        pl = pl[0].upper() + pl[1:]\n",
    "    return pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*From Chapter 2, exercise 3.  It will make the final output look nicer by joining punctuation to the preceding string.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_punctuation(text, characters = [\"'\", '’', ')', ',', '.', ':', ';', '?', '!', ']', \"''\"]): \n",
    "    \"\"\"\n",
    "    Takes a list of strings and attaches punctuation to\n",
    "    the preceding string in the list.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = iter(text)\n",
    "    current = next(text)\n",
    "\n",
    "    for nxt in text:\n",
    "        if nxt in characters:\n",
    "            current += nxt\n",
    "        else:\n",
    "            yield current\n",
    "            current = nxt\n",
    "            \n",
    "\n",
    "    yield current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://funnystories.tumblr.com/post/140309670613/funny-story\n",
    "\n",
    "# For the sake of convenience, I've removed some punctuation.\n",
    "\n",
    "\n",
    "story = \"\"\"One time in sixth grade we were at recess and while I was running to \n",
    "my friends, I just so happened to kick a HUGE rock and without thinking I \n",
    "shouted at the top of my lungs MOTHERFUCKER And with my god-awful luck, my math\n",
    "teacher was sitting at the bench right BESIDE ME. He then took me inside to \n",
    "what I thought was yell at me but he just couldn’t stop laughing and sent \n",
    "me back outside with a literal candy bar. He is still my favorite teacher \n",
    "I've ever had.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pig_latin_text(text):\n",
    "    \"\"\"\n",
    "    Translates text into pig latin.\n",
    "    \"\"\"\n",
    "    pl = []\n",
    "    for t in re.findall(r'\\b[\\S]+\\b|[.,!?]', text):\n",
    "        pl.append(pig_latin(t))\n",
    "        \n",
    "    return \" \".join(join_punctuation(pl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oneay imetay inay ixthsay adegray eway ereway atay ecessray anday ilewhay Iay asway unningray otay ymay iendsfray, Iay ustjay osay appenedhay otay ickkay aay Ugehay ockray anday ithoutway inkingthay Iay outedshay atay ethay optay ofay ymay ungslay Otherfuckermay Anday ithway ymay god-awful ucklay, ymay athmay eachertay asway ittingsay atay ethay enchbay ightray Esidebay Emay. Ehay enthay ooktay emay insideay otay atwhay Iay oughtthay asway ellyay atay emay utbay ehay ustjay ouldn'tcay opstay aughinglay anday entsay emay ackbay outsideay ithway aay iterallay andycay arbay. Ehay isay illstay ymay avoritefay eachertay I'veay everay adhay.\""
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pig_latin_text(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  26.\n",
    "\n",
    "◑ Download some text from a language that has vowel harmony (e.g. Hungarian), extract the vowel sequences of words, and create a vowel bigram table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "path = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\chapter03\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I had considerable difficulties with this exercise.  I orignally tried to use a Hungarian Wikipedia page, but encountered trouble trying to extract the vowels from this page: No matter which regular expression I used, the vowels returned were always unaccented.  I spent quite a bit of time experimenting with different regular expressions until I discovered that diacritics are encoded separately from the vowels on Wikipedia.*\n",
    "\n",
    "*In the meanwhile, I just decided to use a Hungarian text from Project Gutenberg.  I don't know anything about Hungarian literature, but I had a text copy of \"Az arany ember (2. rész)\" by Mór Jókai on my hard drive from another project I had done, so I decided to use this text.  It worked fine for my purposes.*\n",
    "\n",
    "*Instead of using the method outlined in the book for importing Project Gutenberg texts, I have a function that I wrote while I was going through chapter 13 of \"Think Python\" (add link).  I'll use a simplified version of that here:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pg_text(text, encode = \"utf8\"):\n",
    "    \"\"\"\n",
    "    Returns a list of words from a Project Gutenberg text.  \n",
    "    Headers and footers are removed from texts. \n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    text: name of file\n",
    "    encode: text encoding used in file. Default is UTF-8\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    opened_text = open(text, 'r', encoding = encode)\n",
    "    cleaned_text = []\n",
    "    flag = False\n",
    "    start = \"*** START OF\"\n",
    "    end = \"*** END OF\"\n",
    "\n",
    "    # some PG texts don't use spaces to designate start/end of text\n",
    "    alt_start = \"***START OF\"\n",
    "    alt_end = \"***END OF\"\n",
    "    \n",
    "    for line in opened_text:\n",
    "        \n",
    "        # start reading in lines after boilerplate\n",
    "        if ((start in line) or (alt_start in line)) and flag == False:\n",
    "            flag = True\n",
    "        \n",
    "        # return word list once boilerplate has been reached\n",
    "        elif ((end in line) or (alt_end in line)) and flag == True:\n",
    "            return cleaned_text\n",
    "        elif flag == True:\n",
    "                \n",
    "                for word in line.split():\n",
    "                    word = word.strip().lower()\n",
    "                    cleaned_text.append(word)\n",
    "                \n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = clean_pg_text('hungarian.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*When we visually inspect the beginning of the text, we can see that the first line is in English.  Let's remove that so it doesn't interfere with our analysis:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['produced', 'by', 'albert', 'lászló', 'from', 'page', 'images', 'generously', 'made', 'available', 'by', 'the', 'google', 'books', 'library', 'project', 'jókai', 'mór', 'összes', 'művei', 'nemzeti', 'kiadás', 'xlvi.', 'kötet', 'az', 'arany', 'ember.', 'ii.', 'budapest', 'révai', 'testvérek', 'kiadása', '1896', 'az', 'arany', 'ember', 'regény', 'irta', 'jókai', 'mór', 'ii.', 'rész', 'pfeifer', 'ferdinánd', 'tulajdona', 'budapest', 'révai', 'testvérek', 'kiadása', '1896', 'a', 'világon', 'kivül.', 'a', 'leány', 'még', 'azután', 'is', 'ott', 'maradt', 'a', 'férfi', 'kebléhez', 'tapadva,', 'mikor', 'már', 'az', 'eltávozott,', 'a', 'kitől', 'őt', 'öntestével', 'védnie', 'kellett.', 'miért', 'tette', 'azt,', 'hogy', 'keblére', 'vesse', 'magát?', 'hogy', 'azt', 'mondja:', '«én', 'szeretem', 'őt?»', 'el', 'akarta', 'ezzel', 'űzni', 'végképen', 'azt', 'az', 'embert,', 'kinek', 'jelenlététől', 'iszonyodott?', 'lehetetlenné', 'akarta']\n"
     ]
    }
   ],
   "source": [
    "print(raw[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I checked the end as well, and it appears there's some sort of index there.  That will also interfere with our analysis, so let's eliminate that as well:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['létezéséről', 'pedig', 'frivaldszky', 'imre', 'nagynevű', 'természettudósunk', 'által', 'értesültem', 's', 'az', 'a', 'hatvanas', 'években', 'még', 'a', 'maga', 'kivételes', 'állapotában', 'megvolt,', 'mint', 'egy', 'se', 'magyar-,', 'se', 'törökországhoz', 'nem', 'tartozó', 'új', 'alkotású', 'terület.', 'ennyit', 'jónak', 'láttam', 'elmondani.', '_dr.', 'jókai', 'mór._', 'tartalom.', 'második', 'kötet.', 'a', 'világon', 'kivül', '1', 'tropicus', 'capricorni', '8', 'az', 'édes', 'otthon', '30', 'a', 'családi', 'ékszer', '35', 'egy', 'uj', 'vendég', '50', 'a', 'faragó', 'ember', '72', 'noémi', '79', 'melancholia', '97', 'teréza', '120', 'a', 'kettétört', 'kard', '134', 'az', 'első', 'veszteség', '157', 'a', 'jég', '167', 'a', 'rém', '183', 'mit', 'beszél', 'a', 'hold?', '–', 'mit', 'beszél', 'a', 'jég?', '211', 'ki', 'jön?', '217', 'a', 'hulla', '221', 'zófi', 'asszony', '224', 'dódi', 'levele', '230', 'te', 'ügyetlen!', '235', 'athalia', '216', 'az', 'utolsó', 'tőrdöfés', '264', 'a', 'mária-nostrai', 'nő', '270', 'a', '«senki»', '271', 'utóhangok', '279', 'franklin-társulat', 'nyomdája.', \"[transcriber's\", 'note:', 'javítások.', 'az', 'eredeti', 'szöveg', 'helyesírásán', 'nem', 'változtattunk.', 'a', 'nyomdai', 'hibákat', 'javítottuk.', 'ezek', 'listája:', '47', '|fog', 'mindent,', '|fog', 'mindent.', '122', '|közül', 'kimelkedő', '|közül', 'kiemelkedő', '126', '|ez', 'ídő', 'óta', '|ez', 'idő', 'óta', '182', '|sőt', 'itt', 'ott-', '|sőt', 'itt-ott', '189', '|magyarországba.', '|magyarországba.»', '189', '|basát?', 'mordult', '|basát?»', 'mordult', '192', '|keretezve…', '|keretezve…»', '193', '|numerusa', 'sincs.', '|numerusa', 'sincs.»', '224', '|susánna…', '|susánna…»', '256', '|mákhéjakból.', '|mákhéjakból.»]', 'end', 'of', 'the', 'project', 'gutenberg', 'ebook', 'of', 'az', 'arany', 'ember', '(2.', 'rész),', 'by', 'mór', 'jókai']\n"
     ]
    }
   ],
   "source": [
    "print(raw[-200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73955"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = raw[16:-158]\n",
    "len(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Extracting all the vowels from the words in the text:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv = []\n",
    "for hw in raw:\n",
    "    hv.append(re.findall(r'([aeiouáéíóöúüőű])', hw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I know practically nothing about Hungarian, let alone vowel harmony.  However, from the scant research I've done (mostly [this website](http://www.hungarianreference.com/Vowel-Harmony.aspx \"vowel harmony in Hungarian\")), I believe that this phenomenon only affects the final vowels.  So for our analysis, we'll only be looking at the final two vowels.  Therefore, we'll eliminate any word with only one vowel; and we'll eliminate the beginning vowels from words with three or more:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_pairs = []\n",
    "\n",
    "for h in hv:\n",
    "    if len(h) == 2:\n",
    "        hv_pairs.append(h[0] + h[1])\n",
    "    elif len(h) > 2:\n",
    "        hv_pairs.append(h[-2] + h[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43759"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hv_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a    e    i    o    u    á    é    í    ó    ö    ú    ü    ő    ű \n",
      "a 2896  209 1133 1718  181 1215  126   10  424   17   34   10    5    0 \n",
      "e   76 5739 1909   97   23  224 1581   12    9   66    8  438  737   67 \n",
      "i  709 1745  154  548   46  884  234    3   90   13    8   14  167   13 \n",
      "o 2597   39  312  792  126  769   38    9  240    2   89    0    2    3 \n",
      "u  700   23  146  257   15  381    8    4   95    1    0    0    2    0 \n",
      "á 2388   23  441  853  115  392   57    9  360    4   14    5    7    1 \n",
      "é  406 2432  637   28    4   82  293    5    7    8    0  101  244   14 \n",
      "í  108   88   20   75    1   73   25    0   29    0    0   18   19    5 \n",
      "ó  350    7  161   98   17   82   15    1   30    0    0    0    1    1 \n",
      "ö    0  583  174    1    0    5  279    3    0  373    0  100   76   26 \n",
      "ú   94   12   26   27    0   18    0    0   13    2    2    0    5    0 \n",
      "ü    1  244   81    3    0    1   68    0    0  116    0    7   34    4 \n",
      "ő   66  536   78    2    0    4   40    0    0   81    0   34   20    3 \n",
      "ű    2   50    9    0    0    1   19    0    1   23    1    2    1   12 \n"
     ]
    }
   ],
   "source": [
    "cfd = nltk.ConditionalFreqDist(hv_pairs)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Unfortunately, there aren't so many patterns to be seen in this table.  It looks like there's a concentration in the upper-left corner, but that could just be because unaccented vowels are more common.  From what I can tell from the [web page cited earlier](http://www.hungarianreference.com/Vowel-Harmony.aspx \"Vowel Harmony\"), there are two types of vowels in Hungarian: Back vowels (__a__, __á__, __(i)__, __(í)__, __o__, __ó__, __u__, __ú__), and front vowels (__e__, __é__, __(i)__, __(í)__, __ö__, __ő__, __ü__, __ű__).  The vowels __i__ and __í__ are considered intermediate vowels, and don't contribute to vowel harmony.*\n",
    "\n",
    "*What I did next was use the `itertools` package to create a power set of all possible combinations of back vowels.  The combinations are saved as tuples, so I used a list comprehension to convert these to strings.  I removed all the combinations with only i or í, and finally I used `.count()` to look at how many instances of each combination could be found in the original pairs.  I repeated these steps for the front vowels:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oa', 'aa', 'oo', 'iá', 'áu', 'óá', 'uí', 'ái', 'uu', 'áá', 'úá', 'úa', 'oí', 'ai', 'aó', 'aá', 'úo', 'iú', 'óú', 'ía', 'ío', 'uú', 'oú', 'áú', 'ui', 'aú', 'uó', 'íó', 'ói', 'ou', 'íu', 'óí', 'uá', 'úí', 'úu', 'áí', 'íá', 'ia', 'óa', 'oá', 'au', 'aí', 'ua', 'áa', 'uo', 'úú', 'áo', 'ao', 'úi', 'úó', 'io', 'ió', 'oi', 'oó', 'óo', 'íú', 'iu', 'áó', 'óu', 'óó']\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# back vowels\n",
    "bv = ['a', 'á', 'i', 'í', 'o', 'ó', 'u', 'ú']\n",
    "\n",
    "# create power set\n",
    "bvps = set(list(itertools.product(bv, bv)))\n",
    "\n",
    "# convert items in power set to strings\n",
    "bvpj = [b[0] + b[1] for b in bvps]\n",
    "\n",
    "# Remove combinations with only i or í\n",
    "tbd = ['ií', 'íi', 'ii', 'íí']\n",
    "for i in tbd:\n",
    "    bvpj.remove(i)\n",
    "    \n",
    "print(bvpj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22205"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum all possible combinations\n",
    "back_vowels = 0\n",
    "for i in bvpj:\n",
    "    back_vowels += hv_pairs.count(i)\n",
    "    \n",
    "back_vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['éű', 'üű', 'őő', 'őö', 'űű', 'öe', 'íe', 'éi', 'iü', 'íé', 'űi', 'iű', 'íö', 'eü', 'üi', 'űü', 'őí', 'ei', 'üü', 'éü', 'íő', 'öő', 'öű', 'üé', 'őe', 'űé', 'éé', 'őű', 'ée', 'űö', 'éő', 'eö', 'íü', 'üe', 'eé', 'öí', 'üö', 'ié', 'ee', 'űő', 'eí', 'éö', 'ie', 'üí', 'ői', 'éí', 'öé', 'iő', 'öi', 'öö', 'iö', 'íű', 'űí', 'eő', 'űe', 'eű', 'őé', 'őü', 'üő', 'öü']\n"
     ]
    }
   ],
   "source": [
    "# same procedure with front vowels\n",
    "fv = ['e', 'é', 'i', 'í', 'ö', 'ő', 'ü', 'ű']\n",
    "fvps = set(list(itertools.product(fv, fv)))\n",
    "fvpj = [f[0] + f[1] for f in fvps]\n",
    "\n",
    "tbd = ['ií', 'íi', 'ii', 'íí']\n",
    "\n",
    "for i in tbd:\n",
    "    fvpj.remove(i)\n",
    "    \n",
    "print(fvpj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19700"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "front_vowels = 0\n",
    "for i in fvpj:\n",
    "    front_vowels += hv_pairs.count(i)\n",
    "    \n",
    "front_vowels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Again, my knowledge of Hungarian is practically non-existent; but my analysis shows that less than 5% of the words in this novel failed to show vowel harmony.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.236842706643205"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(hv_pairs) - front_vowels - back_vowels)/len(hv_pairs) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I later came back to my original issue and tried to read in the text from a Wikipedia page.  I reluctant to use an entry on a non-Hungarian topic, since the number of non-Hungarian words in the article might affect the analysis.  So I decided to use the entry on Franz Liszt, as he was from Hungary.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://hu.wikipedia.org/wiki/Liszt_Ferenc'\n",
    "test = return_URL_contents(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Liszt Ferenc – Wikipédia Liszt Ferenc A Wikipédiából, a szabad enciklopédiából Ez a közzétett változat , ellenőrizve : 2019. október 3. Ugrás a navigációhoz Ugrás a kereséshez Ez a sz'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Liszt', 'Ferenc', 'Wikipédia', 'Liszt', 'Ferenc', 'A', 'Wikipédiából', 'a', 'szabad', 'enciklopédiából']\n"
     ]
    }
   ],
   "source": [
    "test_words = re.findall(r'\\b[\\S]+\\b', test)\n",
    "print(test_words[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The diacritics in the words of articles on Wikipedia are formed by the use of combining characters.  There are only three combining characters we need to deal with for our analysis of Hungarian vowels: the acute accent (`´` - U+0301); the diaresis (`¨` - U+0308); and the double accute accent - or \"Hungarumlaut\" - (`˝` - U+030b).  We're going to use regular expressions to see for only these three combining characters.  While it is of course possible to create regular expressions to find any and all combining characters, these aren't necessary for our analysis, and will actually interfere with it.*\n",
    "\n",
    "*Wikipedia has very useful articles on [combining characters](https://en.wikipedia.org/wiki/Combining_character 'Combining Characters') and [diacritics](https://en.wikipedia.org/wiki/Diacritic 'Diacritics').*\n",
    "\n",
    "*To find these combining characters, we can't use any of the methods shown in the book. Encoding strings with `encode('unicode_escape')` will return a bytes-like object, which can't be parsed by regular expressions:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Wikipe\\\\u0301dia\\\\u0301bo\\\\u0301l'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ue = 'Wikipédiából'.encode('unicode_escape')\n",
    "ue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "re.findall(r'([aeiou])(\\\\u0301|\\\\u0308|\\\\u030b)*', ue)\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "<ipython-input-35-035e1743cc34> in <module>\n",
    "----> 1 re.findall(r'([aeiou])(\\\\u0301|\\\\u0308|\\\\u030b)*', ue)\n",
    "\n",
    "~\\AppData\\Local\\Continuum\\anaconda3\\lib\\re.py in findall(pattern, string, flags)\n",
    "    221 \n",
    "    222     Empty matches are included in the result.\"\"\"\n",
    "--> 223     return _compile(pattern, flags).findall(string)\n",
    "    224 \n",
    "    225 def finditer(pattern, string, flags=0):\n",
    "\n",
    "TypeError: cannot use a string pattern on a bytes-like object\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The best workaround that I found was to use `ascii` to get the code point integer for non-ascii characters returned as a string:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hv = []\n",
    "for hw in test_words:\n",
    "    test_hv.append(re.findall(r'([aeiou])(\\\\u0301|\\\\u0308|\\\\u030b)*', ascii(hw)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('i', '')], [('e', ''), ('e', '')], [('i', ''), ('i', ''), ('e', '\\\\u0301'), ('i', ''), ('a', '')], [('i', '')], [('e', ''), ('e', '')], [], [('i', ''), ('i', ''), ('e', '\\\\u0301'), ('i', ''), ('a', '\\\\u0301'), ('o', '\\\\u0301')], [('a', '')], [('a', ''), ('a', '')], [('e', ''), ('i', ''), ('o', ''), ('e', '\\\\u0301'), ('i', ''), ('a', '\\\\u0301'), ('o', '\\\\u0301')]]\n"
     ]
    }
   ],
   "source": [
    "print(test_hv[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The regular expression above returns a list for each word with a tuple for each vowel.  If the vowel is unaccented, the second string is an empty string.  The following loop ignores words with only one vowel; concatenates the vowels in words with two vowels; and concatenates the last two vowels from words with more than two vowels:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hvc = []\n",
    "for l in test_hv:\n",
    "    if len(l) == 2:\n",
    "        test_hvc.append(l[0][0] + l[0][1] + l[1][0] + l[1][1])\n",
    "    elif len(l) > 2:\n",
    "        test_hvc.append(l[-2][0] + l[-2][1] + l[-1][0] + l[-1][1])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Currently, accented vowels still have their Unicode escape strings attached to them:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ee', 'ia', 'ee', 'a\\\\u0301o\\\\u0301', 'aa', 'a\\\\u0301o\\\\u0301', 'e\\\\u0301e', 'oa', 'ie', 'o\\\\u0301e']\n"
     ]
    }
   ],
   "source": [
    "print(test_hvc[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*While it's possible to use `re.sub` to substitute escape strings on individual strings, this doesn't work on strings inside lists:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'áo'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'a\\\\u0301', 'á', 'a\\\\u0301o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ee', 'ia', 'ee', 'a\\\\u0301o\\\\u0301', 'aa', 'a\\\\u0301o\\\\u0301', 'e\\\\u0301e', 'oa', 'ie', 'o\\\\u0301e']\n"
     ]
    }
   ],
   "source": [
    "for th in test_hvc[:10]:\n",
    "    re.sub(r'a\\\\u0301', 'á', th)\n",
    "    \n",
    "print(test_hvc[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The only way to deal with this is to make a new list with the substituted values.  In our case, we'll have to make nine substitutions, so that would mean nine new lists.  Instead of having nine near-identical lists floating about, I decided to just use the name of the old list, and re-assigned the new list to it.*\n",
    "\n",
    "*To avoid repeating this step nine times, I created a simple function that will simply use a given `re.sub()` command to generate an updated list.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_list(l, org, sub):\n",
    "    \"\"\"\n",
    "    Uses re.sub to generate a new list.\n",
    "    \n",
    "    Arguments:\n",
    "    l:   list with strings to be replaced\n",
    "    org: regular expression to be replaced  \n",
    "    sub: replacement regular expression\n",
    "    \"\"\"\n",
    "    new_list = []\n",
    "    \n",
    "    for i in l:\n",
    "        new_list.append(re.sub(org, sub, i))\n",
    "        \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To avoid writing out all of the regular expressions to be matched, I tried to automate the process as much as possible by using a pair of `for`-loops.  The loops make one pass for each vowel and diacritic combination - i.e., 15 passes.  However, there are only nine vowels with diacritics in Hungarian.  To keep the loops aligned, I added empty spaces to the strings `diaresis` and `double_acute`:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = 'aeiou'\n",
    "uc = ['\\\\u0301', '\\\\u0308', '\\\\u030b']\n",
    "acute = 'áéíóú'\n",
    "\n",
    "# empty spaces are there for alignment\n",
    "diaresis = '   öü'\n",
    "double_acute = '   őű'\n",
    "diacritics = [acute, diaresis, double_acute]\n",
    "\n",
    "for i in range(len(vowels)):\n",
    "    for j in range(len(uc)):\n",
    "        test_hvc = generate_new_list(test_hvc, r'' + re.escape(vowels[i] + uc[j]) + '', diacritics[j][i])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          a    e    i    o    u    á    é    í    ó    ö    ú    ü    ő    ű \n",
      "     0    0   26    1    0    0    0    0    0    0    0    0    0    0    0 \n",
      "a    3  488  163  324  510   83  163   22    4   17    2    2    0    1    0 \n",
      "e    0   75 1186  366  179   49   25  213    2    0   10    1   46   65    9 \n",
      "i    0  291  370   94   90  155   85   31    0   19    0    0    1   18    0 \n",
      "o    0  413  115  123  147   38  125    7    0   14    1    1    0    1    0 \n",
      "u    0  133  188   37   43   60  147    5    0   18    1    0    0    0    0 \n",
      "á    0  593    5  117  237   57   97    7    4   50    1    0    0    0    0 \n",
      "é    0   21  588   62   21   10   15   88    0    1    0    0   28   48    0 \n",
      "í    0   24   35    5   21   25   24    9    0    2    0    0    7    1    1 \n",
      "ó    0   85   35    8   31    1   17    1    0    0    0    0    0    0    0 \n",
      "ö    0    4   72   18    7    0    5   20    0    0   70    3   14    0    0 \n",
      "ú    0   32    0    1    5    0    6    1    0    0    0    0    0    0    0 \n",
      "ü    0    0   14   15   17    1    1    4    0    0    5    1    3    4    0 \n",
      "ő    0    2   73   16    4    0    0   19    0    0   19    0    1    5    0 \n",
      "ű    0    1   27    1    0    0    0   27    0    0    3    0    0    0    0 \n"
     ]
    }
   ],
   "source": [
    "cfd = nltk.ConditionalFreqDist(test_hvc)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As I did with \"Az arany ember (2. rész)\" above, I'll calculate of the number of front-/back-vowel pairs versus all vowel pairs:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4967"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum all possible combinations\n",
    "back_vowels = 0\n",
    "for i in bvpj:\n",
    "    back_vowels += test_hvc.count(i)\n",
    "    \n",
    "back_vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3614"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "front_vowels = 0\n",
    "for i in fvpj:\n",
    "    front_vowels += test_hvc.count(i)\n",
    "    \n",
    "front_vowels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here, the percentage of vowel pairs that failed to show vowel harmony is higher than in the first experiment, but still quite low overall.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.618086311669584"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(test_hvc) - front_vowels - back_vowels)/len(test_hvc) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Since Wikipedia articles might not be entirely representative of a Hungarian text, I repeated the experiment with an article from a Hungarian news site:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://index.hu/belfold/2019/10/18/orban_viktor_bolcsek_tanacsa_neppart_masodik_talalkozas/'\n",
    "testH = return_URL_contents(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Index - Belföld - Orbán Viktor újra találkozott a Néppárt bölcseivel Keresés Blog.hu Fórum Indafotó Indavideó Indamail Blog.hu Címlap Totalcar JóAutók Dívány Femina Inda Otthontérkép'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testH[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Index', 'Belföld', 'Orbán', 'Viktor', 'újra', 'találkozott', 'a', 'Néppárt', 'bölcseivel', 'Keresés']\n"
     ]
    }
   ],
   "source": [
    "testH_words = re.findall(r'\\b[\\S]+\\b', testH)\n",
    "print(testH_words[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "testH_hv = []\n",
    "for hw in testH_words:\n",
    "    testH_hv.append(re.findall(r'([aeiou])(\\\\u0301|\\\\u0308|\\\\u030b)*', ascii(hw)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('e', '')], [('e', ''), ('o', '\\\\u0308')], [('a', '\\\\u0301')], [('i', ''), ('o', '')], [('u', '\\\\u0301'), ('a', '')], [('a', ''), ('a', '\\\\u0301'), ('o', ''), ('o', '')], [('a', '')], [('e', '\\\\u0301'), ('a', '\\\\u0301')], [('o', '\\\\u0308'), ('e', ''), ('i', ''), ('e', '')], [('e', ''), ('e', ''), ('e', '\\\\u0301')]]\n"
     ]
    }
   ],
   "source": [
    "print(testH_hv[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "testH_hvc = []\n",
    "for l in testH_hv:\n",
    "    if len(l) == 2:\n",
    "        testH_hvc.append(l[0][0] + l[0][1] + l[1][0] + l[1][1])\n",
    "    elif len(l) > 2:\n",
    "        testH_hvc.append(l[-2][0] + l[-2][1] + l[-1][0] + l[-1][1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eo\\\\u0308', 'io', 'u\\\\u0301a', 'oo', 'e\\\\u0301a\\\\u0301', 'ie', 'ee\\\\u0301', 'ou', 'o\\\\u0301u', 'oo']\n"
     ]
    }
   ],
   "source": [
    "print(testH_hvc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = 'aeiou'\n",
    "uc = ['\\\\u0301', '\\\\u0308', '\\\\u030b']\n",
    "acute = 'áéíóú'\n",
    "\n",
    "# empty spaces are there for alignment\n",
    "diaresis = '   öü'\n",
    "double_acute = '   őű'\n",
    "diacritics = [acute, diaresis, double_acute]\n",
    "\n",
    "for i in range(len(vowels)):\n",
    "    for j in range(len(uc)):\n",
    "        testH_hvc = generate_new_list(testH_hvc, r'' + re.escape(vowels[i] + uc[j]) + '', diacritics[j][i])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a  e  i  o  u  á  é  í  ó  ö  ü  ő \n",
      "a 14  4 13  7  1  8  1  0  0  0  0  0 \n",
      "e  3 34 17  7  4  2  5  0  0  7  0  2 \n",
      "i 12 34  1 12  2  4  0  0  3  0  1  0 \n",
      "o 13  3  1 16  3 19  0  0  1  0  0  0 \n",
      "u  8  1  1  1  0  3  0  1  2  0  0  0 \n",
      "á 21  3  7  8  1  5  0  0  2  0  0  0 \n",
      "é  2 10  2  0  0 10  4  0  1  0  1  0 \n",
      "í  6  0  2  1  0  3  1  0  0  0  0  0 \n",
      "ó  2  2  1  1  2  2  1  0  0  0  0  0 \n",
      "ö  0  8  1  0  0  0  3  0  0  1  0  1 \n",
      "ú  2  1  0  1  0  0  0  0  0  0  0  0 \n",
      "ü  0  5  2  0  0  0  1  0  0  1  0  0 \n",
      "ő  0  5  2  1  0  0  1  0  0  1  0  0 \n"
     ]
    }
   ],
   "source": [
    "cfd = nltk.ConditionalFreqDist(testH_hvc)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum all possible combinations\n",
    "back_vowels = 0\n",
    "for i in bvpj:\n",
    "    back_vowels += testH_hvc.count(i)\n",
    "    \n",
    "back_vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "front_vowels = 0\n",
    "for i in fvpj:\n",
    "    front_vowels += testH_hvc.count(i)\n",
    "    \n",
    "front_vowels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The percentage of vowel pairs here is closer to that of the Wikipedia page.  Perhaps it's the novel that's the outlier?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.98044009779951"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(testH_hvc) - front_vowels - back_vowels)/len(testH_hvc) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27. \n",
    "\n",
    "◑ Python's `random` module includes a function `choice()` which randomly chooses an item from a sequence, e.g. `choice(\"aehh \")` will produce one of four possible characters, with the letter `h` being twice as frequent as the others. Write a generator expression that produces a sequence of 500 randomly chosen letters drawn from the string `\"aehh \"`, and put this expression inside a call to the `''.join()` function, to concatenate them into one long string. You should get a result that looks like uncontrolled sneezing or maniacal laughter: `he  haha ee  heheeh eha`. Use `split()` and `join()` again to normalize the whitespace in this string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hehhhh aheeh aehhh ehaehhea ae  hhah ae aahhe a ahahaheahhaa  hheh e hhe h  ha  h hhh hehh h ae heaha hheahh  ahaha a e eaehehha e   heaeh   ahh  hhe hhh eee a eh hhh h eh hea ehhaahhaahhh hhahaahahhhhhaa e h  hehae  eeehahhhheeehaheheha a a  hahaahaaehahhhaeahe eaheh aheeehhe eah hhhhhe  hh aaea hhhheaahehhh ahe eaeeheaaaeahhhe ah h hhhe  heaeahh a h haheahhhaeeaaa h hhhaa heh ehhhhheehahaee eha h hehhh ahhhahahehahhh  a  haehh ahhhha hahehh ea h   hh hhhheehhh aaeahheehhhahehehahh eheaahheaehe'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "new_list = []\n",
    "\n",
    "for i in range(500):\n",
    "    new_list.append(random.choice(\"aehh \"))\n",
    "    \n",
    "string = ''.join(new_list)\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hehhhh aheeh aehhh ehaehhea ae hhah ae aahhe a ahahaheahhaa hheh e hhe h ha h hhh hehh h ae heaha hheahh ahaha a e eaehehha e heaeh ahh hhe hhh eee a eh hhh h eh hea ehhaahhaahhh hhahaahahhhhhaa e h hehae eeehahhhheeehaheheha a a hahaahaaehahhhaeahe eaheh aheeehhe eah hhhhhe hh aaea hhhheaahehhh ahe eaeeheaaaeahhhe ah h hhhe heaeahh a h haheahhhaeeaaa h hhhaa heh ehhhhheehahaee eha h hehhh ahhhahahehahhh a haehh ahhhha hahehh ea h hh hhhheehhh aaeahheehhhahehehahh eheaahheaehe\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(string.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 28. \n",
    "\n",
    "◑ Consider the numeric expressions in the following sentence from the MedLine Corpus: *The corresponding free cortisol fractions in these sera were 4.53 +/- 0.15% and 8.16 +/- 0.23%, respectively*. Should we say that the numeric expression *4.53 +/- 0.15%* is three words? Or should we say that it's a single compound word? Or should we say that it is actually *nine* words, since it's read \"four point five three, plus or minus zero point fifteen percent\"? Or should we say that it's not a \"real\" word at all, since it wouldn't appear in any dictionary? Discuss these different possibilities. Can you think of application domains that motivate at least two of these answers?\n",
    "\n",
    "*__If we were doing lexical analysis (e.g., word frequency), I would say the expression should count as zero words: the number of occurences of a numerical expression such as 4.53 would most likely be irrelevant, since, unlike words, these numerical expressions are not selected by the author. Numerical expressions can also become quite long, which would skew measures such as average word length, etc...  But this would present an inconsistency: why should written numbers (e.g., 'four') be counted as words if the numerical forms (i.e., '4') are not?  As a compromise, I would suggest tallying each numerical expression as one word.__*\n",
    "\n",
    "*__However, if we were dealing with something like a text-to-speech system, then I would say that each digit counts as a single word, since synthesizing a digit involves the same procedure as synthesizing a word.__*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 29.\n",
    "\n",
    "◑ Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of appropriate difficulty for language learners. Let us define $μ_w$ to be the average number of letters per word, and $μ_s$ to be the average number of words per sentence, in a given text. The Automated Readability Index (ARI) of the text is defined to be: $4.71 μ_w + 0.5 μ_s - 21.43$. Compute the ARI score for various sections of the Brown Corpus, including section `f` (lore) and `j` (learned). Make use of the fact that `nltk.corpus.brown.words()` produces a sequence of words, while `nltk.corpus.brown.sents()` produces a sequence of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "def get_brown_ari(cat):\n",
    "    \"\"\"\n",
    "    Returns the Automated Readability Index (ARI) of a given\n",
    "    category of the Brown Corpus.\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate total letters in the category\n",
    "    total_letters = 0\n",
    "    for w in brown.words(categories = cat):\n",
    "        total_letters += len(w)\n",
    "    \n",
    "    # calculate average number of letters per word\n",
    "    mu_w = total_letters/len(brown.words(categories = cat))\n",
    "    \n",
    "    # calculate average number of words per sentence\n",
    "    mu_s = len(brown.words(categories = cat)) / len(brown.sents(categories = cat))\n",
    "    \n",
    "    return (4.71 * mu_w) + (0.5 * mu_s) - 21.43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To make the category names look neater when I'm printing them, I used `re.sub()` to get rid of the underscore in 'belle_lettres', and `.title()` to print all the words in the titles in uppercase.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the Brown Corpus, the Automated Readability Index (ARI) for the category:\n",
      "\n",
      "                                             ...\"Adventure\" is 4.0842.\n",
      "                                             ...\"Belles Lettres\" is 10.9877.\n",
      "                                             ...\"Editorial\" is 9.4710.\n",
      "                                             ...\"Fiction\" is 4.9105.\n",
      "                                             ...\"Government\" is 12.0843.\n",
      "                                             ...\"Hobbies\" is 8.9224.\n",
      "                                             ...\"Humor\" is 7.8878.\n",
      "                                             ...\"Learned\" is 11.9260.\n",
      "                                             ...\"Lore\" is 10.2548.\n",
      "                                             ...\"Mystery\" is 3.8336.\n",
      "                                             ...\"News\" is 10.1767.\n",
      "                                             ...\"Religion\" is 10.2031.\n",
      "                                             ...\"Reviews\" is 10.7697.\n",
      "                                             ...\"Romance\" is 4.3492.\n",
      "                                             ...\"Science Fiction\" is 4.9781.\n"
     ]
    }
   ],
   "source": [
    "print(\"In the Brown Corpus, the Automated Readability Index (ARI) for the category:\\n\")\n",
    "\n",
    "for c in brown.categories():\n",
    "    ari = get_brown_ari(c)\n",
    "    c = re.sub('_', ' ', c)\n",
    "    print(\"{:45}...\\\"{}\\\" is {:.4f}.\".format(\"\",c.title(), ari) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 30. \n",
    "\n",
    "◑ Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer and see if you observe any differences.\n",
    "\n",
    "*For a text I'm going to use the Wikipedia entry on Martin Porter, the creator of the Porter Stemmer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin Porter - Wikipedia Martin Porter From Wikipedia, the free encyclopedia Jump to navigation Jump to search For the musician, see Martin Porter (musician) . Martin F. Porter is the inventor of the Porter Stemmer , [1] one of the most common algor\n"
     ]
    }
   ],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Martin_Porter'\n",
    "\n",
    "to_be_stemmed = return_URL_contents(url)\n",
    "print(to_be_stemmed[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Martin', 'Porter', '-', 'Wikipedia', 'Martin', 'Porter', 'From', 'Wikipedia', ',', 'the', 'free', 'encyclopedia', 'Jump', 'to', 'navigation', 'Jump', 'to', 'search', 'For', 'the', 'musician', ',', 'see', 'Martin', 'Porter', '(', 'musician', ')', '.', 'Martin', 'F.', 'Porter', 'is', 'the', 'inventor', 'of', 'the', 'Porter', 'Stemmer', ',', '[', '1', ']', 'one', 'of', 'the', 'most', 'common', 'algorithms', 'for', 'stemming', 'English', ',', '[', '2', ']', '[', '3', ']', 'and', 'the', 'Snowball', 'programming', 'framework', '.', 'His', '1980', 'paper', '``', 'An', 'algorithm', 'for', 'suffix', 'stripping', \"''\", ',', 'proposing', 'the', 'stemming', 'algorithm', ',', 'has', 'been', 'cited', 'over', '8000', 'times', '(', 'Google', 'Scholar', ')', '.', '[', '4', ']', 'The', 'Muscat', 'search', 'engine', 'comes']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(to_be_stemmed)\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['martin', 'porter', '-', 'wikipedia', 'martin', 'porter', 'from', 'wikipedia', ',', 'the', 'free', 'encyclopedia', 'jump', 'to', 'navig', 'jump', 'to', 'search', 'for', 'the', 'musician', ',', 'see', 'martin', 'porter', '(', 'musician', ')', '.', 'martin', 'F.', 'porter', 'is', 'the', 'inventor', 'of', 'the', 'porter', 'stemmer', ',', '[', '1', ']', 'one', 'of', 'the', 'most', 'common', 'algorithm', 'for', 'stem', 'english', ',', '[', '2', ']', '[', '3', ']', 'and', 'the', 'snowbal', 'program', 'framework', '.', 'hi', '1980', 'paper', '``', 'An', 'algorithm', 'for', 'suffix', 'strip', \"''\", ',', 'propos', 'the', 'stem', 'algorithm', ',', 'ha', 'been', 'cite', 'over', '8000', 'time', '(', 'googl', 'scholar', ')', '.', '[', '4', ']', 'the', 'muscat', 'search', 'engin', 'come']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "print([porter.stem(t) for t in tokens[:100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['martin', 'port', '-', 'wikiped', 'martin', 'port', 'from', 'wikiped', ',', 'the', 'fre', 'encycloped', 'jump', 'to', 'navig', 'jump', 'to', 'search', 'for', 'the', 'mus', ',', 'see', 'martin', 'port', '(', 'mus', ')', '.', 'martin', 'f.', 'port', 'is', 'the', 'inv', 'of', 'the', 'port', 'stem', ',', '[', '1', ']', 'on', 'of', 'the', 'most', 'common', 'algorithm', 'for', 'stem', 'engl', ',', '[', '2', ']', '[', '3', ']', 'and', 'the', 'snowbal', 'program', 'framework', '.', 'his', '1980', 'pap', '``', 'an', 'algorithm', 'for', 'suffix', 'stripping', \"''\", ',', 'propos', 'the', 'stem', 'algorithm', ',', 'has', 'been', 'cit', 'ov', '8000', 'tim', '(', 'googl', 'scholar', ')', '.', '[', '4', ']', 'the', 'musc', 'search', 'engin', 'com']\n"
     ]
    }
   ],
   "source": [
    "print([lancaster.stem(t) for t in tokens[:100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Of the two stemmers, the Lancaster Stemmer is __much__ greedier - it strips practically all endings.  I was keeping a running total of all the endings that it stripped, but gave up in the middle, figuring it might have been easier to just make a list of the endings that weren't stripped.*\n",
    "\n",
    "*In the brief samples above, I noticed that both stemmers removed the following endings: '-s'; '-es'; '-ing' (and double consonants); '-d' from '-ed' (Porter removes just '-d', while Lancaster removes the entire '-ed'); '-ation'; and '-l' from '-ll.*\n",
    "\n",
    "*In at least the sample above, I couldn't find any examples of endings that were stripped by the Porter Stemmer but not by the Lancaster Stemmer. The converse was a much different situation: The Lancaster Stemmer removed '-er', including the ending of Porter's name; '-ia', including 'Wikipedia'; 'e' from '-ee'; '-ician'; '-entor'; '-ish'; and even '-at'.  In my opinion, the Lancaster Stemmer ended up stripping too much of the word, to the point where stems would occasionally become completely unintelligible.*\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 31.\n",
    "\n",
    "◑ Define the variable saying to contain the list `['After', 'all', 'is', 'said', 'and', 'done', ',', 'more',\n",
    "'is', 'said', 'than', 'done', '.']`. Process this list using a `for` loop, and store the length of each word in a new list lengths. Hint: begin by assigning the empty list to `lengths`, using `lengths = []`. Then each time through the loop, use `append()` to add another length value to the list. Now do the same thing using a list comprehension.\n",
    "\n",
    "*Seriously? Aren't these problems supposed to get progressively more difficult?  This problem is a cakewalk compared to some of the problems that came before it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 2, 4, 3, 4, 1, 4, 2, 4, 4, 4, 1]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saying = ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more',\n",
    "'is', 'said', 'than', 'done', '.']\n",
    "lengths = []\n",
    "\n",
    "for s in saying:\n",
    "    lengths.append(len(s))\n",
    "    \n",
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 2, 4, 3, 4, 1, 4, 2, 4, 4, 4, 1]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = [len(s) for s in saying] \n",
    "lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 32.\n",
    "\n",
    "◑ Define a variable silly to contain the string: `'newly formed bland ideas are inexpressible in an infuriating\n",
    "way'`. (This happens to be the legitimate interpretation that bilingual English-Spanish speakers can assign to Chomsky's famous nonsense phrase, *colorless green ideas sleep furiously* according to Wikipedia). Now write code to perform the following tasks:\n",
    "\n",
    " * a. Split `silly` into a list of strings, one per word, using Python's `split()` operation, and save this to a variable called `bland`.\n",
    " \n",
    " * b. Extract the second letter of each word in `silly` and join them into a string, to get `'eoldrnnnna'`.\n",
    " \n",
    " * c. Combine the words in `bland` back into a single string, using `join()`. Make sure the words in the resulting string are separated with whitespace.\n",
    " \n",
    " * d. Print the words of `silly` in alphabetical order, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible', 'in', 'an', 'infuriating', 'way']\n"
     ]
    }
   ],
   "source": [
    "# a\n",
    "\n",
    "silly = 'newly formed bland ideas are inexpressible in an infuriating way'\n",
    "\n",
    "bland = silly.split()\n",
    "print(bland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eoldrnnnna'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b \n",
    "\n",
    "''.join([b[1] for b in bland])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'newly formed bland ideas are inexpressible in an infuriating way'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c\n",
    "\n",
    "' '.join(bland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an\n",
      "are\n",
      "bland\n",
      "formed\n",
      "ideas\n",
      "in\n",
      "inexpressible\n",
      "infuriating\n",
      "newly\n",
      "way\n"
     ]
    }
   ],
   "source": [
    "# d\n",
    "\n",
    "for s in sorted(bland):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 33.\n",
    "\n",
    "◑ The `index()` function can be used to look up items in sequences. For example, `'inexpressible'.index('e')` tells us the index of the first position of the letter `e`.\n",
    "\n",
    " * a. What happens when you look up a substring, e.g. `'inexpressible'.index('re')`?\n",
    " \n",
    " * b. Define a variable `words` containing a list of words. Now use `words.index()` to look up the position of an individual word.\n",
    " \n",
    " * c. Define a variable `silly` as in the exercise above. Use the `index()` function in combination with list slicing to build a list `phrase` consisting of all the words up to (but not including) `in` in `silly`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__a.__*\n",
    "\n",
    "*`.index()` returns the position of the beginning of the string:*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'inexpressible'.index('re')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__b.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\"I'm\", 'too', 'tired', 'think', 'of', 'a', 'more', \n",
    "         'original', 'list']\n",
    "\n",
    "words.index('tired')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__c.__*\n",
    "\n",
    "*Feels a bit naff to do things this way, but oh well... Here's a way to do it in one line of code:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silly = 'newly formed bland ideas are inexpressible in an infuriating way'\n",
    "\n",
    "phrase = [i for i in silly.split()[:silly.split().index('in')]]\n",
    "\n",
    "phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 34.\n",
    "\n",
    "◑ Write code to convert nationality adjectives like *Canadian* and *Australian* to their corresponding nouns *Canada* and *Australia*.\n",
    "\n",
    "*__N.B.:__ The relevant Wikipedia page is now [here](https://en.wikipedia.org/wiki/List_of_adjectival_and_demonymic_forms_for_countries_and_nations \"Countries and Nations\").*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There are rough rules for converting a country's name into its corresponding adjective (and vice versa), but there are many exceptions to this rule (e.g., 'Canada' -> 'Canadian' but 'China' -> 'Chinese; 'Germany' -> 'German' but 'Italy' -> 'Italian'; 'Ecuador' -> 'Ecuadorian' but 'El Salvador' -> 'Salvadoran; etc...).  There are many articles online about this topic, and one from Language Log is available [here](https://languagelog.ldc.upenn.edu/nll/?p=2591 \"Adjectives from country names.\"). I feel it would be a fool's errand to try to hard-code all of these into a function.*\n",
    "\n",
    "*Therefore, I decided to import the list from the Wikipedia page linked above.  I pasted the list into an Excel workbook, did some light editing (e.g., eliminated footnotes and deleted one of the matches for 'Congolese', since one nationality adjective can't have too matches in my function), and saved it as a CSV file, as this will be easier to import into Python. For convenience's sake, I'll upload a copy of this CSV file to my GitHub repo.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = 'C:\\\\Users\\\\matth\\\\Desktop\\\\NLTK'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('Nationalities.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    nats = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Abkhazia', 'Abkhaz,\\xa0Abkhazian'],\n",
       " ['Afghanistan', 'Afghan'],\n",
       " ['Åland Islands', 'Åland Island'],\n",
       " ['Albania', 'Albanian'],\n",
       " ['Algeria', 'Algerian']]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nats[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We need to get rid of `\\xa0` (non-breaking space in Latin1) in the listings.  There are a number of ways we could fix this (including `re.sub` and `string.replace()`), but perhaps the most robust method would be to use `normalize` from the `unicodedata` library.  Strings are immutable, so we'll need to create a new list to do this.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "nats_edited = [[unicodedata.normalize(\"NFKD\", item) for item in pair] for pair in nats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Abkhazia', 'Abkhaz, Abkhazian'],\n",
       " ['Afghanistan', 'Afghan'],\n",
       " ['Åland Islands', 'Åland Island'],\n",
       " ['Albania', 'Albanian'],\n",
       " ['Algeria', 'Algerian']]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nats_edited[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I created a function that will return the name of a country from that country's nationality adjective.  I didn't want to hard code the name of my list into the function, so I had to require it as an argument.  Any suitable list will be a list of sublists, where the first item in the sublist is the country's name, and the second that country's nationality adjective:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_country_name(nationality, l):\n",
    "    \"\"\"\n",
    "    Returns the country name for a given nationality adjective.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    nationality: string with nationality adjective\n",
    "    l:           list of sublists. Country name is the first item in the\n",
    "                 sublist, nationality adjective the second.  Original list\n",
    "                 was modified from the one available at \n",
    "                 https://en.wikipedia.org/wiki/List_of_adjectival_and_demonymic_forms_for_countries_and_nations\n",
    "    \"\"\"\n",
    "    for item in l:\n",
    "        if nationality in item[1]:\n",
    "            return item[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sweden'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_country_name(\"Swedish\", nats_edited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mauritius'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_country_name(\"Mauritian\", nats_edited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gabon'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_country_name(\"Gabonese\", nats_edited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Canada'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_country_name(\"Canadian\", nats_edited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Congo, Democratic Republic of the'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_country_name(\"Congolese\", nats_edited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 35. \n",
    "\n",
    "◑ Read the [LanguageLog post](http://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html \"LanguageLog post\") on phrases of the form *as best as p can* and *as best p can*, where *p* is a pronoun. Investigate this phenomenon with the help of a corpus and the `findall()` method for searching tokenized text described in [3.5](https://www.nltk.org/book/ch03.html#sec-useful-applications-of-regular-expressions \"3.5\"). \n",
    "\n",
    "*I'd like to carry out this analysis with texts provided with NLTK.  However, it turns out none of these phrases is particularly common in these texts.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nothing in the Brown Corpus for either __as best as p can__ or __as best p can__.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bw = nltk.Text(brown.words())\n",
    "\n",
    "\n",
    "bw.findall(r\"<as> <best> <as> <.*> <can>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bw.findall(r\"<as> <best> <.*> <can>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As a sanity check to make sure `.findall()` was working, I tried a simpler phrase to see if that would generate any hits.  I tried __best p can__, and it generated a single hit.  This means that `.findall()` is indeed working, and the phrases in question are not in the Brown Corpus.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best they can\n"
     ]
    }
   ],
   "source": [
    "bw.findall(r\"<best> <.*> <can>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I tried the experiment again with the texts from Project Gutenberg available in the NLTK, and the phrases again were almost nowhere to be found:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma.txt:\n",
      "\t\n",
      "austen-persuasion.txt:\n",
      "\t\n",
      "austen-sense.txt:\n",
      "\t\n",
      "bible-kjv.txt:\n",
      "\t\n",
      "blake-poems.txt:\n",
      "\t\n",
      "bryant-stories.txt:\n",
      "\t\n",
      "burgess-busterbrown.txt:\n",
      "\t\n",
      "carroll-alice.txt:\n",
      "\t\n",
      "chesterton-ball.txt:\n",
      "\t\n",
      "chesterton-brown.txt:\n",
      "\t\n",
      "chesterton-thursday.txt:\n",
      "\t\n",
      "edgeworth-parents.txt:\n",
      "\t\n",
      "melville-moby_dick.txt:\n",
      "\tbest we can\n",
      "milton-paradise.txt:\n",
      "\t\n",
      "shakespeare-caesar.txt:\n",
      "\t\n",
      "shakespeare-hamlet.txt:\n",
      "\t\n",
      "shakespeare-macbeth.txt:\n",
      "\t\n",
      "whitman-leaves.txt:\n",
      "\tbest I can\n"
     ]
    }
   ],
   "source": [
    "for text in nltk.corpus.gutenberg.fileids():\n",
    "    print(text + \":\")\n",
    "    print(\"\\t\", end = '')\n",
    "    nltk.Text(nltk.corpus.gutenberg.words(text)).findall(r\"<best> <.*> <can>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The LanguageLog article cited above tried to investigate the frequency of the two sets of phrases by searching various web engines, so it only seems logical that we'd try to replicate this.  However, treating the web as a corpus is fraught with problems. For one thing, it's well established that search results are far from consistent.  Google's exact method for finding matches is proprietary, so we can never expect to know how precisely matches are formed.  But it is well known that identical searches made from different locations or even with different browsers will return different results.  A further problem is that it's not possible to use regular expressions in Google queries.  While we can use double quotes to search for exact strings, the sheer number of results that are often returned seems to indicate that results that only partially match the string in question are also being returned. Another issue is that the existence of a given phrase online is no guarantee of its correctness.  We could imagine an EFL discussion board where a grammatically-incorrect phrase is repeated again and again, thus driving the number of hits for this phrase.*\n",
    "\n",
    "*Despite these limitations, comparing the number of results for two phrases can still give us a rough idea of which phrase is more common.*\n",
    "\n",
    "*Naturally, going online and manually checking each phrase (and variation thereof) would be a trivial - albeit tedious - task.  As we are studying programming, it only seems naturally that we should seek out a way to automate this and do this programmatically.  Unfortunately, this isn't quite as simple as it might appear to be.  While there are a number of tutorials online for this very task - many of which that use modules that have been outlined in this chapter (e.g., `BeautifulSoup`, `requests`, etc...); unfortunately these processes no longer seem to work. I'm not quite sure why, but if Google has changed the way they present results, that would break most of the methods that have worked up until now.*\n",
    "\n",
    "*Fortunately, I did find one module that does work for my purposes: `googlesearch`, whose documentation is [here](https://python-googlesearch.readthedocs.io/en/latest/ \"googlesearch documentation\"), and whose GitHub repo is [here](https://github.com/MarioVilas/googlesearch \"googlesearch GitHub repo\").  They even have a ready-made function that returns the number of hits for a query.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = [\"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\"]\n",
    "\n",
    "phrases = [\"as best as X can\", \"as best X can\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hits for \"as best as I can\":     14,210,000,000\n",
      "Number of hits for \"as best I can\":        16,070,000,000\n",
      "Number of hits for \"as best as you can\":   23,250,000,000\n",
      "Number of hits for \"as best you can\":      23,000,000,000\n",
      "Number of hits for \"as best as he can\":    8,660,000,000\n",
      "Number of hits for \"as best he can\":       7,700,000,000\n",
      "Number of hits for \"as best as she can\":   4,840,000,000\n",
      "Number of hits for \"as best she can\":      5,070,000,000\n",
      "Number of hits for \"as best as it can\":    22,130,000,000\n",
      "Number of hits for \"as best it can\":       25,270,000,000\n",
      "Number of hits for \"as best as we can\":    19,420,000,000\n",
      "Number of hits for \"as best we can\":       17,860,000,000\n",
      "Number of hits for \"as best as they can\":  11,950,000,000\n",
      "Number of hits for \"as best they can\":     17,180,000,000\n"
     ]
    }
   ],
   "source": [
    "from googlesearch import hits\n",
    "\n",
    "for p in ps:\n",
    "    for ph in phrases:\n",
    "    \n",
    "        check = re.sub('X', p, ph)\n",
    "        no_results = hits(check)\n",
    "        \n",
    "        print(\"Number of hits for \\\"{}\\\": {} {:,}\".format(check, \" \" * (19 - len(check)), no_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There's no clear pattern to the results.  Sometimes \"as best as p can\" return more results, sometimes \"as best p can\" does.  A graph would be easier to interpret than a table, so let's make lists of the values so that we can compose graphs:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = [[], []]\n",
    "\n",
    "for ph in range(len(phrases)):\n",
    "    for p in ps:\n",
    "    \n",
    "    \n",
    "        check = re.sub('X', p, phrases[ph])\n",
    "        no_results = hits(check)\n",
    "        \n",
    "        totals[ph].append(no_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "ph1 = plt.plot(totals[0])\n",
    "ph2 = plt.plot(totals[1])\n",
    "\n",
    "plt.xticks(range(len(ps)), labels = ps)\n",
    "plt.legend((ph1[0], ph2[0]), phrases)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*These results look much different from the ones in the LanguageLog post.  Here, the results look like they are nearly neck and neck. Also observe that the majority of the hits are in the billions.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There is one more online corpus that could be useful for our purposes: Google Books.  Google Books has a Ngram Viewer that allows us to graphically compare the relative frequencies of a number of phrases.  This corpus is not without its problems (a good blog on this can be found [here](http://stanford.edu/~risi/tutorials/absolute_ngram_counts.html \"Google Ngrams Critique\")), but it offers us a quick and easy way to compare frequencies in a huge corpus.*\n",
    "\n",
    "*The blog linked in the previous paragrah also showed a method for replicating the results of web query in a jupyter notebook with `IFrame`.  One drawback of this is that it's only possible to replicate one query per notebook cell, which means it's not possible to iterate through all the queries.  However, to simplify things somewhat, I did create a function that would require us to specify nothing but the pronoun in the two phrases we wanted to compare.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "def compare_best_as(pronoun):\n",
    "    \n",
    "    return IFrame(\"https://books.google.com/ngrams/graph?content=as+best+as+\" + pronoun + \"+can%2C+as+best+\" + pronoun +  \"+can&year_start=1800&year_end=2000&corpus=15&smoothing=3\",\n",
    "       width=1100, height=700 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1100\"\n",
       "            height=\"700\"\n",
       "            src=\"https://books.google.com/ngrams/graph?content=as+best+as+I+can%2C+as+best+I+can&year_start=1800&year_end=2000&corpus=15&smoothing=3\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x278823c1eb8>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_best_as(\"I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1100\"\n",
       "            height=\"700\"\n",
       "            src=\"https://books.google.com/ngrams/graph?content=as+best+as+you+can%2C+as+best+you+can&year_start=1800&year_end=2000&corpus=15&smoothing=3\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x278823e2a58>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_best_as(\"you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1100\"\n",
       "            height=\"700\"\n",
       "            src=\"https://books.google.com/ngrams/graph?content=as+best+as+he+can%2C+as+best+he+can&year_start=1800&year_end=2000&corpus=15&smoothing=3\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x278823e8ef0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_best_as(\"he\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1100\"\n",
       "            height=\"700\"\n",
       "            src=\"https://books.google.com/ngrams/graph?content=as+best+as+she+can%2C+as+best+she+can&year_start=1800&year_end=2000&corpus=15&smoothing=3\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x278823dc7f0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_best_as(\"she\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1100\"\n",
       "            height=\"700\"\n",
       "            src=\"https://books.google.com/ngrams/graph?content=as+best+as+it+can%2C+as+best+it+can&year_start=1800&year_end=2000&corpus=15&smoothing=3\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x278823f6d30>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_best_as(\"it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1100\"\n",
       "            height=\"700\"\n",
       "            src=\"https://books.google.com/ngrams/graph?content=as+best+as+we+can%2C+as+best+we+can&year_start=1800&year_end=2000&corpus=15&smoothing=3\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x27882411668>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_best_as(\"we\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1100\"\n",
       "            height=\"700\"\n",
       "            src=\"https://books.google.com/ngrams/graph?content=as+best+as+they+can%2C+as+best+they+can&year_start=1800&year_end=2000&corpus=15&smoothing=3\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2788243ef98>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_best_as(\"they\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*When we examine the graphs, we see that the phrase \"as best p can\" is much more common than \"as best as p can\".  While Google Books is not a perfect corpus, I feel it's fair to assume that an occurence of a phrase in a written work is more likely to be grammatically correct than one found online, so we should treat the Google Books results as more reliable than the basic Google query.  Furthermore, the fact that the two sets of results are so different should cause us to cast doubt on the idea of using basic Google queries for linguistic analysis.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 36. \n",
    "\n",
    "◑ Study the *lolcat* version of the book of Genesis, accessible as `nltk.corpus.genesis.words('lolcat.txt')`, and the rules for converting text into *lolspeak*. Define regular expressions to convert English words into corresponding lolspeak words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Oh', 'hai', '.', 'In', 'teh', 'beginnin', 'Ceiling', 'Cat', 'maded', 'teh', 'skiez', 'An', 'da', 'Urfs', ',', 'but', 'he', 'did', 'not', 'eated']\n"
     ]
    }
   ],
   "source": [
    "lolcat_gen = nltk.corpus.genesis.words('lolcat.txt')\n",
    "print(lolcat_gen[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Unfortunately, the link for lolcat rules given in the NLTK book (http://www.lolcatbible.com/index.php?title=How_to_speak_lolcat) is offline and appears to no longer be online.  Currently, it does not appear that Wikipedia has a page with the rules of lolspeak.  I was able to find an academic paper that discussed the lolcat phenomenon [here](https://www.academia.edu/25368696/A_comparative_analysis_of_LOLspeak_and_dogespeak \"LOLspeak paper\") (registration required).  The paper did give numerous examples of LOLspeak, but unfortunately there was not an exhaustive grammar.*\n",
    "\n",
    "*Without going into too much detail, let me just state that I think it's extremely difficult to create software that is able to programmatically translate normal English into lolspeak.  As far as I can tell, the vast majority of lolspeak online has been produced by humans who apply the 'rules' of lolspeak 'grammar' arbitrarily. While it is possible to do a lot with regular expressions, we would still need to know the parts of speech of many of the words in the phrase in question in order to 'accurately' translate it into lolspeak.  While this may be technically feasible, it would still represent more work than I'm willing to put in at the moment.*\n",
    "\n",
    "*Although I wasn't able to find a lolspeak 'grammar', I found a lolcat translator [here](https://speaklolcat.com/ \"lolcat translator\"), and from this I was able to reverse engineer a large amount of lolspeak.  To do this, I found a list of the 1,000 most common words in English and a list of common irregular verbs in English.  I put these words in the translator, 100 words at a time (anything more would crash the system), and saved the results in an Excel spreadsheet.  From there I used an Excel function to figure out which words are different in normal English and lolspeak.  I was also able to divine a basic set of rules (e.g., '-er' becomes '-r', 'ss' becomes '-s', etc...).  I then deleted from the spreadsheet those lolspeak words that followed these rules of transformation, so that the only words left were those words whose translations could not be predicted by the lolspeak 'grammar' (e.g., 'that' -> 'DAT', 'was' -> 'WUZ', 'for' -> '4', etc...).*\n",
    "\n",
    "*From there I saved the spreadsheet to a CSV file, which I imported into Python and saved as a `dict`.  In my function `convert_to_lolspeak`, one of the first steps is to replace all of these words which can be found in this dictionary.  From there, I went through a series of `re.sub()` commands to make the transformations commonly seen in lolspeak.  I tried to use a `for` loop to iterate through all the possible transformations (as I did above for exercise 24), but for whatever reason this wouldn't work here.*\n",
    "\n",
    "*This is a very simplified model and is not able to capture all of the nuance of lolspeak.  But the point of this exercise is just practice with regular expressions, and to that end I'm satisfied with the result.*\n",
    "\n",
    "\n",
    "*__N.B.:__ There's a more complete dictionary here - https://github.com/ddribin/lolspeak.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv\n",
    "\n",
    "path = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\"\n",
    "os.chdir(path)\n",
    "\n",
    "d = {}\n",
    "for row in csv.reader(open('lolspeak.csv', 'r')):\n",
    "    key, value = row\n",
    "    d[key] = value\n",
    "\n",
    "\n",
    "def convert_to_lolspeak(text):\n",
    "    \"\"\"\n",
    "    Converts a text to lolspeak.\n",
    "    \"\"\"\n",
    "    \n",
    "    text_list = text.split()\n",
    "\n",
    "    # convert everything to lowercase so we can search for terms \n",
    "    # in the dictionary\n",
    "    text_list = [t.lower() for t in text_list]\n",
    "\n",
    "    # replace those terms that can be found in the lolspeak dictionary\n",
    "    for i in range(len(text_list)):\n",
    "        if text_list[i] in d:\n",
    "            text_list[i] = d[text_list[i]]\n",
    "\n",
    "    new_list = []\n",
    "\n",
    "    # use regular expressions for the rest\n",
    "    for t in text_list:\n",
    "        # the values in the lolspeak dictionary are all uppercase words,\n",
    "        # so the next conditional will run only on words that haven't\n",
    "        # been modified\n",
    "        if t.islower():\n",
    "            if len(t) > 3 and t[-1] != 'd':\n",
    "                t = re.sub('ed', 'd', t)\n",
    "            t = re.sub('ear', 'er', t)\n",
    "            t = re.sub(r'ight', 'ite', t)\n",
    "            t = re.sub(r'[^s]s\\b', 'z', t)\n",
    "            t = re.sub(r'ss', 's', t)\n",
    "            t = re.sub(r'tion|sion', 'shun', t)\n",
    "            t = re.sub(r'ing', 'in', t)\n",
    "            t = re.sub(r'ture', 'chur', t)\n",
    "            t = re.sub(r'ove', 'oov', t)\n",
    "            t = re.sub(r'er', 'r', t)\n",
    "            \n",
    "            t = re.sub(r'ee', 'e', t)\n",
    "\n",
    "        # for stylistic reasons, everything will be returned in uppercase\n",
    "        new_list.append(t.upper())\n",
    "\n",
    "    # there's just one grammatical exception I'm going to deal with \n",
    "    # in this function: converting \"can I\" to \"I CAN\"\n",
    "    for i in range(len(new_list) - 1):\n",
    "        if new_list[i] == \"CAN\" and new_list[i + 1] == \"I\":\n",
    "            new_list[i], new_list[i + 1] = new_list[i + 1], new_list[i]\n",
    "\n",
    "    return(' '.join(new_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I CAN TEH CHEZBURGR ?\n"
     ]
    }
   ],
   "source": [
    "test = \"can i the cheeseburger ?\"\n",
    "\n",
    "print(convert_to_lolspeak(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIS IZ JUS ANOTHR TEST 2 C HOW LOLSPEAK RESPONZ IN DIS SITUASHUN\n"
     ]
    }
   ],
   "source": [
    "test = 'this is just another test to see how lolspeak responds in this situation'\n",
    "\n",
    "print(convert_to_lolspeak(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOZ AN PEEPS SEM A BIT STRANGE IN LOLSPEAK\n"
     ]
    }
   ],
   "source": [
    "test = 'dogs and people seem a bit strange in lolspeak'\n",
    "\n",
    "print(convert_to_lolspeak(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 37.\n",
    "\n",
    "◑ Read about the `re.sub()` function for string substitution using regular expressions, using `help(re.sub)` and by consulting the further readings for this chapter. Use `re.sub` in writing code to remove HTML tags from an HTML file, and to normalize whitespace.\n",
    "\n",
    "*Below is the first result I got when I googled 'simple HTML example'.  The site was at [this address](http://help.websiteos.com/websiteos/example_of_a_simple_html_page.htm \"simple html example\").*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"\n",
    "<HTML>\n",
    "\n",
    "<HEAD>\n",
    "\n",
    "<TITLE>Your Title Here</TITLE>\n",
    "\n",
    "</HEAD>\n",
    "\n",
    "<BODY BGCOLOR=\"FFFFFF\">\n",
    "\n",
    "<CENTER><IMG SRC=\"clouds.jpg\" ALIGN=\"BOTTOM\"> </CENTER>\n",
    "\n",
    "<HR>\n",
    "\n",
    "<a href=\"http://somegreatsite.com\">Link Name</a>\n",
    "\n",
    "is a link to another nifty site\n",
    "\n",
    "<H1>This is a Header</H1>\n",
    "\n",
    "<H2>This is a Medium Header</H2>\n",
    "\n",
    "Send me mail at <a href=\"mailto:support@yourcompany.com\">\n",
    "\n",
    "support@yourcompany.com</a>.\n",
    "\n",
    "<P> This is a new paragraph!\n",
    "\n",
    "<P> <B>This is a new paragraph!</B>\n",
    "\n",
    "<BR> <B><I>This is a new sentence without a paragraph break, in bold italics.</I></B>\n",
    "\n",
    "<HR>\n",
    "\n",
    "</BODY>\n",
    "\n",
    "</HTML>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Your Title Here Link Name is a link to another nifty site This is a Header This is a Medium Header Send me mail at support@yourcompany.com. This is a new paragraph! This is a new paragraph! This is a new sentence without a paragraph break, in bold italics. '"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting rid of tages\n",
    "example = re.sub(r'<.*?>', '', example)\n",
    "\n",
    "# normalizing whitespace\n",
    "example = re.sub(r'\\s+', ' ', example)\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 38. \n",
    "\n",
    "★ An interesting challenge for tokenization is words that have been split across a line-break. E.g. if *long-term* is split, then we have the string `long-\\nterm`.\n",
    "\n",
    " * a. Write a regular expression that identifies words that are hyphenated at a line-break. The expression will need to include the `\\n` character.\n",
    " \n",
    " * b. Use `re.sub()` to remove the `\\n` character from these words.\n",
    " \n",
    " * c. How might you identify words that should not remain hyphenated once the newline is removed, e.g. `'encyclo-\\npedia'`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['long-\\nterm', 'encyclo-\\npedia']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a\n",
    "\n",
    "test = \"I'm trying to find the long-\\nterm solution in this encyclo-\\npedia.\"\n",
    "re.findall(r'\\b\\w*-\\n\\w*\\b', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm trying to find the long-term solution.\""
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#b \n",
    "\n",
    "re.sub(r'-\\n', '-', test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The easiest to accomplish part __c.__ is to try to perform some sort of look up of the hyphenated word.  If the hyphenated word is in the word list, the hyphen stays in; otherwise it's removed.*\n",
    "\n",
    "*Unfortunately, the English wordlist provided doesn't have many hyphenated words:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jean-christophe\n",
      "jean-pierre\n"
     ]
    }
   ],
   "source": [
    "for w in wordlist:\n",
    "    if '-' in w:\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*So we'll have to create one using one of the corpora.  I'm sure the Brown Corpus has hyphenated words, so let's use that:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_words = sorted(set([w.lower() for w in nltk.corpus.brown.words()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The function below uses regular expressions to find all the hyphenated words with line breaks in a text.  It iterates through all the hits, checking them against a master word list.  If the hyphenated form is in the word list, the hyphen stays in; otherwise it is removed.*  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_line_break(text, wordlist):\n",
    "    \"\"\"\n",
    "    Finds hyphenated words with line breaks in a text and \n",
    "    removes hyphens if they are the results of a split on \n",
    "    a line break.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    text:     String which may have hyphens.\n",
    "    wordlist: List of accepted word forms.\n",
    "    \"\"\"\n",
    "    \n",
    "    # the function will only run if there's a hyphen in the string.\n",
    "    if '-' in text:\n",
    "        \n",
    "        # find all the hyphenated words\n",
    "        hyphen = re.findall(r'\\b\\w*-\\n\\w*\\b', text)\n",
    "        \n",
    "        # perform lookup for each hyphenated word\n",
    "        for h in hyphen:\n",
    "            # remove the newspace to get the hyphenated form\n",
    "            check = re.sub(r'-\\n', '-', h)\n",
    "            \n",
    "            # keep the hyphen if the hyphenated form is in the wordlist\n",
    "            if check in wordlist:\n",
    "                text =  re.sub(h, check, text)\n",
    "            # otherwise remove it\n",
    "            else:\n",
    "                text = re.sub(h, re.sub(r'-\\n', '', h), text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm trying to find the long-term solution.\""
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"I'm trying to find the long-\\nterm solution.\"\n",
    "\n",
    "check_line_break(test, brown_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The word encyclopedia shouldn't have a hyphen.\""
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"The word encyclo-\\npedia shouldn't have a hyphen.\"\n",
    "\n",
    "check_line_break(test, brown_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The function won't break if there aren't any hyphens.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test.'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"This is a test.\"\n",
    "check_line_break(test, brown_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*And it should find all the hyphenated words with line breaks inside of them:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm trying to find the long-term solution in this encyclopedia.\""
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"I'm trying to find the long-\\nterm solution in this encyclo-\\npedia.\"\n",
    "\n",
    "check_line_break(test, brown_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 39. \n",
    "\n",
    "★ Read the Wikipedia entry on *Soundex*. Implement this algorithm in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I've basically recreated the algorithm as outlined in the Wikipedia entry.  There's one place in the Wikipedia entry where the explanation is a little confusing (if not downright inaccurate): In the first step, we want to drop only the non-initial occurrences of \"h\" and \"w\".  We want to replace vowels with zeros in case they come between two instances of consonants with identical values.  In a later stage, consecutive duplicate consonant values will be merged into one, unless they occur on either side of a vowel.  Therefore, we want to retain the zeros until the previous step is finished.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soundex_digit(chr):\n",
    "    \"\"\"\n",
    "    Returns a numerical digit based on a consonant's value\n",
    "    in the Soundex algorithm.\n",
    "    \"\"\"\n",
    "        \n",
    "    cs = ['bfpv', 'cgjkqsxz', 'dt', 'l', 'mn', 'r']\n",
    "    for i in range(len(cs)):\n",
    "        if chr.lower() in cs[i]:\n",
    "            return str(i + 1)\n",
    "        \n",
    "    return \"0\"\n",
    "\n",
    "def remove_consecutive_duplicates(text):\n",
    "    \"\"\"\n",
    "    Returns a string with consecutive duplicates removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    new_text = text[0]\n",
    "    \n",
    "    for i in text[1:]:\n",
    "        if i != new_text[-1]:\n",
    "            new_text += i\n",
    "            \n",
    "    return new_text\n",
    "\n",
    "def create_soundex(name):\n",
    "    \"\"\"\n",
    "    Uses the Soundex algorithm to create an index for a name.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with the first letter.  Capitalize this letter\n",
    "    # if it's not already capitalized.\n",
    "    soundex = name[0].upper()\n",
    "    \n",
    "    # Remove all occurrences of h and w\n",
    "    name = re.sub(r'[hw]', '', name[1:])\n",
    "    \n",
    "    # Replace vowels with zeros. After we assign values to consonants,\n",
    "    # consecutive identical values will be removed, except in \n",
    "    # the case that they are on either side of a vowel. So the zeros will\n",
    "    # be retained until this point.  After this they will be deleted.\n",
    "    name = re.sub(r'[aeiouy]', '0', name)\n",
    "    \n",
    "    # Replace all consonants with digits using get_soundex_digit\n",
    "    new_name = ''\n",
    "    for n in name:\n",
    "        new_name += (re.sub(r'[^0]', get_soundex_digit(n), n))\n",
    "\n",
    "    name = new_name\n",
    "    \n",
    "    # Replace all adjacent identical digits with one digit\n",
    "    name = remove_consecutive_duplicates(name)\n",
    "\n",
    "    # First digit can't be of the same group as the beginning character\n",
    "    if get_soundex_digit(soundex) == name[0]:\n",
    "        name = name[1:]\n",
    "    \n",
    "    # Remove zeros\n",
    "    name = re.sub(r'0', '', name)\n",
    "    \n",
    "    # In case everything after the first letter is a vowel\n",
    "    if len(name) == 0:\n",
    "        return soundex + ('000')\n",
    "        \n",
    "    soundex += name\n",
    "    \n",
    "    # Index can only be four characters long\n",
    "    if len(soundex) == 4:\n",
    "        return soundex\n",
    "    elif len(soundex) > 4:\n",
    "        return soundex[:4]\n",
    "    else:\n",
    "        # Add zeros if the index is shorter than 4\n",
    "        while len(soundex) < 4:\n",
    "            soundex += \"0\"\n",
    "\n",
    "        return soundex\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*These were the examples given on the Wikipedia entry for Soundex.  My code returned the same results as the Wikipedia entry.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R163', 'R163', 'R150', 'A261', 'A261', 'T522', 'P236', 'H555']\n"
     ]
    }
   ],
   "source": [
    "names = [\"Robert\", \"robert\", \"Rubin\", \"Ashcraft\", \"Ashcroft\", \"Tymczak\", \n",
    "         \"Pfister\", \"Honeyman\"]\n",
    "\n",
    "print([create_soundex(n) for n in names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Found a bunch of examples of Soundex indexing [at this blog here](https://sistemanalize.wordpress.com/2017/12/21/big-data-phonetic-similarity-soundex/ \"Soundex examples\").  My code returned the same values as those on the blog.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F634', 'J525', 'J523', 'S530', 'M235', 'M600', 'A356', 'S560', 'A560', 'M200']\n"
     ]
    }
   ],
   "source": [
    "names = [\"Fairdale\", \"Jonson\", \"Jonston\", \"Smith\", \"MacDonald\", \"Mayer\",\n",
    "         \"Admire\", \"Smire\", \"Amire\", \"Mayes\"]\n",
    "\n",
    "print([create_soundex(n) for n in names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M000', 'M620', 'M652', 'M620', 'M560', 'M653', 'M610', 'M625']\n"
     ]
    }
   ],
   "source": [
    "names = [\"Mayo\", \"Meiers\", \"Mierins\", \"Miers\", \"Mimre\", \"Miranda\", \n",
    "         \"Mirfe\", \"Mirissimo\"]\n",
    "\n",
    "print([create_soundex(n) for n in names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 40. \n",
    "\n",
    "★ Obtain raw texts from two or more genres and compute their respective reading difficulty scores as in the earlier exercise on reading difficulty. E.g. compare ABC Rural News and ABC Science News (`nltk.corpus.abc`). Use Punkt to perform sentence segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = nltk.corpus.abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Instead of reinventing the wheel, I'm just going to redo the function `get_brown_ari` that I wrote above for question 29.  The only difference worth mentioning is that we have to add the name of the corpus as one of the arguments.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ari(corpus, cat):\n",
    "    \"\"\"\n",
    "    Returns the Automated Readability Index (ARI) of a given\n",
    "    category of a NLTK corpus.\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate total letters in the category\n",
    "    total_letters = 0\n",
    "    for w in corpus.words(fileids = cat):\n",
    "        total_letters += len(w)\n",
    "    \n",
    "    # calculate average number of letters per word\n",
    "    mu_w = total_letters/len(corpus.words(fileids = cat))\n",
    "    \n",
    "    # calculate average number of words per sentence\n",
    "    mu_s = len(corpus.words(fileids = cat)) / len(corpus.sents(fileids = cat))\n",
    "    \n",
    "    return (4.71 * mu_w) + (0.5 * mu_s) - 21.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the ABC Corpus, the Automated Readability Index (ARI) for the category:\n",
      "\n",
      "                                             ...\"Rural\" is 12.3487.\n",
      "                                             ...\"Science\" is 12.5276.\n"
     ]
    }
   ],
   "source": [
    "print(\"In the ABC Corpus, the Automated Readability Index (ARI) for the category:\\n\")\n",
    "\n",
    "for f in abc.fileids():\n",
    "    ari = get_ari(abc, f)\n",
    "    f = re.sub('.txt', '', f)\n",
    "    print(\"{:45}...\\\"{}\\\" is {:.4f}.\".format(\"\",f.title(), ari) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 41.\n",
    "\n",
    "★ Rewrite the following nested loop as a nested list comprehension:\n",
    "\n",
    "```\t\n",
    ">>> words = ['attribution', 'confabulation', 'elocution',\n",
    "...          'sequoia', 'tenacious', 'unidirectional']\n",
    ">>> vsequences = set()\n",
    ">>> for word in words:\n",
    "...     vowels = []\n",
    "...     for char in word:\n",
    "...         if char in 'aeiou':\n",
    "...             vowels.append(char)\n",
    "...     vsequences.add(''.join(vowels))\n",
    ">>> sorted(vsequences)\n",
    "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['attribution', 'confabulation', 'elocution',\n",
    "         'sequoia', 'tenacious', 'unidirectional']\n",
    "\n",
    "sorted(set([''.join([char for char in word if char in 'aeiou']) for word in words]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 42. \n",
    "\n",
    "★ Use WordNet to create a semantic index for a text collection. Extend the concordance search program in 3.6, indexing each word using the offset of its first synset, e.g. `wn.synsets('dog')[0].offset` (and optionally the offset of some of its ancestors in the hypernym hierarchy).\n",
    "\n",
    "*This looks to be very difficult...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2084071"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog')[0].offset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
