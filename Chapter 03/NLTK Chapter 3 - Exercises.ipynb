{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Chapter 3\n",
    "\n",
    "## Processing Raw Text\n",
    "\n",
    "*The html version of this chapter in the NLTK book is available [here](https://www.nltk.org/book/ch03.html#exercises \"Ch03 Exercises\").*\n",
    "\n",
    "### 8   Exercises\n",
    "\n",
    "###### 1. \n",
    "\n",
    "☼ Define a string `s = 'colorless'`. Write a Python statement that changes this to \"colourless\" using only the slice and concatenation operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'colourless'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'colorless'\n",
    "s = s[:4] + 'u' + s[4:]\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.\n",
    "\n",
    "☼ We can use the slice notation to remove morphological endings on words. For example, `'dogs'[:-1]` removes the last character of `dogs`, leaving `dog`. Use slice notation to remove the affixes from these words (we've inserted a hyphen to indicate the affix boundary, but omit this from your strings): `dish-es`, `run-ning`, `nation-ality`, `un-do`, `pre-heat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dish', 'run', 'nation', 'un', 'pre']\n"
     ]
    }
   ],
   "source": [
    "affixed = [('dishes', 2), \n",
    "           ('running', 4),\n",
    "           ('nationality', 5),\n",
    "           ('undo', 2),\n",
    "           ('preheat', 4)]\n",
    "\n",
    "print([s[:-a] for s, a in affixed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.\n",
    "\n",
    "☼ We saw how we can generate an `IndexError` by indexing beyond the end of a string. Is it possible to construct an index that goes too far to the left, before the start of the string?\n",
    "\n",
    "*Yes.  I'm not going to run the code in my notebook, because then the cells below this one wouldn't run.*\n",
    "\n",
    "```\n",
    ">>>trial = \"trial\"\n",
    ">>>for i in range(1, len(trial) + 2):\n",
    ">>>    print(trial[-i])\n",
    "    \n",
    "l\n",
    "a\n",
    "i\n",
    "r\n",
    "t\n",
    "---------------------------------------------------------------------------\n",
    "IndexError                                Traceback (most recent call last)\n",
    "<ipython-input-21-98077138b076> in <module>\n",
    "      1 trial = \"trial\"\n",
    "      2 for i in range(1, len(trial) + 2):\n",
    "----> 3     print(trial[-i])\n",
    "\n",
    "IndexError: string index out of range\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. \n",
    "\n",
    "☼ We can specify a \"step\" size for the slice. The following returns every second character within the slice: `monty[6:11:2]`. It also works in the reverse direction: `monty[10:5:-2]` Try these for yourself, then experiment with different step values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tittitie tírýhkeee řltl řstittitie tírýhsřc.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A Czech tongue twister:\n",
    "tt = \"Třistatřiatřicet stříbrných křepelek přeletělo přes třistatřiatřicet stříbrných střech.\"\n",
    "\n",
    "# Every other letter\n",
    "tt[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.cřshýrít eitittitsř ltlř eeekhýrít eitittiT'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every other letter from the end\n",
    "tt[::-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tstaittbý el etoř iaiřesínhtc'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every third letter\n",
    "tt[::3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You get the point...*\n",
    "\n",
    "##### 5. \n",
    "\n",
    "☼ What happens if you ask the interpreter to evaluate `monty[::-1]`? Explain why this is a reasonable result.\n",
    "\n",
    "*It prints the word backwards.  It's simply printing from the end by steps of -1:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'murder'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"redrum\"[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.\n",
    "\n",
    "☼ Describe the class of strings matched by the following regular expressions.\n",
    "\n",
    "a. `[a-zA-Z]+`\n",
    "\n",
    "b. `[A-Z][a-z]*`\n",
    "\n",
    "c. `p[aeiou]{,2}t`\n",
    "\n",
    "d. `\\d+(\\.\\d+)?`\n",
    "\n",
    "e. `([^aeiou][aeiou][^aeiou])*`\n",
    "\n",
    "f. `\\w+|[^\\w\\s]+`\n",
    "\n",
    "Test your answers using `nltk.re_show()`.\n",
    "\n",
    "*__a.__ `[a-zA-Z]+` will match anything alphabetical:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{cAMELCASE} 6186258313 {hybr}1{d}\n"
     ]
    }
   ],
   "source": [
    "import nltk, re\n",
    "\n",
    "nltk.re_show(r'[a-zA-Z]+', \"cAMELCASE 6186258313 hybr1d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>__b.__ `[A-Z][a-z]*` will match words beginning with uppercase letters, or any uppercase letters in other positions:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{I} think words beginning with {Uppercase} {Letters} will be matched, or any uppercase letters found in o{T}{H}{E}{R} positions.\n"
     ]
    }
   ],
   "source": [
    "test = 'I think words beginning with Uppercase Letters will be matched, ' \\\n",
    "       'or any uppercase letters found in oTHER positions.'\n",
    "\n",
    "nltk.re_show(r'[A-Z][a-z]*', test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__c.__ `p[aeiou]{,2}t` will match all words with __p__, up to two vowels, and a letter __t__.  This is a lot of words: In the wordlist we've been using in this chapter, this RegExp will return nearly 7,000 words, since any word with __pt__ will be a match.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6978"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist = [w.lower() for w in nltk.corpus.words.words('en')]\n",
    "len([w for w in wordlist if re.search(r'p[aeiou]{,2}t', w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abaptiston', 'abepithymia', 'ableptical', 'ableptically', 'abrupt', 'abruptedly', 'abruption', 'abruptly', 'abruptness', 'absorpt', 'absorptance', 'absorptiometer', 'absorptiometric', 'absorption', 'absorptive', 'absorptively', 'absorptiveness', 'absorptivity', 'absumption', 'acalypterae']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in wordlist if re.search(r'p[aeiou]{,2}t', w)][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If we add the `^` and <code>\\$</code> operators, we'll instead end up with all 3-letter words beginning and ending with __p__ and __t__ with one vowel in the middle, or all 4-letter words beginning and ending with __p__ and __t__ with two vowels in the middle:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pat', 'pat', 'paut', 'peat', 'pet', 'piet', 'piet', 'pit', 'poet', 'poot', 'pot', 'pout', 'put']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in wordlist if re.search(r'^p[aeiou]{,2}t$', w)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__d.__ `\\d+(\\.\\d+)?` will match any numbers and decimal points, no matter how many numbers are to the left/right of the decimal.  It will not match dashes, dollar signs, or any other symbol associated with number.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1234}\n",
      "{12.34}\n",
      "example {123.4} in a string\n",
      "{1}-{234}\n",
      "{12},{4}\n",
      "${12.34}\n"
     ]
    }
   ],
   "source": [
    "test = ['1234', '12.34', 'example 123.4 in a string', '1-234', '12,4', '$12.34']\n",
    "for t in test:\n",
    "    nltk.re_show(r'\\d+(\\.\\d+)?', t) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If we use two decimals, the second is ignored:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.23}.{4}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'\\d+(\\.\\d+)?', '1.23.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can alter that by changing the `?` to a `+`:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.23.4}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'\\d+(\\.\\d+)+', '1.23.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>__e.__ `([^aeiou][aeiou][^aeiou])*` will match any non-vowel\\vowel\\non-vowel combination, no matter how many times it's repeated.  White spaces are considered non-vowels, so a string such as `to ` would match.  `nltk.re_show()` behaves quite strangely with this RegExp - a string like `\"baab\"` would return `{}b{}a{}a{}b{}`.  However, I have evaluated this RegExp with online evaluators (such as [this one](https://regexr.com/ \"regexr.com\"), and there the responsive is as expected:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{babbabbabbab}{}a{pap}{}a{}\n"
     ]
    }
   ],
   "source": [
    "string = \"babbabbab\" \\\n",
    "         \"babapapa\"\n",
    "nltk.re_show(r'([^aeiou][aeiou][^aeiou])*', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}b{}a{}a{}b{}\n"
     ]
    }
   ],
   "source": [
    "string = \"baab\"\n",
    "nltk.re_show(r'([^aeiou][aeiou][^aeiou])*', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__f.__ `\\w+|[^\\w\\s]+` will match either any alphanumeric string of any length, or a string of any length that does not contain alphanumeric characters or whitespace - i.e., all punctuation and any other non-whitespace/non-alphanumeric characters:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{This} {RegExp} {needs} {a} {fairly} {long} {string} {to} {show} {what} {it} {can} {%#$^%&*} {do}{.}\n"
     ]
    }
   ],
   "source": [
    "string = \"This RegExp needs a fairly long string to show what it can %#$^%&* do.\"\n",
    "nltk.re_show(r'\\w+|[^\\w\\s]+', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.\n",
    "\n",
    "*☼ Write regular expressions to match the following classes of strings:*\n",
    "\n",
    " + *__a.__ A single determiner (assume that __a__, __an__, and __the__ are the only determiners).*\n",
    " + <i>__b.__ An arithmetic expression using integers, addition, and multiplication, such as `2*3+8`.</i>\n",
    " \n",
    "*__a.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think {a} relevant string like {the} one here is {an} example of what we need.\n"
     ]
    }
   ],
   "source": [
    "string = \"I think a relevant string like the one here is an example of what we need.\"\n",
    "nltk.re_show(r'\\b[Aa]n?\\b|\\b[Tt]he\\b', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__b.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2 * 3 + 8}\n"
     ]
    }
   ],
   "source": [
    "string = \"2 * 3 + 8\"\n",
    "nltk.re_show(r'(\\d|[+*= ])+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{11 + 4 * 2}\n"
     ]
    }
   ],
   "source": [
    "string = \"11 + 4 * 2\"\n",
    "nltk.re_show(r'(\\d|[+*= ])+', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.\n",
    "\n",
    "\n",
    "☼ Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML markup removed. Use `from urllib import request`  and then `request.urlopen('http://nltk.org/').read().decode('utf8')` to access the contents of the URL.\n",
    "\n",
    "*This chapter of the NLTK book dealt with removing HTML tags, but didn't really touch on removing the style & scripts tags that are present in most pages today. [This Stack Overflow discussion](https://stackoverflow.com/questions/30565404/remove-all-style-scripts-and-html-tags-from-an-html-page#answers \"Removing Style, Scripts, and HTML tags\") has a good discussion on how to use BeautifulSoup to do that, specifically with the `extract` and `stripped_strings` methods.  The code below borrows heavily from [this answer in the above Stack Overflow discussion](https://stackoverflow.com/a/30565597 \"Removing Style, Scripts, and HTML tags - answer\"):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "from unicodedata import normalize\n",
    "\n",
    "def return_URL_contents(url):\n",
    "    html = request.urlopen(url).read().decode('utf8')\n",
    "    raw = BeautifulSoup(html, 'html.parser')\n",
    "    for r in raw(['script', 'style']):\n",
    "        r.extract() # remove tags\n",
    "    \n",
    "    text = ' '.join(raw.stripped_strings) # retrieve tag content\n",
    "    \n",
    "    return normalize('NFKD', text) # normalize escape sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What Virtual Reality Can Teach a Driverless Car - The New York Times Sections SEARCH Skip to content Skip to site index Business Log In Log In Today’s Paper Business | What Virtual Reality Can Teach a Driverless Car Subscribe Log In Advertisement Supported by What Virtual Reality Can Teach a Driverless Car By Cade Metz Oct. 29, 2017 SAN FRANCISCO — As the computers that operate driverless cars digest the rules of the road, some engineers think it might be nice if they can learn from mistakes made in virtual reality rather than on real streets. Companies like Toyota, Uber and Waymo have discussed at length how they are testing autonomous vehicles on the streets of Mountain View, Calif., Phoenix and other cities. What is not as well known is that they are also testing vehicles inside computer simulations of these same cities. Virtual cars, equipped with the same software as the real thing, spend thousands of hours driving their digital worlds. Think of it as a way of identifying flaws in the way the cars operate without endangering real people. If a car makes a mistake on a simulated drive, engineers can tweak its software accordingly, laying down new rules of behavior. On Monday, Waymo, the autonomous car company that spun out of Google, is expected to show off its simulator tests when it takes a group of reporters to its secretive testing center in California’s Central Valley. Researchers are also developing methods that would allow cars to actually learn new behavior from these simulations, gathering skills more quickly than human engineers could ever lay them down with explicit software code. “Simulation is a tremendous thing,” said Gill Pratt, chief executive of the Toyota Research Institute, one of the artificial intelligence labs exploring this kind of virtual training for autonomous vehicles and other robotics. Image In virtual reality simulations, driverless cars operated by Waymo can practice driving the same intersection, in the same driving conditions, tho'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.nytimes.com/2017/10/29/business/virtual-reality-driverless-cars.html?module=inline\"\n",
    "\n",
    "return_URL_contents(url)[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Guido van Rossum - Wikipedia Guido van Rossum From Wikipedia, the free encyclopedia Jump to navigation Jump to search Dutch programmer and creator of Python In this Dutch name , the family name is van Rossum , not Rossum . Guido van Rossum Guido van Rossum at the Dropbox headquarters in 2014 Born ( 1956-01-31 ) 31 January 1956 (age 63) [1] Haarlem , Netherlands [2] [3] Residence Belmont, California , U.S. Nationality Dutch Alma mater University of Amsterdam Occupation Computer programmer, author Employer Dropbox [4] Known for Creating the Python programming language Spouse(s) Kim Knapp ( m. 2000) Children 1 [5] Awards Award for the Advancement of Free Software (2001) Website gvanrossum .github .io Guido van Rossum ( Dutch: [ˈɣido vɑn ˈrɔsʏm, -səm] ; born 31 January 1956) is a Dutch programmer best known as the author of the Python programming language , for which he was the \" Benevolent dictator for life \" (BDFL) until he stepped down from the position in July 2018. [6] [7] He is currently a member of the Python Steering Council. [8] Contents 1 Life and education 2 Work 2.1 Python 2.2 Computer Programming for Everybody 2.3 Mondrian 2.4 Dropbox 3 Awards 4 References 5 External links Life and education [ edit ] Van Rossum was born and raised in the Netherlands , where he received a master\\'s degree in mathematics and computer science from the University of Amsterdam in 1982. He has a brother, Just van Rossum, who is a type designer and programmer who designed the typeface used in the \"Python Powered\" logo. [ citation needed ] Guido lives in Belmont , California, with his wife, Kim Knapp, [9] and their son. [10] [11] [12] According to his home page and Dutch naming conventions , the \" van \" in his name is capitalized when he is referred to by surname alone, but not when using his first and last name together. [13] Work [ edit ] While working at the Centrum Wiskunde & Informatica (CWI), Van Rossum wrote and contributed a glob() routine to BSD Unix in 1986 [14] [15] and h'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Guido_van_Rossum\"\n",
    "\n",
    "return_URL_contents(url)[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. \n",
    "\n",
    "☼ Save some text into a file `corpus.txt`. Define a function `load(f)` that reads from the file named in its sole argument, and returns a string containing the text of the file.\n",
    "\n",
    " + a. Use `nltk.regexp_tokenize()` to create a tokenizer that tokenizes the various kinds of punctuation in this text. Use one multi-line regular expression, with inline comments, using the verbose flag `(?x)`.\n",
    " \n",
    " + b. Use `nltk.regexp_tokenize()` to create a tokenizer that tokenizes the following kinds of expression: monetary amounts; dates; names of people and organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\chapter03\"\n",
    "\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.nytimes.com/2017/10/22/technology/artificial-intelligence-experts-salaries.html?action=click&module=RelatedCoverage&pgtype=Article&region=Footer'\n",
    "\n",
    "text = return_URL_contents(url)\n",
    "\n",
    "with open('corpus.txt', 'w', encoding = \"utf-8\") as f:\n",
    "    f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(f):\n",
    "    text = open(f, encoding = \"utf-8\")\n",
    "    raw = text.read()\n",
    "    \n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt = load('corpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__a.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '.', '.', '.', '.', '.', ',', '.', '.', ',', ':', '.', '.', '.', ',', '.', ',', '.', '.', '.', ',', '.', '.', ',', ',', ',', ',', '.', '.', '.', '.', '.', ',', '.', '.', '.', '.', ',', ',', ',', ',', '.', ',', ',', '.', ',', '.', '.', '.', '.', ',', ',', '.', '.', '.', '.', '.', '.', ',', '.', ',', ',', '.', '.', '.', '.', ',', ',', ',', ',', '.', ',', ',', ',', ',', '.', '.', '.', '.', ',', ',', ',', '.', ',', ',', '.', ',', '.', ',', ',', ',', '.', '.', '.', ',', ',', '.', ',', '.', ',', ',', '.', ',', '.', ',', ',', ',', '.', '.', '.', ',', '.', ',', '.', '.', '.', '.', ',', '.', '.', '.', ',', '.', ',', ',', '.', '.', '(', ',', ',', ')', '.', ',', '.', ',', ',', '.', '.', ',', '.', '.', '.', ',', '.', '.', '.', ',', ',', '.', ',', '.', '.', ',', '.', ',', '.', '.', ',', '.', ',', ',', ',', ',', '.', '.', '.', ',', ',', '.', '.', ',', '.', ',', '.', ',', '.', '.', ',', ',', '.', '.', ',', '.', '.', '.', ',', ',', ':', '.', '.', '.', '.', '.', '.', '.', '.', \"'\", \"'\", ':', \"'\", ':', '.', '.', '.', '.', '.', \"'\", \"'\", ':', \"'\", ':', '.', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'''(?x)\n",
    "    [][.,;\"'?!():_-`] # finds punctuation\n",
    "'''\n",
    "\n",
    "print(nltk.regexp_tokenize(nyt, pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__b.__ Using regular expressions to extract information such as proper names - which can take numerous forms - is wrought with problems, and the regular expressions below are far from perfect.  It could very well be that the point of this exercise was to demonstrate just how difficult this approach is.*\n",
    "\n",
    "*Proper names are almost always capitalized; but so are many other words, and this approach alone is practically guaranteed to generate false positives/negatives.  I tried to limit the number of false positives by only examining sequences of two or more strings that began with uppercase letters; but that would eliminate any companies with a one-word name, as well as any references to a person using only his/her first/last name.  Another issue is that this returns strings where multiple words are in uppercase, such as titles.*\n",
    "\n",
    "*Monetary values are more straightforward, but I noticed that large amounts are often written out, so I created a special use case for this.  I didn't bother with all the different currency symbols for commonly-used currencies and instead made do with using the dollar sign.*\n",
    "\n",
    "*Finally, dates have a variety of formats around the world, but for the purpose of this exercise I only focussed on those formats in common use in the U.S., i.e. numerical dates (__10/16/19__ or __10/16/2019__) or literal dates (__Oct. 16, 2019__ or __October 16, 2019__.)*\n",
    "\n",
    "*While researching this question, I came across several methods that looked interesting, but which I did not pursue because of time limitations.  One was finding first names in a text by using the `names` corpus in NLTK.  It's easy to see how this would work; but I didn't explore this, because it seems the focus of this unit is regular expressions.  An additional issue would be the lack of corpora for last and company names.  Another method that looked interesting was using Python's `datetime` module to deal with dates.  But, as was the case above, time limitations prevented me from trying this.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tech Giants Are Paying Huge Salaries', 'Scarce A.', 'I. Talent', 'The New York Times Technology', 'Tech Giants Are Paying Huge Salaries', 'Scarce A.', 'I. Talent Subscribe Log In Image Credit Credit Christina Chung Sections Skip', 'Tech Giants Are Paying Huge Salaries', 'Scarce A.', 'I. Talent Nearly', 'Credit Credit Christina Chung Supported', 'By Cade Metz Oct', '22', '2017', 'Silicon Valley', 'I. Tech', 'Typical A.', '$300,000', '$500,000', 'Anthony Levandowski', '2007', '$120 million', 'Image Luke Zettlemoyer', 'Allen Institute', 'Artificial Intelligence', 'Credit Kyle Johnson', 'The New York Times Salaries', 'National Football League', 'Christopher Fernandez', 'Silicon Valley', '10,000', 'Andrew Moore', 'Carnegie Mellon University', '$650 million', '2014', '50', '400', '$138 million', '$345,000', 'Jessica Cataneo', '1950', '2013', 'Amazon Echo', '40', 'Carnegie Mellon', '2015', 'Stanford University', '20', 'Oren Etzioni', 'Allen Institute', 'Artificial Intelligence', 'Luke Zettlemoyer', '$180,000', 'Allen Institute', 'Google Brain', 'United States', 'Eastern Europe', 'Chris Nicholson', 'San Francisco', 'United States', 'Yoshua Bengio', 'Follow Cade Metz', '1', 'New York', 'L. Salaries', 'I. Talent', 'Order Reprints', 'Subscribe Advertisement Site Index Go', 'Home Page', '2020', 'New York', 'Pop Culture', 'Reader Center Wirecutter Live Events The Learning Network', '2020', 'New York', 'Pop Culture', 'Reader Center Wirecutter Live Events The Learning Network', 'Crossword Cooking', 'Site Information Navigation', '2019', 'The New York Times Company Contact Us Work', 'Brand Studio Your Ad Choices Privacy Terms', 'Service Terms', 'Sale Site Map Help Subscriptions']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'''(?x)\n",
    "          \n",
    "          (?:[A-Z])(?:[a-z]+|\\.)(?:\\s+[A-Z](?:[a-z]+|\\.))*(?:\\s+[A-Z])(?:[a-z]+|\\.)\n",
    "                                         # proper names\n",
    "          | \\$\\d+\\s\\b[tr|b|m]illion\\b    # literal monetary amounts\n",
    "          | \\$?\\d+(?:[,\\.]\\d+)?          # numerical monetary amounts\n",
    "          | \\d{2}\\[\\\\]\\d{2}\\-\\\\]\\d{4}    # numerical dates (U.S. format)\n",
    "          | [A-Z][a-z.]*\\s\\d{2}\\,\\s\\d{2, 4} # literal dates (U.S. format)\n",
    "\n",
    "          \n",
    "        '''\n",
    "\n",
    "print(nltk.regexp_tokenize(nyt, pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  10.\n",
    "\n",
    "☼ Rewrite the following loop as a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n"
     ]
    }
   ],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "result = []\n",
    "for word in sent:\n",
    "    word_len = (word, len(word))\n",
    "    result.append(word_len)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n"
     ]
    }
   ],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "result = [(word, len(word)) for word in sent]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11.\n",
    "\n",
    "☼ Define a string `raw` containing a sentence of your own choosing. Now, split `raw` on some character other than space, such as '`s`'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How much ', ' would a ', 'chuck chuck if a ', 'chuck could chuck ', '?']"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\"\n",
    "raw.split('wood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12.\n",
    "\n",
    "☼ Write a `for` loop to print out the characters of a string, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "o\n",
      "m\n",
      "p\n",
      "a\n",
      "r\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "s\n",
      "o\n",
      "m\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n"
     ]
    }
   ],
   "source": [
    "string = \"Compared to some of the previous exercises, this seems comically easy.\"\n",
    "\n",
    "for s in string[:20]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 13.\n",
    "\n",
    "☼ What is the difference between calling `split` on a string with no argument or with `' '` as the argument, e.g. `sent.split()` versus `sent.split(' ')`? What happens when the string being split contains tab characters, consecutive space characters, or a sequence of tabs and spaces? \n",
    "\n",
    "*`sent.split()` splits all whitespace identically.*\n",
    "\n",
    "*`sent.split(' ')` splits all whitespace literally.  I.e., tabs will be represented as `\\t`, and each individaul whitespace will be spilt into its own string.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With `sent.split()`:\n",
      "['This', 'string', 'is', 'a', 'pretty', 'simple', 'string.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This', 'string', 'is', 'a', 'pretty', 'simple', 'string.']\n",
      "\n",
      "With `sent.split()`:\n",
      "['This', 'strings', 'has', 'tabs.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This\\tstrings\\thas\\ttabs.']\n",
      "\n",
      "With `sent.split()`:\n",
      "['This', 'string', 'has', 'lots', 'of', 'space.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This', '', '', '', '', '', '', '', 'string', '', '', '', '', '', '', '', '', '', 'has', '', '', '', '', '', 'lots', '', '', '', 'of', '', '', '', '', 'space.']\n",
      "\n",
      "With `sent.split()`:\n",
      "['This', 'string', 'has', 'tabs', 'and', 'spaces.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This\\tstring', '', '', '', '', '', '', '', '', 'has\\ttabs', '', '', '', '', '', '', 'and\\tspaces.']\n"
     ]
    }
   ],
   "source": [
    "s1 = \"This string is a pretty simple string.\"\n",
    "s2 = \"This\\tstrings\\thas\\ttabs.\"\n",
    "s3 = \"This        string          has      lots    of     space.\"\n",
    "s4 = \"This\\tstring         has\\ttabs       and\\tspaces.\"\n",
    "\n",
    "Ss = [s1, s2, s3, s4]\n",
    "\n",
    "for s in Ss:\n",
    "    print(\"\\nWith `sent.split()`:\")\n",
    "    print(s.split())\n",
    "    print(\"\\nWith `sent.split(' ')`:\")\n",
    "    print(s.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. \n",
    "\n",
    "☼ Create a variable `words` containing a list of words. Experiment with `words.sort()` and `sorted(words)`. What is the difference?\n",
    "\n",
    "*`words.sort()` doesn't return a value, but it alters the ordering of the list, so that whenever I call the list again, the returned list will be the ordered one, and not the one I originally stored.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wörter', 'focail', 'mots', 'ord', 'ord', 'palabras', 'palavras', 'parole', 'sanat', 'slova', 'szavak', 'słowa', 'từ ngữ', 'woorden', 'words', 'words', 'λόγια', 'ווערטער']\n"
     ]
    }
   ],
   "source": [
    "words = [\"slova\", \"ord\", \"Wörter\", \"λόγια\", \"words\", \"palabras\", \"sanat\", \n",
    "         \"mots\", \"focail\", \"szavak\", \"parole\", \"words\", \"woorden\", \"ord\", \n",
    "         \"słowa\", \"palavras\", \"từ ngữ\", \"ווערטער\"]\n",
    "\n",
    "words.sort()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wörter', 'focail', 'mots', 'ord', 'ord', 'palabras', 'palavras', 'parole', 'sanat', 'slova', 'szavak', 'słowa', 'từ ngữ', 'woorden', 'words', 'words', 'λόγια', 'ווערטער']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`sorted(words)` will return a result, but the ordering of the original list will not be changed:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wörter', 'focail', 'mots', 'ord', 'ord', 'palabras', 'palavras', 'parole', 'sanat', 'slova', 'szavak', 'słowa', 'từ ngữ', 'woorden', 'words', 'words', 'λόγια', 'ווערטער']\n"
     ]
    }
   ],
   "source": [
    "words = [\"slova\", \"ord\", \"Wörter\", \"λόγια\", \"words\", \"palabras\", \"sanat\", \n",
    "         \"mots\", \"focail\", \"szavak\", \"parole\", \"words\", \"woorden\", \"ord\", \n",
    "         \"słowa\", \"palavras\", \"từ ngữ\", \"ווערטער\"]\n",
    "\n",
    "print(sorted(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['slova', 'ord', 'Wörter', 'λόγια', 'words', 'palabras', 'sanat', 'mots', 'focail', 'szavak', 'parole', 'words', 'woorden', 'ord', 'słowa', 'palavras', 'từ ngữ', 'ווערטער']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. \n",
    "\n",
    "☼ Explore the difference between strings and integers by typing the following at a Python prompt: `\"3\" * 7` and `3 * 7`. Try converting between strings and integers using `int(\"3\")` and `str(3)`.\n",
    "\n",
    "*Multiplying a string $x$ by an integer $y$ will just cause $x$ to be printed to the console $y$ times:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3333333'"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"3\" * 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>`3 * 7` will just give us the product:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 * 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can convert strings to integers with `int()` and vice-versa with `str()`:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(\"3\") * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3333333'"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(3) * 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16. \n",
    "\n",
    "☼ Use a text editor to create a file called `prog.py` containing the single line `monty = 'Monty Python'`. Next, start up a new session with the Python interpreter, and enter the expression `monty` at the prompt. You will get an error from the interpreter. Now, try the following (note that you have to leave off the `.py` part of the filename):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Errors in my notebook prevent the remaining cells from running, so this is being saved as markdown:*\n",
    "\n",
    "```\n",
    "monty\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "NameError                                 Traceback (most recent call last)\n",
    "<ipython-input-308-d4cc90107335> in <module>\n",
    "----> 1 monty\n",
    "\n",
    "NameError: name 'monty' is not defined\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python'"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prog import monty\n",
    "monty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This time, Python should return with a value. You can also try `import prog`, in which case Python should be able to evaluate the expression `prog.monty` at the prompt.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python'"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import prog\n",
    "\n",
    "prog.monty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17. \n",
    "\n",
    "☼ What happens when the formatting strings `%6s` and `%-6s` are used to display strings that are longer than six characters?\n",
    "\n",
    "*This looks to be a legacy question from an older version of the book, since this is the older method of formatting in Python.  As the question is written, `%6s` won't do anything to a longer string:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'another test'"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"another test\"\n",
    "\n",
    "\"%6s\" % (test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`%-6s` will add padding to a string shorter than six characters:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey   '"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"%-6s\" % (\"hey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I suspect the authors may have left out a decimal.  `%.6s` has a quite different effect:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anothe'"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"%.6s\" % (test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. \n",
    "\n",
    "◑ Read in some text from a corpus, tokenize it, and print the list of all *wh*-word types that occur. (*wh*-words in English are used in questions, relative clauses and exclamations: *who*, *which*, *what*, and so on.) Print them in order. Are any words duplicated in this list, because of the presence of case distinctions or punctuation?\n",
    "\n",
    "*This question is a little difficult to follow.  Most of the corpora we're using have texts that have already been tokenized, so the first part of this question seems a bit redundant.  However, just to play along,  I'll use the raw text version of one of the Project Gutenberg texts.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "raw = gutenberg.raw('bryant-stories.txt')\n",
    "\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "tokens = sorted(set(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The next part of the question also seems a little hard to follow: The __wh-__words are a closed set (__what__, __when__, __where__, __which__, __who__, __whose__, and __why__), and if I explicitly define them, I won't find any exceptions.  The only possible way around this is to search for all words that begin with __wh__.  But if I do so, most of the hits will be false positives, because most words that start with __wh-__ are outside of this set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Whale', 'What', 'When', 'Whenever', 'Where', 'Whether', 'Whiff', 'While', 'Whirling', 'White', 'Who', 'Whose', 'Why', 'what', 'whatever', 'wheat', 'wheelbarrow', 'wheeled', 'when', 'whence', 'whenever', 'where', 'wherein', 'wherever', 'whether', 'which', 'while', 'whimpering', 'whin', 'whinny', 'whipped', 'whirlpool', 'whiruled', 'whisk', 'whisked', 'whisper', 'whisper_', 'whispered', 'whispering', 'whispers', 'whistle', 'whistled', 'white', 'white-haired', 'white-robed', 'whither', 'who', 'whole', 'wholly', 'whom', 'whose', 'why']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in tokens if re.search('^[Ww]h', w)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As expected, most of the results are false positives.  There are also  versions of the __wh-__ words starting with both upper- and lowercase letters; as well as 'whom', the accusative form of 'who'.  But there are also a number of words that could be considered __wh-__ words that I wouldn't have thought of searching for, such as words ending with '-ever' (e.g., 'whatever', 'whenever', etc...); variant forms of 'where' (i.e., 'whence' and 'whither'), as well as 'whether' and 'wherein'.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. \n",
    "\n",
    "◑ Create a file consisting of words and (made up) frequencies, where each line consists of a word, the space character, and a positive integer, e.g. `fuzzy 53`. Read the file into a Python list using `open(filename).readlines()`. Next, break each line into its two fields using `split()`, and convert the number into an integer using `int()`. The result should be a list of the form: `[['fuzzy', 53], ...]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy = open('fuzzy.txt', encoding = \"utf-8\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fuzzy', 53],\n",
       " ['wuzzy', 92],\n",
       " ['was', 128],\n",
       " ['a', 4897],\n",
       " ['bear', 23],\n",
       " ['had', 47],\n",
       " ['no', 93],\n",
       " ['hair', 23],\n",
       " [\"wasn't\", 78],\n",
       " ['he', 77]]"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[word, int(value)] for word, value in (f.split() for f in fuzzy)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20. \n",
    "\n",
    "◑ Write code to access a favorite webpage and extract some text from it. For example, access a weather site and extract the forecast top temperature for your town or city today.\n",
    "\n",
    "*This is not so straightforward.  Most websites today do not have a static link to weather values, and it's almost certainly easier to use an API.  [OpenWeather](https://openweathermap.org/api \"OpenWeather\") offers such an API, though it requires registration, which takes a few hours to process.  The results will be in JSON, which needs to be parsed, and the temperature will be in Kelvin, which will need to be converted:*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# needs to be deleted before posting to GH\n",
    "key = '10f1049e5862ee388ad748981accb4c1'\n",
    "\n",
    "def k_to_c(temp):\n",
    "    \"\"\"Converts Kelvin to Celsius.\"\"\"\n",
    "    return temp - 273.15\n",
    "\n",
    "def k_to_f(temp):\n",
    "    \"\"\"Converts Kelvin to Fahrenheit.\"\"\"\n",
    "    return (temp - 273.15) * 1.8 + 32\n",
    "\n",
    "def get_temp(city):\n",
    "    \"\"\"\n",
    "    Gets current, high, and low temperatures for a given city.\n",
    "    \"\"\"\n",
    "    r = requests.get('http://api.openweathermap.org/data/2.5/weather?q=' + city + '&APPID=' + key)\n",
    "    \n",
    "    if r.json()['cod'] == '404':\n",
    "        print(\"Sorry, we don't know where that city is.\")\n",
    "    else:\n",
    "        current_k = r.json()['main']['temp']\n",
    "        min_k = r.json()['main']['temp_min']\n",
    "        max_k = r.json()['main']['temp_max']\n",
    "        print(\"The current temperature is {:.1f} C°/{:.1f} F°.\".format(k_to_c(current_k), k_to_f(current_k)))\n",
    "        print(\"Today's high temperature is {:.1f} C°/{:.1f} F°.\".format(k_to_c(max_k), k_to_f(max_k)))\n",
    "        print(\"Today's low temperature is {:.1f} C°/{:.1f} F°.\".format(k_to_c(min_k), k_to_f(min_k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature is 17.8 C°/64.0 F°.\n",
      "Today's high temperature is 20.6 C°/69.0 F°.\n",
      "Today's low temperature is 14.0 C°/57.2 F°.\n"
     ]
    }
   ],
   "source": [
    "get_temp('Hiroshima')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature is 18.7 C°/65.6 F°.\n",
      "Today's high temperature is 20.6 C°/69.0 F°.\n",
      "Today's low temperature is 16.0 C°/60.8 F°.\n"
     ]
    }
   ],
   "source": [
    "get_temp('Kure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, we don't know where that city is.\n"
     ]
    }
   ],
   "source": [
    "get_temp('aaaa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21.\n",
    "\n",
    "◑ Write a function `unknown()` that takes a URL as its argument, and returns a list of unknown words that occur on that webpage. In order to do this, extract all substrings consisting of lowercase letters (using `re.findall()`) and remove any items from this set that occur in the Words Corpus (`nltk.corpus.words`). Try to categorize these words manually and discuss your findings.\n",
    "\n",
    "*If we literally do what the instructions tell us, we'll get quite a large list, as inflected word forms (-ing, -ed, -s, etc...) are mostly absent from the Words Corpus.  So I've added parameters to `unknown` that will let us exclude these common words.  This may lead to a small number of false negatives; but I feel this is an acceptable cost, given the very high number of false positives.  It might be advisable to run the function twice, once without the endings excluded and once with, so that the user can inspect for him-/herself which words are being excluded.*\n",
    "\n",
    "*Additionally, irregular verbs are absent in the wordlist, so I added these manually.*\n",
    "\n",
    "*After adjusting which words are excluded, we'll see that most of the words are fairly new words ('podcast', 'app', ...) which haven't had a chance to be added to the corpus.  We also see compound words ('counterproductive', 'rollout', ...) whose constituent parts are part of the corpus.  We will also see the occasional lower-case last name: it's common for authors to post their Twitter handles next to their byline, and these handles are usually lowercase.  Upper-case names were pruned in the function.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = [w.lower() for w in nltk.corpus.words.words('en')]\n",
    "\n",
    "# irregular verbs\n",
    "verbs = ['ate', 'beat', 'beaten', 'became', 'become', 'began', 'begun', 'bent', \n",
    "         'bet', 'bid', 'bit', 'bitten', 'blew', 'blown', 'bought', 'broke', 'broken', \n",
    "         'brought', 'built', 'burnt', 'came', 'caught', 'chose', 'chosen', 'come', \n",
    "         'cost', 'cut', 'did', 'dived', 'done', 'dove', 'drank', 'drawn', 'dreamt', \n",
    "         'drew', 'driven', 'drove', 'drunk', 'dug', 'eaten', 'fallen', 'fell', 'felt', \n",
    "         'flew', 'flown', 'forgave', 'forgiven', 'forgot', 'forgotten', 'fought', 'found', \n",
    "         'froze', 'frozen', 'gave', 'given', 'gone', 'got', 'gotten', 'grew', 'grown', \n",
    "         'had', 'heard', 'held', 'hid', 'hidden', 'hit', 'hung', 'hurt', 'kept', 'knew', \n",
    "         'known', 'laid', 'lain', 'lay', 'led', 'left', 'lent', 'let', 'lost', 'made', \n",
    "         'meant', 'met', 'paid', 'put', 'ran', 'rang', 'read', 'ridden', 'risen', 'rode', \n",
    "         'rose', 'run', 'rung', 'said', 'sang', 'sat', 'saw', 'seen', 'sent', 'showed', \n",
    "         'shown', 'shut', 'slept', 'sold', 'spent', 'spoke', 'spoken', 'stood', 'sung', \n",
    "         'swam', 'swum', 'taken', 'taught', 'thought', 'threw', 'thrown', 'told', 'took', \n",
    "         'tore', 'torn', 'understood', 'went', 'woke', 'woken', 'won', 'wore', 'worn', \n",
    "         'written', 'wrote']\n",
    "\n",
    "wordlist += verbs\n",
    "\n",
    "\n",
    "\n",
    "def unknown(url, es = False, s = False, ed = False, ing = False, n = False, er = False):\n",
    "    \n",
    "    # get text\n",
    "    raw = return_URL_contents(url)\n",
    "    \n",
    "    # get lower-case words\n",
    "    raw_lower = re.findall(r'\\b[a-z]+\\b', raw)\n",
    "    \n",
    "    # find unknown words and eliminate duplicates\n",
    "    unknown = sorted(set([w for w in raw_lower if w not in wordlist]))\n",
    "    \n",
    "    # find common words that are not in wordlist because of \n",
    "    # morphological changes\n",
    "    exclude = []\n",
    "    \n",
    "    # words with -es plurals\n",
    "    if es:\n",
    "        es = [i for i in unknown if i[-2:] == 'es' and i[:-2] in wordlist] \n",
    "        exclude += es\n",
    "        # -y becomes -ies\n",
    "        ies = [i for i in unknown if i[-3:] == 'ies' and i[:-3] + 'y' in wordlist]\n",
    "        exclude += ies\n",
    "        \n",
    "    # regular plurals\n",
    "    if s:\n",
    "        s = [i for i in unknown if i[-1] == 's' and i[:-1] in wordlist]\n",
    "        exclude += s\n",
    "        \n",
    "    # regular past tense forms\n",
    "    if ed:\n",
    "        # verbs with final -e\n",
    "        d = [i for i in unknown if i[-1:] == 'd' and i[:-1] in wordlist]\n",
    "        exclude += d\n",
    "        # regular verbs\n",
    "        ed = [i for i in unknown if i[-2:] == 'ed' and i[:-2] in wordlist]\n",
    "        exclude += ed\n",
    "        # verbs that double final consonant\n",
    "        dd = [i for i in unknown if i[-2:] == 'ed' and i[:-3] in wordlist]\n",
    "        exclude += dd\n",
    "        \n",
    "    # regular gerunds\n",
    "    if ing:\n",
    "        # verbs with final -e\n",
    "        ng = [i for i in unknown if i[-3:] == 'ing' and i[:-3] + 'e' in wordlist]\n",
    "        exclude += ng\n",
    "        # regular verbs\n",
    "        ing = [i for i in unknown if i[-3:] == 'ing' and i[:-3] in wordlist]\n",
    "        exclude += ing\n",
    "        # verbs that double final consonat\n",
    "        nng = [i for i in unknown if i[-3:] == 'ing' and i[:-4] in wordlist]\n",
    "        exclude += nng\n",
    "        \n",
    "    if n:\n",
    "        # negative contractions without final -'t\n",
    "        n = [i for i in unknown if i[-1:] == 'n' and i[:-1] in wordlist]\n",
    "        exclude += n\n",
    "        \n",
    "    if er:\n",
    "        # comparative forms\n",
    "        er = [i for i in unknown if i[-2:] == 'er' and i[:-2] in wordlist]\n",
    "        exclude += er\n",
    "        # comparative forms with final -y\n",
    "        ier = [i for i in unknown if i[-3:] == 'ier' and i[:-3] + 'y' in wordlist]\n",
    "        exclude += ier\n",
    "        # comparative forms with final -e\n",
    "        r = [i for i in unknown if i[-2:] == 'er' and i[:-1] in wordlist]\n",
    "        exclude += r\n",
    "        # superlative forms\n",
    "        est = [i for i in unknown if i[-3:] == 'est' and i[:-3] in wordlist]\n",
    "        exclude += est\n",
    "        # superlative forms with final -y\n",
    "        st = [i for i in unknown if i[-3:] == 'est' and i[:-2] in wordlist]\n",
    "        exclude += st\n",
    "        # superlative forms with final -e\n",
    "        iest = [i for i in unknown if i[-4:] == 'iest' and i[:-4] + 'y' in wordlist]\n",
    "        exclude += iest\n",
    "        \n",
    "    # return only those unknown words that have not been excluded\n",
    "    # by the above list comprehensions\n",
    "    return [i for i in unknown if i not in exclude]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actions', 'agencies', 'answers', 'app', 'attacking', 'attempts', 'banned', 'biohacker', 'breaks', 'bringing', 'businesses', 'called', 'candidates', 'changing', 'companies', 'compares', 'comparing', 'competitors', 'couldn', 'criticized', 'debated', 'details', 'diagnosing', 'didn', 'discussed', 'doesn', 'doors', 'employees', 'enemies', 'enjoyed', 'executives', 'explained', 'explodes', 'explores', 'frontrunner', 'fundraising', 'giants', 'harshest', 'has', 'helped', 'hosted', 'ignoring', 'including', 'indicated', 'infused', 'joined', 'laws', 'lists', 'lives', 'mailing', 'mainstream', 'members', 'millennials', 'minutes', 'missed', 'monopolies', 'numbers', 'offered', 'okay', 'options', 'playing', 'podcast', 'politicians', 'practices', 'pressed', 'proposals', 'pushed', 'represents', 'required', 'rights', 'rules', 'sectors', 'sees', 'sharing', 'shifted', 'showcased', 'signing', 'simmering', 'specifics', 'stories', 'streamers', 'techlash', 'teddyschleifer', 'themes', 'things', 'topics', 'tougher', 'users', 'using', 'wants', 'wasn', 'whacks', 'years']\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.vox.com/recode/2019/10/16/20916712/cnn-democratic-presidential-debate-big-tech-silicon-valley-warren-harris'\n",
    "\n",
    "print(unknown(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['app', 'biohacker', 'frontrunner', 'fundraising', 'mainstream', 'okay', 'podcast', 'techlash', 'teddyschleifer']\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.vox.com/recode/2019/10/16/20916712/cnn-democratic-presidential-debate-big-tech-silicon-valley-warren-harris'\n",
    "\n",
    "print(unknown(url, es = True, s = True, ed = True, ing = True, n = True, er = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abandoning', 'acquiescing', 'adding', 'advocates', 'aligned', 'angels', 'announced', 'appearances', 'applications', 'areas', 'arts', 'asserted', 'attacking', 'attempted', 'automobiles', 'berated', 'books', 'briefed', 'captured', 'citing', 'closest', 'closing', 'columnists', 'comments', 'communists', 'compared', 'complaining', 'concerns', 'condemns', 'continues', 'contributed', 'controlled', 'corrections', 'counterproductive', 'countries', 'created', 'criticized', 'dated', 'decades', 'defended', 'defends', 'denied', 'denounced', 'deployed', 'described', 'detained', 'deterring', 'dismissed', 'dismissing', 'earlier', 'edited', 'editorials', 'elected', 'email', 'emerged', 'emphasized', 'events', 'expressing', 'fears', 'feels', 'fighters', 'focusing', 'followed', 'forces', 'friends', 'gained', 'hands', 'has', 'having', 'heated', 'horses', 'hours', 'implying', 'including', 'insisted', 'interests', 'interjected', 'interpreted', 'invading', 'isn', 'issues', 'jeopardizing', 'jobs', 'journeys', 'launched', 'leaders', 'lebanon', 'letters', 'listings', 'looks', 'managing', 'matters', 'meltdown', 'mercenaries', 'minimized', 'mishandled', 'misquoted', 'movies', 'multimedia', 'near', 'newsletters', 'obituaries', 'obtained', 'officials', 'ol', 'op', 'opened', 'operations', 'opponents', 'others', 'outposts', 'overrated', 'parenting', 'parties', 'penalties', 'permitting', 'peterbakernyt', 'points', 'policies', 'positions', 'preparing', 'presidents', 'prisoners', 'promoting', 'proposed', 'pulled', 'pulling', 'pushed', 'rebuked', 'rebutted', 'reemerge', 'referring', 'relayed', 'remarks', 'reported', 'reporters', 'reporting', 'reports', 'rollout', 'scratched', 'seemed', 'served', 'services', 'sharpest', 'shouldn', 'soldiers', 'statements', 'states', 'stronger', 'subscriptions', 'suggested', 'talks', 'things', 'ties', 'tools', 'trips', 'troops', 'visited', 'walked', 'wants', 'wars', 'wasn', 'years']\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.nytimes.com/2019/10/16/world/middleeast/trump-erdogan-turkey-syria-kurds.html?action=click&module=Top%20Stories&pgtype=Homepage\"\n",
    "\n",
    "print(unknown(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['counterproductive', 'email', 'lebanon', 'meltdown', 'multimedia', 'near', 'ol', 'op', 'peterbakernyt', 'reemerge', 'rollout']\n"
     ]
    }
   ],
   "source": [
    "print(unknown(url, es = True, s = True, ed = True, ing = True, n = True, er = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['allenskratch', 'cyclo', 'll', 'longtime', 'maglia', 'plc', 'taylorphinney', 'triallist', 'unsubscribe', 'vibe']\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.cyclingnews.com/news/taylor-phinney-set-to-retire/\"\n",
    "print(unknown(url, es = True, s = True, ed = True, ing = True, n = True, er = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22. \n",
    "\n",
    "◑ Examine the results of processing the URL `http://news.bbc.co.uk/` using the regular expressions suggested above. You will see that there is still a fair amount of non-textual data there, particularly Javascript commands. You may also find that sentence breaks have not been properly preserved. Define further regular expressions that improve the extraction of text from this web page.\n",
    "\n",
    "*I find this question really poorly written.  We're supposed to use the 'regular expressions suggested above', but which ones?  The chapter is full of them!*\n",
    "\n",
    "*I've already made several functions that do fairly good jobs of extracting text and removing Javascript commands.  I don't really feel like breaking one of these functions just for the sake of practice...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Home - BBC News Homepage Accessibility links Skip to content Accessibility Help BBC Account Notifications Home News Sport Weather iPlayer Sounds CBBC CBeebies Food Bitesize Arts Taster Local TV Radio Three Menu Search Search the BBC Search the BBC BBC News News Navigation Sections Home Home selected Video World Asia UK Business Tech Science Stories Entertainment & Arts Health World News TV In Pictures Reality Check Newsbeat Special Reports Explainers Long Reads Have Your Say More More sections Home Home selected Video World World Home Africa Australia Europe Latin America Middle East US & Canada Asia Asia Home China India UK UK Home England N. Ireland Scotland Wales Politics Local News Business Business Home Market Data Global Trade Companies Entrepreneurship Technology of Business Business of Sport Global Education Economy Global Car Industry Tech Science Stories Entertainment & Arts Health World News TV In Pictures Reality Check Newsbeat Special Reports Explainers Long Reads Have Your Say BBC News Home Breaking Breaking news Close breaking news Latest Stories Most Read Skip to most read Latest Stories Most Read Top Stories Turkey-Syria 'not our border', says Trump His remarks come as the US comes under fire at home and abroad for withdrawing its troops from Syria. 3h 3 hours ago Middle East Turkey-Syria 'not our border', says Trump His remarks come as the US comes under fire at home and abroad for withdrawing its troops from Syria. 3h 3 hours ago Middle East Related content Video Trump exchange with reporter over troop withdrawal Viewpoint: Syria could cost Trump re-election Video Turkey presses on with Syria military offensive 'No Brexit deal tonight', UK source says UK and EU officials are continuing technical talks ahead of Thursday's crunch EU council meeting. 2h 2 hours ago UK Politics Hong Kong protest leader violently attacked Photographs show Jimmy Sham of the Civil Human Rights Front lying in the street, covered in blood. 8h 8 hours ago China Hundreds arr\""
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.bbc.com/news\"\n",
    "return_URL_contents(url)[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23. \n",
    "◑ Are you able to write a regular expression to tokenize text in such a way that the word *don't* is tokenized into *do* and *n't*? Explain why this regular expression won't work: `«n't|\\w+»`.\n",
    "\n",
    "*The short answer is that I'm not really sure.  I understand that this book is (was) designed so that it could be used in the classroom, and therefore the answers were not included; but I believe a large percentage - perhaps even the majority - of users are students engaging in self-study, and for these people (myself included), explanations would be greatly appreciated.*\n",
    "\n",
    "<i>My guess is that matches are greedy, and the regular expression evaluator will try to return the largest match that it can.  To turn this off, we need to add <code>(.*?)</code>.  Also, we need to add parentheses around `n't` to specify it as a capturing group.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('do', \"n't\")]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"(.*?)(n't)|\\w+\", \"don't\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24. \n",
    "\n",
    "◑ Try to write code to convert text into *hAck3r*, using regular expressions and substitution, where `e` → `3`, `i` → `1`, `o` → `0`, `l` → `|`, `s` → `5`, `.` → `5w33t!`, `ate` → `8`. Normalize the text to lowercase before converting it. Add more substitutions of your own. Now try to map `s` to two different values: `$` for word-initial `s`, and `5` for word-internal `s`.\n",
    "\n",
    "*This seems like a perfect place to `re.sub()`, which, typically, is not introduced until later in the exercise set...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H3||0 5uck3r55w33t!  I 8 y0ur |unch5w33t!  It wa5 d3|15h5w33t!'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"Hello suckers.  I ate your lunch.  It was delish.\"\n",
    "\n",
    "test = test.lower()\n",
    "\n",
    "org = ['ate', 'e', 'i', 'o', 'l', 's', '\\.']\n",
    "sub = ['8', '3', '1', '0', '|', '5', '5w33t!']\n",
    "\n",
    "for i in range(len(org)):\n",
    "    test = re.sub(org[i], sub[i], test)\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adding my own substitutions:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%3+3r %1%3r %1ck3d a %3ck 0f %1ck|3d %3%%3r55w33t!'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"Peter Piper picked a peck of pickled peppers.\"\n",
    "\n",
    "test = test.lower()\n",
    "\n",
    "org = ['e', 'i', 'o', 'l', 's', 't', 'p', '\\.']\n",
    "sub = ['3', '1', '0', '|', '5', '+', '%', '5w33t!']\n",
    "\n",
    "for i in range(len(org)):\n",
    "    test = re.sub(org[i], sub[i], test)\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Differentiating between initial $s$ and medial $s$:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$u513 |1v35 1n m1551551pp15w33t!'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"Susie lives in Mississippi.\"\n",
    "\n",
    "test = test.lower()\n",
    "\n",
    "org = ['ate', 'e', 'i', 'o', 'l', 't', r'\\bs', 's', '\\.']\n",
    "sub = ['8', '3', '1', '0', '|', '+', '$', '5', '5w33t!']\n",
    "\n",
    "for i in range(len(org)):\n",
    "    test = re.sub(org[i], sub[i], test)\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25. \n",
    "\n",
    "◑ Pig Latin is a simple transformation of English text. Each word of the text is converted as follows: move any consonant (or consonant cluster) that appears at the start of the word to the end, then append *ay*, e.g. *string* → *ingstray*, *idle* → *idleay*. http://en.wikipedia.org/wiki/Pig_Latin\n",
    "\n",
    " * a. Write a function to convert a word to Pig Latin.\n",
    " * b. Write code that converts text, instead of individual words.\n",
    " * c. Extend it further to preserve capitalization, to keep qu together (i.e. so that `quiet` becomes `ietquay`), and to detect when `y` is used as a consonant (e.g. `yellow`) vs a vowel (e.g. `style`).\n",
    " \n",
    "*Instead of going through the instructions in order, I think it might be easier to handle all of the exceptions from the beginning.  I'm also not going to include all the intermediate code, so what follows is my final answer to all parts of this question:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pig_latin(word):\n",
    "    \"\"\"\n",
    "    Returns pig latin version of word.\n",
    "    \"\"\"\n",
    "    \n",
    "    # replace 'dumb' quotes\n",
    "    word = re.sub(\"’\", \"'\", word)\n",
    "        \n",
    "    # won't work on non-alphabetic strings\n",
    "    if not word.isalpha():\n",
    "        if \"'\" not in word:\n",
    "            return word\n",
    "    \n",
    "    # Return uppercase word if original is in uppercase\n",
    "    caps = False\n",
    "    if word[0].isupper():\n",
    "        caps = True\n",
    "    word = word.lower()\n",
    "    \n",
    "    # word starts with vowel\n",
    "    if word[0] in 'AEIOUaeiou':\n",
    "        pl = word + 'ay'\n",
    "        \n",
    "    # some tokenizers will produce non-words    \n",
    "    elif len(word) == 1:\n",
    "        return word\n",
    "    \n",
    "    # word begins with 'y' - treated as a consonant\n",
    "    # otherwise 'y' is a vowel, or first vowel is not 'y'\n",
    "    elif word[0] == 'y':\n",
    "        pl = word[1:] + 'yay'\n",
    "    \n",
    "    # word begins with 'qu'\n",
    "    elif word[:2] == \"qu\":\n",
    "        pl = word[2:] + \"quay\"\n",
    "    \n",
    "    # all other cases\n",
    "    else:\n",
    "        start, end = re.findall(r'\\b^[^aeiouy]*|[aeiouy]{1}\\S*', word)\n",
    "        pl = end + start + 'ay'\n",
    "    \n",
    "    # restore word to uppercase if necessary\n",
    "    if caps == True:\n",
    "        pl = pl[0].upper() + pl[1:]\n",
    "    return pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*From Chapter 2, exercise 3.  It will make the final output look nicer by joining punctuation to the preceding string.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_punctuation(text, characters = [\"'\", '’', ')', ',', '.', ':', ';', '?', '!', ']', \"''\"]): \n",
    "    \"\"\"\n",
    "    Takes a list of strings and attaches punctuation to\n",
    "    the preceding string in the list.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = iter(text)\n",
    "    current = next(text)\n",
    "\n",
    "    for nxt in text:\n",
    "        if nxt in characters:\n",
    "            current += nxt\n",
    "        else:\n",
    "            yield current\n",
    "            current = nxt\n",
    "            \n",
    "\n",
    "    yield current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://funnystories.tumblr.com/post/140309670613/funny-story\n",
    "\n",
    "# For the sake of convenience, I've removed some punctuation.\n",
    "\n",
    "\n",
    "story = \"\"\"One time in sixth grade we were at recess and while I was running to \n",
    "my friends, I just so happened to kick a HUGE rock and without thinking I \n",
    "shouted at the top of my lungs MOTHERFUCKER And with my god-awful luck, my math\n",
    "teacher was sitting at the bench right BESIDE ME. He then took me inside to \n",
    "what I thought was yell at me but he just couldn’t stop laughing and sent \n",
    "me back outside with a literal candy bar. He is still my favorite teacher \n",
    "I've ever had.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pig_latin_text(text):\n",
    "    \"\"\"\n",
    "    Translates text into pig latin.\n",
    "    \"\"\"\n",
    "    pl = []\n",
    "    for t in re.findall(r'\\b[\\S]+\\b|[.,!?]', text):\n",
    "        pl.append(pig_latin(t))\n",
    "        \n",
    "    return \" \".join(join_punctuation(pl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oneay imetay inay ixthsay adegray eway ereway atay ecessray anday ilewhay Iay asway unningray otay ymay iendsfray, Iay ustjay osay appenedhay otay ickkay aay Ugehay ockray anday ithoutway inkingthay Iay outedshay atay ethay optay ofay ymay ungslay Otherfuckermay Anday ithway ymay god-awful ucklay, ymay athmay eachertay asway ittingsay atay ethay enchbay ightray Esidebay Emay. Ehay enthay ooktay emay insideay otay atwhay Iay oughtthay asway ellyay atay emay utbay ehay ustjay ouldn'tcay opstay aughinglay anday entsay emay ackbay outsideay ithway aay iterallay andycay arbay. Ehay isay illstay ymay avoritefay eachertay I'veay everay adhay.\""
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pig_latin_text(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ka', 'ka', 'ka', 'ka', 'ka', 'ro', 'ka', 'ka', 'vi', 'ko']"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  26.\n",
    "\n",
    "◑ Download some text from a language that has vowel harmony (e.g. Hungarian), extract the vowel sequences of words, and create a vowel bigram table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "path = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\chapter03\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I had considerable difficulties with this exercise.  I orignally tried to use a Hungarian Wikipedia page, but encountered trouble trying to extract the vowels from this page: No matter which regular expression I used, the vowels returned were always unaccented.  I spent quite a bit of time experimenting with different regular expressions until I discovered that diacritics are encoded separately from the vowels on Wikipedia.*\n",
    "\n",
    "*In the meanwhile, I just decided to use a Hungarian text from Project Gutenberg.  I don't know anything about Hungarian literature, but I had a text copy of \"Az arany ember (2. rész)\" by Mór Jókai on my hard drive from another project I had done, so I decided to use this text.  It worked fine for my purposes.*\n",
    "\n",
    "*Instead of using the method outlined in the book for importing Project Gutenberg texts, I have a function that I wrote while I was going through chapter 13 of \"Think Python\" (add link).  I'll use a simplified version of that here:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pg_text(text, encode = \"utf8\"):\n",
    "    \"\"\"\n",
    "    Returns a list of words from a Project Gutenberg text.  \n",
    "    Headers and footers are removed from texts. \n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    text: name of file\n",
    "    encode: text encoding used in file. Default is UTF-8\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    opened_text = open(text, 'r', encoding = encode)\n",
    "    cleaned_text = []\n",
    "    flag = False\n",
    "    start = \"*** START OF\"\n",
    "    end = \"*** END OF\"\n",
    "\n",
    "    # some PG texts don't use spaces to designate start/end of text\n",
    "    alt_start = \"***START OF\"\n",
    "    alt_end = \"***END OF\"\n",
    "    \n",
    "    for line in opened_text:\n",
    "        \n",
    "        # start reading in lines after boilerplate\n",
    "        if ((start in line) or (alt_start in line)) and flag == False:\n",
    "            flag = True\n",
    "        \n",
    "        # return word list once boilerplate has been reached\n",
    "        elif ((end in line) or (alt_end in line)) and flag == True:\n",
    "            return cleaned_text\n",
    "        elif flag == True:\n",
    "                \n",
    "                for word in line.split():\n",
    "                    word = word.strip().lower()\n",
    "                    cleaned_text.append(word)\n",
    "                \n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = clean_pg_text('hungarian.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*When we visually inspect the beginning of the text, we can see that the first line is in English.  Let's remove that so it doesn't interfere with our analysis:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['produced', 'by', 'albert', 'lászló', 'from', 'page', 'images', 'generously', 'made', 'available', 'by', 'the', 'google', 'books', 'library', 'project', 'jókai', 'mór', 'összes', 'művei', 'nemzeti', 'kiadás', 'xlvi.', 'kötet', 'az', 'arany', 'ember.', 'ii.', 'budapest', 'révai', 'testvérek', 'kiadása', '1896', 'az', 'arany', 'ember', 'regény', 'irta', 'jókai', 'mór', 'ii.', 'rész', 'pfeifer', 'ferdinánd', 'tulajdona', 'budapest', 'révai', 'testvérek', 'kiadása', '1896', 'a', 'világon', 'kivül.', 'a', 'leány', 'még', 'azután', 'is', 'ott', 'maradt', 'a', 'férfi', 'kebléhez', 'tapadva,', 'mikor', 'már', 'az', 'eltávozott,', 'a', 'kitől', 'őt', 'öntestével', 'védnie', 'kellett.', 'miért', 'tette', 'azt,', 'hogy', 'keblére', 'vesse', 'magát?', 'hogy', 'azt', 'mondja:', '«én', 'szeretem', 'őt?»', 'el', 'akarta', 'ezzel', 'űzni', 'végképen', 'azt', 'az', 'embert,', 'kinek', 'jelenlététől', 'iszonyodott?', 'lehetetlenné', 'akarta']\n"
     ]
    }
   ],
   "source": [
    "print(raw[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I checked the end as well, and it appears there's some sort of index there.  That will also interfere with our analysis, so let's eliminate that as well:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['létezéséről', 'pedig', 'frivaldszky', 'imre', 'nagynevű', 'természettudósunk', 'által', 'értesültem', 's', 'az', 'a', 'hatvanas', 'években', 'még', 'a', 'maga', 'kivételes', 'állapotában', 'megvolt,', 'mint', 'egy', 'se', 'magyar-,', 'se', 'törökországhoz', 'nem', 'tartozó', 'új', 'alkotású', 'terület.', 'ennyit', 'jónak', 'láttam', 'elmondani.', '_dr.', 'jókai', 'mór._', 'tartalom.', 'második', 'kötet.', 'a', 'világon', 'kivül', '1', 'tropicus', 'capricorni', '8', 'az', 'édes', 'otthon', '30', 'a', 'családi', 'ékszer', '35', 'egy', 'uj', 'vendég', '50', 'a', 'faragó', 'ember', '72', 'noémi', '79', 'melancholia', '97', 'teréza', '120', 'a', 'kettétört', 'kard', '134', 'az', 'első', 'veszteség', '157', 'a', 'jég', '167', 'a', 'rém', '183', 'mit', 'beszél', 'a', 'hold?', '–', 'mit', 'beszél', 'a', 'jég?', '211', 'ki', 'jön?', '217', 'a', 'hulla', '221', 'zófi', 'asszony', '224', 'dódi', 'levele', '230', 'te', 'ügyetlen!', '235', 'athalia', '216', 'az', 'utolsó', 'tőrdöfés', '264', 'a', 'mária-nostrai', 'nő', '270', 'a', '«senki»', '271', 'utóhangok', '279', 'franklin-társulat', 'nyomdája.', \"[transcriber's\", 'note:', 'javítások.', 'az', 'eredeti', 'szöveg', 'helyesírásán', 'nem', 'változtattunk.', 'a', 'nyomdai', 'hibákat', 'javítottuk.', 'ezek', 'listája:', '47', '|fog', 'mindent,', '|fog', 'mindent.', '122', '|közül', 'kimelkedő', '|közül', 'kiemelkedő', '126', '|ez', 'ídő', 'óta', '|ez', 'idő', 'óta', '182', '|sőt', 'itt', 'ott-', '|sőt', 'itt-ott', '189', '|magyarországba.', '|magyarországba.»', '189', '|basát?', 'mordult', '|basát?»', 'mordult', '192', '|keretezve…', '|keretezve…»', '193', '|numerusa', 'sincs.', '|numerusa', 'sincs.»', '224', '|susánna…', '|susánna…»', '256', '|mákhéjakból.', '|mákhéjakból.»]', 'end', 'of', 'the', 'project', 'gutenberg', 'ebook', 'of', 'az', 'arany', 'ember', '(2.', 'rész),', 'by', 'mór', 'jókai']\n"
     ]
    }
   ],
   "source": [
    "print(raw[-200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73955"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = raw[16:-158]\n",
    "len(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Extracting all the vowels from the words in the text:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv = []\n",
    "for hw in raw:\n",
    "    hv.append(re.findall(r'([aeiouáéíóöúüőű])', hw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I know practically nothing about Hungarian, let alone vowel harmony.  However, from the scant research I've done (mostly [this website](http://www.hungarianreference.com/Vowel-Harmony.aspx \"vowel harmony in Hungarian\")), I believe that this phenomenon only affects the final vowels.  So for our analysis, we'll only be looking at the final two vowels.  Therefore, we'll eliminate any word with only one vowel; and we'll eliminate the beginning vowels from words with three or more:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_pairs = []\n",
    "\n",
    "for h in hv:\n",
    "    if len(h) == 2:\n",
    "        hv_pairs.append(h[0] + h[1])\n",
    "    elif len(h) > 2:\n",
    "        hv_pairs.append(h[-2] + h[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43759"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hv_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a    e    i    o    u    á    é    í    ó    ö    ú    ü    ő    ű \n",
      "a 2896  209 1133 1718  181 1215  126   10  424   17   34   10    5    0 \n",
      "e   76 5739 1909   97   23  224 1581   12    9   66    8  438  737   67 \n",
      "i  709 1745  154  548   46  884  234    3   90   13    8   14  167   13 \n",
      "o 2597   39  312  792  126  769   38    9  240    2   89    0    2    3 \n",
      "u  700   23  146  257   15  381    8    4   95    1    0    0    2    0 \n",
      "á 2388   23  441  853  115  392   57    9  360    4   14    5    7    1 \n",
      "é  406 2432  637   28    4   82  293    5    7    8    0  101  244   14 \n",
      "í  108   88   20   75    1   73   25    0   29    0    0   18   19    5 \n",
      "ó  350    7  161   98   17   82   15    1   30    0    0    0    1    1 \n",
      "ö    0  583  174    1    0    5  279    3    0  373    0  100   76   26 \n",
      "ú   94   12   26   27    0   18    0    0   13    2    2    0    5    0 \n",
      "ü    1  244   81    3    0    1   68    0    0  116    0    7   34    4 \n",
      "ő   66  536   78    2    0    4   40    0    0   81    0   34   20    3 \n",
      "ű    2   50    9    0    0    1   19    0    1   23    1    2    1   12 \n"
     ]
    }
   ],
   "source": [
    "cfd = nltk.ConditionalFreqDist(hv_pairs)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Unfortunately, there aren't so many patterns to be seen in this table.  It looks like there's a concentration in the upper-left corner, but that could just be because unaccented vowels are more common.  From what I can tell from the [web page cited earlier](http://www.hungarianreference.com/Vowel-Harmony.aspx \"Vowel Harmony\"), there are two types of vowels in Hungarian: Back vowels (__a__, __á__, __(i)__, __(í)__, __o__, __ó__, __u__, __ú__), and front vowels (__e__, __é__, __(i)__, __(í)__, __ö__, __ő__, __ü__, __ű__).  The vowels __i__ and __í__ are considered intermediate vowels, and don't contribute to vowel harmony.*\n",
    "\n",
    "*What I did next was use the `itertools` package to create a power set of all possible combinations of back vowels.  The combinations are saved as tuples, so I used a list comprehension to convert these to strings.  I removed all the combinations with only i or í, and finally I used `.count()` to look at how many instances of each combination could be found in the original pairs.  I repeated these steps for the front vowels:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['au', 'uá', 'oo', 'óo', 'íó', 'íú', 'iu', 'uo', 'áu', 'áá', 'úí', 'iá', 'áo', 'oá', 'úi', 'aá', 'ua', 'uu', 'oa', 'óu', 'uó', 'ío', 'uú', 'oú', 'óú', 'úá', 'úú', 'ai', 'óó', 'oí', 'áú', 'áó', 'óá', 'ái', 'aí', 'iú', 'ió', 'íá', 'íu', 'ía', 'óí', 'ui', 'ia', 'úu', 'uí', 'aú', 'úo', 'aó', 'ói', 'aa', 'áí', 'io', 'ou', 'oó', 'óa', 'úa', 'oi', 'áa', 'úó', 'ao']\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# back vowels\n",
    "bv = ['a', 'á', 'i', 'í', 'o', 'ó', 'u', 'ú']\n",
    "\n",
    "# create power set\n",
    "bvps = set(list(itertools.product(bv, bv)))\n",
    "\n",
    "# convert items in power set to strings\n",
    "bvpj = [b[0] + b[1] for b in bvps]\n",
    "\n",
    "# Remove combinations with only i or í\n",
    "tbd = ['ií', 'íi', 'ii', 'íí']\n",
    "for i in tbd:\n",
    "    bvpj.remove(i)\n",
    "    \n",
    "print(bvpj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22205"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum all possible combinations\n",
    "back_vowels = 0\n",
    "for i in bvpj:\n",
    "    back_vowels += hv_pairs.count(i)\n",
    "    \n",
    "back_vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ie', 'éé', 'öe', 'öö', 'űé', 'űű', 'őő', 'eí', 'ié', 'iű', 'öü', 'öé', 'íű', 'űe', 'eő', 'őí', 'íő', 'üű', 'éű', 'ei', 'őé', 'üé', 'éi', 'éö', 'őe', 'eü', 'üi', 'üö', 'űő', 'őü', 'íe', 'öi', 'őű', 'öí', 'iö', 'üő', 'íé', 'íö', 'üí', 'űü', 'éő', 'éí', 'íü', 'őö', 'iü', 'öű', 'iő', 'űi', 'űí', 'öő', 'űö', 'eö', 'eé', 'éü', 'üü', 'ee', 'ői', 'üe', 'ée', 'eű']\n"
     ]
    }
   ],
   "source": [
    "# same procedure with front vowels\n",
    "fv = ['e', 'é', 'i', 'í', 'ö', 'ő', 'ü', 'ű']\n",
    "fvps = set(list(itertools.product(fv, fv)))\n",
    "fvpj = [f[0] + f[1] for f in fvps]\n",
    "\n",
    "tbd = ['ií', 'íi', 'ii', 'íí']\n",
    "\n",
    "for i in tbd:\n",
    "    fvpj.remove(i)\n",
    "    \n",
    "print(fvpj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19700"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "front_vowels = 0\n",
    "for i in fvpj:\n",
    "    front_vowels += hv_pairs.count(i)\n",
    "    \n",
    "front_vowels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Again, my knowledge of Hungarian is practically non-existent; but my analysis shows that less than 5% of the words in this novel failed to show vowel harmony.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.236842706643205"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(hv_pairs) - front_vowels - back_vowels)/len(hv_pairs) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I later came back to my original issue and tried to read in the text from a Wikipedia page.  I reluctant to use an entry on a non-Hungarian topic, since the number of non-Hungarian words in the article might affect the analysis.  So I decided to use the entry on Franz Liszt, as he was from Hungary.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://hu.wikipedia.org/wiki/Liszt_Ferenc'\n",
    "test = return_URL_contents(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Liszt Ferenc – Wikipédia Liszt Ferenc A Wikipédiából, a szabad enciklopédiából Ez a közzétett változat , ellenőrizve : 2019. október 3. Ugrás a navigációhoz Ugrás a kereséshez Ez a szócikk a magyar zeneszerzőről szól. Hasonló címmel lásd még: Franz von Liszt . Liszt Ferenc Liszt Ferenc 1856-ban ( Wilhelm von Kaulbach festménye) Életrajzi adatok Születési név Liszt Ferencz Született 1811 . október 22. Doborján , Magyarország Származás magyar Elhunyt 1886 . július 31. (74 évesen) Bayreuth , Németország Sírhely Bayreuth Élettárs Marie d’Agoult (1805–1876) Carolyne zu Sayn-Wittgenstein (1819–1887) Gyermekei Cosima Wagner Daniel Liszt Blandine Rachel Liszt Szülei Anna Liszt Ádám Liszt Iskolái Bécsi Zene- és Előadóművészeti Egyetem Pályafutás Műfajok szimfónia klasszikus zene Hangszer zongora Díjak A művészetek és a tudományok érdemrendje Order of the Golden Spur Bavarian Maximilian Order for Science and Art (1884) Knight of the Order of Christ Tevékenység zeneszerző , karmester IPI-névazonosító 00018232018 Liszt Ferenc aláírása A Wikimédia Commons tartalmaz Liszt Ferenc témájú médiaállományokat. Liszt Ferenc ( németül Franz Liszt ; Doborján , 1811 . október 22. – Bayreuth , 1886 . július 31. ) [1] magyar zeneszerző , zongoraművész , karmester és zenetanár. Minden idők egyik legkiválóbb zongoraművésze, aki egyben a 19. századi romantika egyik legjelentősebb zeneszerzője. Apja, Liszt Ádám , Esterházy herceg hivatalnoka, korán felismerte fiának kivételes zenei tehetségét, és minden lehetőséget megragadott kibontakoztatására. Liszt kilencéves korában már nyilvánosság előtt zongorázott Sopronban és Pozsonyban , majd hamarosan műpártoló főurak támogatásával Bécsben folytathatta tanulmányait Czerny és Salieri tanítványaként. 1822 . december 1-jén mutatkozott be Bécsben. A 11 éves Liszt első koncertje nagy feltűnést keltett. Itt találkozott Be'"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Liszt', 'Ferenc', 'Wikipédia', 'Liszt', 'Ferenc', 'A', 'Wikipédiából', 'a', 'szabad', 'enciklopédiából']\n"
     ]
    }
   ],
   "source": [
    "test_words = re.findall(r'\\b[\\S]+\\b', test)\n",
    "print(test_words[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Having problems with double acutes...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['19',\n",
       " 'századi',\n",
       " 'romantika',\n",
       " 'egyik',\n",
       " 'legjelentősebb',\n",
       " 'zeneszerzője',\n",
       " 'Apja',\n",
       " 'Liszt',\n",
       " 'Ádám',\n",
       " 'Esterházy']"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words[175:185]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hv = []\n",
    "for hw in test_words:\n",
    "    test_hv.append(re.findall(r'([aeiou])(\\\\u\\d{3,4}b*)*', ascii(hw)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'legjelento\\\\u030bsebb'\""
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ascii('legjelentősebb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'zeneszerzo\\\\u030bje'\""
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ascii('zeneszerzője')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [('a', '\\\\u0301'), ('a', ''), ('i', '')],\n",
       " [('o', ''), ('a', ''), ('i', ''), ('a', '')],\n",
       " [('e', ''), ('i', '')],\n",
       " [('e', ''), ('e', ''), ('e', ''), ('o', '\\\\u030b'), ('e', '')],\n",
       " [('e', ''), ('e', ''), ('e', ''), ('o', '\\\\u030b'), ('e', '')],\n",
       " [('a', '')],\n",
       " [('i', '')],\n",
       " [('u', ''), ('a', '\\\\u0301')],\n",
       " [('e', ''), ('a', '\\\\u0301')]]"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hv[175:185]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('i', '')], [('e', ''), ('e', '')], [('i', ''), ('i', ''), ('e', '\\\\u0301'), ('i', ''), ('a', '')], [('i', '')], [('e', ''), ('e', '')], [], [('i', ''), ('i', ''), ('e', '\\\\u0301'), ('i', ''), ('a', '\\\\u0301'), ('o', '\\\\u0301')], [('a', '')], [('a', ''), ('a', '')], [('e', ''), ('i', ''), ('o', ''), ('e', '\\\\u0301'), ('i', ''), ('a', '\\\\u0301'), ('o', '\\\\u0301')]]\n"
     ]
    }
   ],
   "source": [
    "print(test_hv[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The regular expression above returns a list for each word with a tuple for each vowel.  If the vowel is unaccented, the second string is an empty string.  The following loop ignores words with only one vowel; concatenates the vowels in words with two vowels; and concatenates the last two vowels from words with more than two vowels:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hvc = []\n",
    "for l in test_hv:\n",
    "    if len(l) == 2:\n",
    "        test_hvc.append(l[0][0] + l[0][1] + l[1][0] + l[1][1])\n",
    "    elif len(l) > 2:\n",
    "        test_hvc.append(l[-2][0] + l[-2][1] + l[-1][0] + l[-1][1])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Currently, accented vowels still have their Unicode escape strings attached to them:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ee', 'ia', 'ee', 'a\\\\u0301o\\\\u0301', 'aa', 'a\\\\u0301o\\\\u0301', 'e\\\\u0301e', 'oa', 'ie', 'o\\\\u0301e']\n"
     ]
    }
   ],
   "source": [
    "print(test_hvc[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*While it's possible to use `re.sub` to substitute escape strings on individual strings, this doesn't work on strings inside lists:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'áo'"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'a\\\\u0301', 'á', 'a\\\\u0301o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ee', 'ia', 'ee', 'a\\\\u0301o\\\\u0301', 'aa', 'a\\\\u0301o\\\\u0301', 'e\\\\u0301e', 'oa', 'ie', 'o\\\\u0301e']\n"
     ]
    }
   ],
   "source": [
    "for th in test_hvc[:10]:\n",
    "    re.sub(r'a\\\\u0301', 'á', th)\n",
    "    \n",
    "print(test_hvc[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The only way to deal with this is to make a new list with the substituted values.  In our case, we'll have to make nine substitutions, so that would mean nine new lists.  Instead of having nine near-identical lists floating about, I decided to just use the name of the old list, and re-assigned the new list to it.*\n",
    "\n",
    "*To avoid repeating this step nine times, I created a simple function that will simply use a given `re.sub()` command to generate an updated list.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_list(l, org, sub):\n",
    "    \"\"\"\n",
    "    Uses re.sub to generate a new list.\n",
    "    \n",
    "    Arguments:\n",
    "    l:   list with strings to be replaced\n",
    "    org: regular expression to be replaced  \n",
    "    sub: replacement regular expression\n",
    "    \"\"\"\n",
    "    new_list = []\n",
    "    \n",
    "    for i in l:\n",
    "        new_list.append(re.sub(org, sub, i))\n",
    "        \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To avoid writing out all of the regular expressions to be matched, I tried to automate the process as much as possible by using a pair of `for`-loops.  The loops make one pass for each vowel and diacritic combination - i.e., 15 passes.  However, there are only nine vowels with diacritics in Hungarian.  To keep the loops aligned, I added empty spaces to the strings `diaresis` and `double_acute`:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = 'aeiou'\n",
    "uc = ['\\\\u0301', '\\\\u0308', '\\\\u030b']\n",
    "#uc = ['\\\\u0301', '\\\\u0308']\n",
    "acute = 'áéíóú'\n",
    "\n",
    "# empty spaces are there for alignment\n",
    "diaresis = '   öü'\n",
    "double_acute = '   őű'\n",
    "diacritics = [acute, diaresis, double_acute]\n",
    "#diacritics = [acute, diaresis]\n",
    "\n",
    "for i in range(len(vowels)):\n",
    "    for j in range(len(uc)):\n",
    "        test_hvc = generate_new_list(test_hvc, r'' + re.escape(vowels[i] + uc[j]) + '', diacritics[j][i])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It appears we are missing double acutes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ee', 'ia', 'ee', 'áó', 'aa', 'áó', 'ée', 'oa', 'ie', 'óe', 'óo', 'ée', 'ói', 'aa', 'ou', 'oo', 'íe', 'ee', 'ee', 'ie']\n"
     ]
    }
   ],
   "source": [
    "print(test_hvc[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We're still not in the clear: it appears this list still has a number of vowels with their unicode escape strings still attached.  By examining a few, it appears that these are from French or Russian words.  Since they're not need for our analysis, let's just excise them:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test_hvc:\n",
    "    if len(i) != 2:\n",
    "        print(i, end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hvc = [t for t in test_hvc if len(t) == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          a    e    i    o    u    á    é    í    ó    ö    ú    ü \n",
      "     0    0   26    1    0    0    0    0    0    0    0    0    0 \n",
      "a    3  488  163  324  510   75  163   22    4   17    2    2    0 \n",
      "e    0   75 1186  366  179   45   25  213    2    0   10    1   46 \n",
      "i    0  291  370   94   90  155   85   32    0   19    0    0    1 \n",
      "o    0  413  115  123  147  197  125    7    0   14    1    1    0 \n",
      "u    0  135  280   53   47   58  147   51    0   18   23    0    1 \n",
      "á    0  593    5  117  237   57   97    7    4   50    1    0    0 \n",
      "é    0   21  588   62   21   10   15   88    0    1    0    0   28 \n",
      "í    0   24   35    5   21   25   24    9    0    2    0    0    7 \n",
      "ó    0   85   35    8   31    1   17    1    0    0    0    0    0 \n",
      "ö    0    4   72   18    7    0    5   20    0    0   70    3   14 \n",
      "ú    0   32    0    1    5    0    6    1    0    0    0    0    0 \n",
      "ü    0    0   14   15   17    1    1    4    0    0    5    1    3 \n"
     ]
    }
   ],
   "source": [
    "cfd = nltk.ConditionalFreqDist(test_hvc)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*still having problems with double acute accents...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
