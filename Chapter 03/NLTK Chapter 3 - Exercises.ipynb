{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Chapter 3\n",
    "\n",
    "## Processing Raw Text\n",
    "\n",
    "*The html version of this chapter in the NLTK book is available [here](https://www.nltk.org/book/ch03.html#exercises \"Ch03 Exercises\").*\n",
    "\n",
    "### 8   Exercises\n",
    "\n",
    "###### 1. \n",
    "\n",
    "☼ Define a string `s = 'colorless'`. Write a Python statement that changes this to \"colourless\" using only the slice and concatenation operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'colourless'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'colorless'\n",
    "s = s[:4] + 'u' + s[4:]\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.\n",
    "\n",
    "☼ We can use the slice notation to remove morphological endings on words. For example, `'dogs'[:-1]` removes the last character of `dogs`, leaving `dog`. Use slice notation to remove the affixes from these words (we've inserted a hyphen to indicate the affix boundary, but omit this from your strings): `dish-es`, `run-ning`, `nation-ality`, `un-do`, `pre-heat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dish', 'run', 'nation', 'un', 'pre']\n"
     ]
    }
   ],
   "source": [
    "affixed = [('dishes', 2), \n",
    "           ('running', 4),\n",
    "           ('nationality', 5),\n",
    "           ('undo', 2),\n",
    "           ('preheat', 4)]\n",
    "\n",
    "print([s[:-a] for s, a in affixed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.\n",
    "\n",
    "☼ We saw how we can generate an `IndexError` by indexing beyond the end of a string. Is it possible to construct an index that goes too far to the left, before the start of the string?\n",
    "\n",
    "*Yes.  I'm not going to run the code in my notebook, because then the cells below this one wouldn't run.*\n",
    "\n",
    "```\n",
    ">>>trial = \"trial\"\n",
    ">>>for i in range(1, len(trial) + 2):\n",
    ">>>    print(trial[-i])\n",
    "    \n",
    "l\n",
    "a\n",
    "i\n",
    "r\n",
    "t\n",
    "---------------------------------------------------------------------------\n",
    "IndexError                                Traceback (most recent call last)\n",
    "<ipython-input-21-98077138b076> in <module>\n",
    "      1 trial = \"trial\"\n",
    "      2 for i in range(1, len(trial) + 2):\n",
    "----> 3     print(trial[-i])\n",
    "\n",
    "IndexError: string index out of range\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. \n",
    "\n",
    "☼ We can specify a \"step\" size for the slice. The following returns every second character within the slice: `monty[6:11:2]`. It also works in the reverse direction: `monty[10:5:-2]` Try these for yourself, then experiment with different step values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tittitie tírýhkeee řltl řstittitie tírýhsřc.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A Czech tongue twister:\n",
    "tt = \"Třistatřiatřicet stříbrných křepelek přeletělo přes třistatřiatřicet stříbrných střech.\"\n",
    "\n",
    "# Every other letter\n",
    "tt[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.cřshýrít eitittitsř ltlř eeekhýrít eitittiT'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every other letter from the end\n",
    "tt[::-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tstaittbý el etoř iaiřesínhtc'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every third letter\n",
    "tt[::3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You get the point...*\n",
    "\n",
    "##### 5. \n",
    "\n",
    "☼ What happens if you ask the interpreter to evaluate `monty[::-1]`? Explain why this is a reasonable result.\n",
    "\n",
    "*It prints the word backwards.  It's simply printing from the end by steps of -1:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'murder'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"redrum\"[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.\n",
    "\n",
    "☼ Describe the class of strings matched by the following regular expressions.\n",
    "\n",
    "a. `[a-zA-Z]+`\n",
    "\n",
    "b. `[A-Z][a-z]*`\n",
    "\n",
    "c. `p[aeiou]{,2}t`\n",
    "\n",
    "d. `\\d+(\\.\\d+)?`\n",
    "\n",
    "e. `([^aeiou][aeiou][^aeiou])*`\n",
    "\n",
    "f. `\\w+|[^\\w\\s]+`\n",
    "\n",
    "Test your answers using `nltk.re_show()`.\n",
    "\n",
    "*__a.__ `[a-zA-Z]+` will match anything alphabetical:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{cAMELCASE} 6186258313 {hybr}1{d}\n"
     ]
    }
   ],
   "source": [
    "import nltk, re\n",
    "\n",
    "nltk.re_show(r'[a-zA-Z]+', \"cAMELCASE 6186258313 hybr1d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>__b.__ `[A-Z][a-z]*` will match words beginning with uppercase letters, or any uppercase letters in other positions:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{I} think words beginning with {Uppercase} {Letters} will be matched, or any uppercase letters found in o{T}{H}{E}{R} positions.\n"
     ]
    }
   ],
   "source": [
    "test = 'I think words beginning with Uppercase Letters will be matched, ' \\\n",
    "       'or any uppercase letters found in oTHER positions.'\n",
    "\n",
    "nltk.re_show(r'[A-Z][a-z]*', test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__c.__ `p[aeiou]{,2}t` will match all words with __p__, up to two vowels, and a letter __t__.  This is a lot of words: In the wordlist we've been using in this chapter, this RegExp will return nearly 7,000 words, since any word with __pt__ will be a match.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6978"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist = [w.lower() for w in nltk.corpus.words.words('en')]\n",
    "len([w for w in wordlist if re.search(r'p[aeiou]{,2}t', w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abaptiston', 'abepithymia', 'ableptical', 'ableptically', 'abrupt', 'abruptedly', 'abruption', 'abruptly', 'abruptness', 'absorpt', 'absorptance', 'absorptiometer', 'absorptiometric', 'absorption', 'absorptive', 'absorptively', 'absorptiveness', 'absorptivity', 'absumption', 'acalypterae']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in wordlist if re.search(r'p[aeiou]{,2}t', w)][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If we add the `^` and <code>\\$</code> operators, we'll instead end up with all 3-letter words beginning and ending with __p__ and __t__ with one vowel in the middle, or all 4-letter words beginning and ending with __p__ and __t__ with two vowels in the middle:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pat', 'pat', 'paut', 'peat', 'pet', 'piet', 'piet', 'pit', 'poet', 'poot', 'pot', 'pout', 'put']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in wordlist if re.search(r'^p[aeiou]{,2}t$', w)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__d.__ `\\d+(\\.\\d+)?` will match any numbers and decimal points, no matter how many numbers are to the left/right of the decimal.  It will not match dashes, dollar signs, or any other symbol associated with number.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1234}\n",
      "{12.34}\n",
      "example {123.4} in a string\n",
      "{1}-{234}\n",
      "{12},{4}\n",
      "${12.34}\n"
     ]
    }
   ],
   "source": [
    "test = ['1234', '12.34', 'example 123.4 in a string', '1-234', '12,4', '$12.34']\n",
    "for t in test:\n",
    "    nltk.re_show(r'\\d+(\\.\\d+)?', t) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If we use two decimals, the second is ignored:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.23}.{4}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'\\d+(\\.\\d+)?', '1.23.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can alter that by changing the `?` to a `+`:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.23.4}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'\\d+(\\.\\d+)+', '1.23.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>__e.__ `([^aeiou][aeiou][^aeiou])*` will match any non-vowel\\vowel\\non-vowel combination, no matter how many times it's repeated.  White spaces are considered non-vowels, so a string such as `to ` would match.  `nltk.re_show()` behaves quite strangely with this RegExp - a string like `\"baab\"` would return `{}b{}a{}a{}b{}`.  However, I have evaluated this RegExp with online evaluators (such as [this one](https://regexr.com/ \"regexr.com\"), and there the responsive is as expected:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{babbabbabbab}{}a{pap}{}a{}\n"
     ]
    }
   ],
   "source": [
    "string = \"babbabbab\" \\\n",
    "         \"babapapa\"\n",
    "nltk.re_show(r'([^aeiou][aeiou][^aeiou])*', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}b{}a{}a{}b{}\n"
     ]
    }
   ],
   "source": [
    "string = \"baab\"\n",
    "nltk.re_show(r'([^aeiou][aeiou][^aeiou])*', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__f.__ `\\w+|[^\\w\\s]+` will match either any alphanumeric string of any length, or a string of any length that does not contain alphanumeric characters or whitespace - i.e., all punctuation and any other non-whitespace/non-alphanumeric characters:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{This} {RegExp} {needs} {a} {fairly} {long} {string} {to} {show} {what} {it} {can} {%#$^%&*} {do}{.}\n"
     ]
    }
   ],
   "source": [
    "string = \"This RegExp needs a fairly long string to show what it can %#$^%&* do.\"\n",
    "nltk.re_show(r'\\w+|[^\\w\\s]+', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.\n",
    "\n",
    "*☼ Write regular expressions to match the following classes of strings:*\n",
    "\n",
    " + *__a.__ A single determiner (assume that __a__, __an__, and __the__ are the only determiners).*\n",
    " + <i>__b.__ An arithmetic expression using integers, addition, and multiplication, such as `2*3+8`.</i>\n",
    " \n",
    "*__a.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think {a} relevant string like {the} one here is {an} example of what we need.\n"
     ]
    }
   ],
   "source": [
    "string = \"I think a relevant string like the one here is an example of what we need.\"\n",
    "nltk.re_show(r'\\b[Aa]n?\\b|\\b[Tt]he\\b', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__b.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2 * 3 + 8}\n"
     ]
    }
   ],
   "source": [
    "string = \"2 * 3 + 8\"\n",
    "nltk.re_show(r'(\\d|[+*= ])+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{11 + 4 * 2}\n"
     ]
    }
   ],
   "source": [
    "string = \"11 + 4 * 2\"\n",
    "nltk.re_show(r'(\\d|[+*= ])+', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.\n",
    "\n",
    "\n",
    "☼ Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML markup removed. Use `from urllib import request`  and then `request.urlopen('http://nltk.org/').read().decode('utf8')` to access the contents of the URL.\n",
    "\n",
    "*This chapter of the NLTK book dealt with removing HTML tags, but didn't really touch on removing the style & scripts tags that are present in most pages today. [This Stack Overflow discussion](https://stackoverflow.com/questions/30565404/remove-all-style-scripts-and-html-tags-from-an-html-page#answers \"Removing Style, Scripts, and HTML tags\") has a good discussion on how to use BeautifulSoup to do that, specifically with the `extract` and `stripped_strings` methods.  The code below borrows heavily from [this answer in the above Stack Overflow discussion](https://stackoverflow.com/a/30565597 \"Removing Style, Scripts, and HTML tags - answer\"):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "from unicodedata import normalize\n",
    "\n",
    "def return_URL_contents(url):\n",
    "    html = request.urlopen(url).read().decode('utf8')\n",
    "    raw = BeautifulSoup(html, 'html.parser')\n",
    "    for r in raw(['script', 'style']):\n",
    "        r.extract() # remove tags\n",
    "    \n",
    "    text = ' '.join(raw.stripped_strings) # retrieve tag content\n",
    "    \n",
    "    return normalize('NFKD', text) # normalize escape sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What Virtual Reality Can Teach a Driverless Car - The New York Times Sections SEARCH Skip to content Skip to site index Business Log In Log In Today’s Paper Business | What Virtual Reality Can Teach a Driverless Car Subscribe Log In Advertisement Supported by What Virtual Reality Can Teach a Driverless Car By Cade Metz Oct. 29, 2017 SAN FRANCISCO — As the computers that operate driverless cars digest the rules of the road, some engineers think it might be nice if they can learn from mistakes made in virtual reality rather than on real streets. Companies like Toyota, Uber and Waymo have discussed at length how they are testing autonomous vehicles on the streets of Mountain View, Calif., Phoenix and other cities. What is not as well known is that they are also testing vehicles inside computer simulations of these same cities. Virtual cars, equipped with the same software as the real thing, spend thousands of hours driving their digital worlds. Think of it as a way of identifying flaws in the way the cars operate without endangering real people. If a car makes a mistake on a simulated drive, engineers can tweak its software accordingly, laying down new rules of behavior. On Monday, Waymo, the autonomous car company that spun out of Google, is expected to show off its simulator tests when it takes a group of reporters to its secretive testing center in California’s Central Valley. Researchers are also developing methods that would allow cars to actually learn new behavior from these simulations, gathering skills more quickly than human engineers could ever lay them down with explicit software code. “Simulation is a tremendous thing,” said Gill Pratt, chief executive of the Toyota Research Institute, one of the artificial intelligence labs exploring this kind of virtual training for autonomous vehicles and other robotics. Image In virtual reality simulations, driverless cars operated by Waymo can practice driving the same intersection, in the same driving conditions, tho'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.nytimes.com/2017/10/29/business/virtual-reality-driverless-cars.html?module=inline\"\n",
    "\n",
    "return_URL_contents(url)[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Guido van Rossum - Wikipedia Guido van Rossum From Wikipedia, the free encyclopedia Jump to navigation Jump to search Dutch programmer and creator of Python In this Dutch name , the family name is van Rossum , not Rossum . Guido van Rossum Guido van Rossum at the Dropbox headquarters in 2014 Born ( 1956-01-31 ) 31 January 1956 (age 63) [1] Haarlem , Netherlands [2] [3] Residence Belmont, California , U.S. Nationality Dutch Alma mater University of Amsterdam Occupation Computer programmer, author Employer Dropbox [4] Known for Creating the Python programming language Spouse(s) Kim Knapp ( m. 2000) Children 1 [5] Awards Award for the Advancement of Free Software (2001) Website gvanrossum .github .io Guido van Rossum ( Dutch: [ˈɣido vɑn ˈrɔsʏm, -səm] ; born 31 January 1956) is a Dutch programmer best known as the author of the Python programming language , for which he was the \" Benevolent dictator for life \" (BDFL) until he stepped down from the position in July 2018. [6] [7] He is currently a member of the Python Steering Council. [8] Contents 1 Life and education 2 Work 2.1 Python 2.2 Computer Programming for Everybody 2.3 Mondrian 2.4 Dropbox 3 Awards 4 References 5 External links Life and education [ edit ] Van Rossum was born and raised in the Netherlands , where he received a master\\'s degree in mathematics and computer science from the University of Amsterdam in 1982. He has a brother, Just van Rossum, who is a type designer and programmer who designed the typeface used in the \"Python Powered\" logo. [ citation needed ] Guido lives in Belmont , California, with his wife, Kim Knapp, [9] and their son. [10] [11] [12] According to his home page and Dutch naming conventions , the \" van \" in his name is capitalized when he is referred to by surname alone, but not when using his first and last name together. [13] Work [ edit ] While working at the Centrum Wiskunde & Informatica (CWI), Van Rossum wrote and contributed a glob() routine to BSD Unix in 1986 [14] [15] and h'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Guido_van_Rossum\"\n",
    "\n",
    "return_URL_contents(url)[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. \n",
    "\n",
    "☼ Save some text into a file `corpus.txt`. Define a function `load(f)` that reads from the file named in its sole argument, and returns a string containing the text of the file.\n",
    "\n",
    " + a. Use `nltk.regexp_tokenize()` to create a tokenizer that tokenizes the various kinds of punctuation in this text. Use one multi-line regular expression, with inline comments, using the verbose flag `(?x)`.\n",
    " \n",
    " + b. Use `nltk.regexp_tokenize()` to create a tokenizer that tokenizes the following kinds of expression: monetary amounts; dates; names of people and organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\chapter03\"\n",
    "\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.nytimes.com/2017/10/22/technology/artificial-intelligence-experts-salaries.html?action=click&module=RelatedCoverage&pgtype=Article&region=Footer'\n",
    "\n",
    "text = return_URL_contents(url)\n",
    "\n",
    "with open('corpus.txt', 'w', encoding = \"utf-8\") as f:\n",
    "    f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(f):\n",
    "    text = open(f, encoding = \"utf-8\")\n",
    "    raw = text.read()\n",
    "    \n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt = load('corpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__a.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '.', '.', '.', '.', '.', ',', '.', '.', ',', ':', '.', '.', '.', ',', '.', ',', '.', '.', '.', ',', '.', '.', ',', ',', ',', ',', '.', '.', '.', '.', '.', ',', '.', '.', '.', '.', ',', ',', ',', ',', '.', ',', ',', '.', ',', '.', '.', '.', '.', ',', ',', '.', '.', '.', '.', '.', '.', ',', '.', ',', ',', '.', '.', '.', '.', ',', ',', ',', ',', '.', ',', ',', ',', ',', '.', '.', '.', '.', ',', ',', ',', '.', ',', ',', '.', ',', '.', ',', ',', ',', '.', '.', '.', ',', ',', '.', ',', '.', ',', ',', '.', ',', '.', ',', ',', ',', '.', '.', '.', ',', '.', ',', '.', '.', '.', '.', ',', '.', '.', '.', ',', '.', ',', ',', '.', '.', '(', ',', ',', ')', '.', ',', '.', ',', ',', '.', '.', ',', '.', '.', '.', ',', '.', '.', '.', ',', ',', '.', ',', '.', '.', ',', '.', ',', '.', '.', ',', '.', ',', ',', ',', ',', '.', '.', '.', ',', ',', '.', '.', ',', '.', ',', '.', ',', '.', '.', ',', ',', '.', '.', ',', '.', '.', '.', ',', ',', ':', '.', '.', '.', '.', '.', '.', '.', '.', \"'\", \"'\", ':', \"'\", ':', '.', '.', '.', '.', '.', \"'\", \"'\", ':', \"'\", ':', '.', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'''(?x)\n",
    "    [][.,;\"'?!():_-`] # finds punctuation\n",
    "'''\n",
    "\n",
    "print(nltk.regexp_tokenize(nyt, pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__b.__ Using regular expressions to extract information such as proper names - which can take numerous forms - is wrought with problems, and the regular expressions below are far from perfect.  It could very well be that the point of this exercise was to demonstrate just how difficult this approach is.*\n",
    "\n",
    "*Proper names are almost always capitalized; but so are many other words, and this approach alone is practically guaranteed to generate false positives/negatives.  I tried to limit the number of false positives by only examining sequences of two or more strings that began with uppercase letters; but that would eliminate any companies with a one-word name, as well as any references to a person using only his/her first/last name.  Another issue is that this returns strings where multiple words are in uppercase, such as titles.*\n",
    "\n",
    "*Monetary values are more straightforward, but I noticed that large amounts are often written out, so I created a special use case for this.  I didn't bother with all the different currency symbols for commonly-used currencies and instead made do with using the dollar sign.*\n",
    "\n",
    "*Finally, dates have a variety of formats around the world, but for the purpose of this exercise I only focussed on those formats in common use in the U.S., i.e. numerical dates (__10/16/19__ or __10/16/2019__) or literal dates (__Oct. 16, 2019__ or __October 16, 2019__.)*\n",
    "\n",
    "*While researching this question, I came across several methods that looked interesting, but which I did not pursue because of time limitations.  One was finding first names in a text by using the `names` corpus in NLTK.  It's easy to see how this would work; but I didn't explore this, because it seems the focus of this unit is regular expressions.  An additional issue would be the lack of corpora for last and company names.  Another method that looked interesting was using Python's `datetime` module to deal with dates.  But, as was the case above, time limitations prevented me from trying this.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tech Giants Are Paying Huge Salaries', 'Scarce A.', 'I. Talent', 'The New York Times Technology', 'Tech Giants Are Paying Huge Salaries', 'Scarce A.', 'I. Talent Subscribe Log In Image Credit Credit Christina Chung Sections Skip', 'Tech Giants Are Paying Huge Salaries', 'Scarce A.', 'I. Talent Nearly', 'Credit Credit Christina Chung Supported', 'By Cade Metz Oct', '22', '2017', 'Silicon Valley', 'I. Tech', 'Typical A.', '$300,000', '$500,000', 'Anthony Levandowski', '2007', '$120 million', 'Image Luke Zettlemoyer', 'Allen Institute', 'Artificial Intelligence', 'Credit Kyle Johnson', 'The New York Times Salaries', 'National Football League', 'Christopher Fernandez', 'Silicon Valley', '10,000', 'Andrew Moore', 'Carnegie Mellon University', '$650 million', '2014', '50', '400', '$138 million', '$345,000', 'Jessica Cataneo', '1950', '2013', 'Amazon Echo', '40', 'Carnegie Mellon', '2015', 'Stanford University', '20', 'Oren Etzioni', 'Allen Institute', 'Artificial Intelligence', 'Luke Zettlemoyer', '$180,000', 'Allen Institute', 'Google Brain', 'United States', 'Eastern Europe', 'Chris Nicholson', 'San Francisco', 'United States', 'Yoshua Bengio', 'Follow Cade Metz', '1', 'New York', 'L. Salaries', 'I. Talent', 'Order Reprints', 'Subscribe Advertisement Site Index Go', 'Home Page', '2020', 'New York', 'Pop Culture', 'Reader Center Wirecutter Live Events The Learning Network', '2020', 'New York', 'Pop Culture', 'Reader Center Wirecutter Live Events The Learning Network', 'Crossword Cooking', 'Site Information Navigation', '2019', 'The New York Times Company Contact Us Work', 'Brand Studio Your Ad Choices Privacy Terms', 'Service Terms', 'Sale Site Map Help Subscriptions']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'''(?x)\n",
    "          \n",
    "          (?:[A-Z])(?:[a-z]+|\\.)(?:\\s+[A-Z](?:[a-z]+|\\.))*(?:\\s+[A-Z])(?:[a-z]+|\\.)\n",
    "                                         # proper names\n",
    "          | \\$\\d+\\s\\b[tr|b|m]illion\\b    # literal monetary amounts\n",
    "          | \\$?\\d+(?:[,\\.]\\d+)?          # numerical monetary amounts\n",
    "          | \\d{2}\\[\\\\]\\d{2}\\-\\\\]\\d{4}    # numerical dates (U.S. format)\n",
    "          | [A-Z][a-z.]*\\s\\d{2}\\,\\s\\d{2, 4} # literal dates (U.S. format)\n",
    "\n",
    "          \n",
    "        '''\n",
    "\n",
    "print(nltk.regexp_tokenize(nyt, pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  10.\n",
    "\n",
    "☼ Rewrite the following loop as a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n"
     ]
    }
   ],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "result = []\n",
    "for word in sent:\n",
    "    word_len = (word, len(word))\n",
    "    result.append(word_len)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n"
     ]
    }
   ],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "result = [(word, len(word)) for word in sent]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11.\n",
    "\n",
    "☼ Define a string `raw` containing a sentence of your own choosing. Now, split `raw` on some character other than space, such as '`s`'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How much ', ' would a ', 'chuck chuck if a ', 'chuck could chuck ', '?']"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\"\n",
    "raw.split('wood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12.\n",
    "\n",
    "☼ Write a `for` loop to print out the characters of a string, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "o\n",
      "m\n",
      "p\n",
      "a\n",
      "r\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "s\n",
      "o\n",
      "m\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "p\n",
      "r\n",
      "e\n",
      "v\n",
      "i\n",
      "o\n",
      "u\n",
      "s\n",
      " \n",
      "e\n",
      "x\n",
      "e\n",
      "r\n",
      "c\n",
      "i\n",
      "s\n",
      "e\n",
      "s\n",
      ",\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "s\n",
      "e\n",
      "e\n",
      "m\n",
      "s\n",
      " \n",
      "c\n",
      "o\n",
      "m\n",
      "i\n",
      "c\n",
      "a\n",
      "l\n",
      "l\n",
      "y\n",
      " \n",
      "e\n",
      "a\n",
      "s\n",
      "y\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "string = \"Compared to some of the previous exercises, this seems comically easy.\"\n",
    "\n",
    "for s in string:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 13.\n",
    "\n",
    "☼ What is the difference between calling `split` on a string with no argument or with `' '` as the argument, e.g. `sent.split()` versus `sent.split(' ')`? What happens when the string being split contains tab characters, consecutive space characters, or a sequence of tabs and spaces? \n",
    "\n",
    "*`sent.split()` splits all whitespace identically.*\n",
    "\n",
    "*`sent.split(' ')` splits all whitespace literally.  I.e., tabs will be represented as `\\t`, and each individaul whitespace will be spilt into its own string.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With `sent.split()`:\n",
      "['This', 'string', 'is', 'a', 'pretty', 'simple', 'string.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This', 'string', 'is', 'a', 'pretty', 'simple', 'string.']\n",
      "\n",
      "With `sent.split()`:\n",
      "['This', 'strings', 'has', 'tabs.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This\\tstrings\\thas\\ttabs.']\n",
      "\n",
      "With `sent.split()`:\n",
      "['This', 'string', 'has', 'lots', 'of', 'space.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This', '', '', '', '', '', '', '', 'string', '', '', '', '', '', '', '', '', '', 'has', '', '', '', '', '', 'lots', '', '', '', 'of', '', '', '', '', 'space.']\n",
      "\n",
      "With `sent.split()`:\n",
      "['This', 'string', 'has', 'tabs', 'and', 'spaces.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This\\tstring', '', '', '', '', '', '', '', '', 'has\\ttabs', '', '', '', '', '', '', 'and\\tspaces.']\n"
     ]
    }
   ],
   "source": [
    "s1 = \"This string is a pretty simple string.\"\n",
    "s2 = \"This\\tstrings\\thas\\ttabs.\"\n",
    "s3 = \"This        string          has      lots    of     space.\"\n",
    "s4 = \"This\\tstring         has\\ttabs       and\\tspaces.\"\n",
    "\n",
    "Ss = [s1, s2, s3, s4]\n",
    "\n",
    "for s in Ss:\n",
    "    print(\"\\nWith `sent.split()`:\")\n",
    "    print(s.split())\n",
    "    print(\"\\nWith `sent.split(' ')`:\")\n",
    "    print(s.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. \n",
    "\n",
    "☼ Create a variable `words` containing a list of words. Experiment with `words.sort()` and `sorted(words)`. What is the difference?\n",
    "\n",
    "*`words.sort()` doesn't return a value, but it alters the ordering of the list, so that whenever I call the list again, the returned list will be the ordered one, and not the one I originally stored.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wörter', 'focail', 'mots', 'ord', 'ord', 'palabras', 'palavras', 'parole', 'sanat', 'slova', 'szavak', 'słowa', 'từ ngữ', 'woorden', 'words', 'words', 'λόγια', 'ווערטער']\n"
     ]
    }
   ],
   "source": [
    "words = [\"slova\", \"ord\", \"Wörter\", \"λόγια\", \"words\", \"palabras\", \"sanat\", \n",
    "         \"mots\", \"focail\", \"szavak\", \"parole\", \"words\", \"woorden\", \"ord\", \n",
    "         \"słowa\", \"palavras\", \"từ ngữ\", \"ווערטער\"]\n",
    "\n",
    "words.sort()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wörter', 'focail', 'mots', 'ord', 'ord', 'palabras', 'palavras', 'parole', 'sanat', 'slova', 'szavak', 'słowa', 'từ ngữ', 'woorden', 'words', 'words', 'λόγια', 'ווערטער']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`sorted(words)` will return a result, but the ordering of the original list will not be changed:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wörter', 'focail', 'mots', 'ord', 'ord', 'palabras', 'palavras', 'parole', 'sanat', 'slova', 'szavak', 'słowa', 'từ ngữ', 'woorden', 'words', 'words', 'λόγια', 'ווערטער']\n"
     ]
    }
   ],
   "source": [
    "words = [\"slova\", \"ord\", \"Wörter\", \"λόγια\", \"words\", \"palabras\", \"sanat\", \n",
    "         \"mots\", \"focail\", \"szavak\", \"parole\", \"words\", \"woorden\", \"ord\", \n",
    "         \"słowa\", \"palavras\", \"từ ngữ\", \"ווערטער\"]\n",
    "\n",
    "print(sorted(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['slova', 'ord', 'Wörter', 'λόγια', 'words', 'palabras', 'sanat', 'mots', 'focail', 'szavak', 'parole', 'words', 'woorden', 'ord', 'słowa', 'palavras', 'từ ngữ', 'ווערטער']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. \n",
    "\n",
    "☼ Explore the difference between strings and integers by typing the following at a Python prompt: `\"3\" * 7` and `3 * 7`. Try converting between strings and integers using `int(\"3\")` and `str(3)`.\n",
    "\n",
    "*Multiplying a string $x$ by an integer $y$ will just cause $x$ to be printed to the console $y$ times:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3333333'"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"3\" * 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>`3 * 7` will just give us the product:</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 * 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can convert strings to integers with `int()` and vice-versa with `str()`:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(\"3\") * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3333333'"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(3) * 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16. \n",
    "\n",
    "☼ Use a text editor to create a file called `prog.py` containing the single line `monty = 'Monty Python'`. Next, start up a new session with the Python interpreter, and enter the expression `monty` at the prompt. You will get an error from the interpreter. Now, try the following (note that you have to leave off the `.py` part of the filename):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Errors in my notebook prevent the remaining cells from running, so this is being saved as markdown:*\n",
    "\n",
    "```\n",
    "monty\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "NameError                                 Traceback (most recent call last)\n",
    "<ipython-input-308-d4cc90107335> in <module>\n",
    "----> 1 monty\n",
    "\n",
    "NameError: name 'monty' is not defined\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python'"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prog import monty\n",
    "monty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This time, Python should return with a value. You can also try `import prog`, in which case Python should be able to evaluate the expression `prog.monty` at the prompt.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python'"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import prog\n",
    "\n",
    "prog.monty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17. \n",
    "\n",
    "☼ What happens when the formatting strings `%6s` and `%-6s` are used to display strings that are longer than six characters?\n",
    "\n",
    "*This looks to be a legacy question from an older version of the book, since this is the older method of formatting in Python.  As the question is written, `%6s` won't do anything to a longer string:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'another test'"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"another test\"\n",
    "\n",
    "\"%6s\" % (test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`%-6s` will add padding to a string shorter than six characters:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey   '"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"%-6s\" % (\"hey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I suspect the authors may have left out a decimal.  `%.6s` has a quite different effect:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anothe'"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"%.6s\" % (test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. \n",
    "\n",
    "◑ Read in some text from a corpus, tokenize it, and print the list of all *wh*-word types that occur. (*wh*-words in English are used in questions, relative clauses and exclamations: *who*, *which*, *what*, and so on.) Print them in order. Are any words duplicated in this list, because of the presence of case distinctions or punctuation?\n",
    "\n",
    "*This question is a little difficult to follow.  Most of the corpora we're using have texts that have already been tokenized, so the first part of this question seems a bit redundant.  However, just to play along,  I'll use the raw text version of one of the Project Gutenberg texts.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "raw = gutenberg.raw('bryant-stories.txt')\n",
    "\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "tokens = sorted(set(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The next part of the question also seems a little hard to follow: The __wh-__words are a closed set (__what__, __when__, __where__, __which__, __who__, __whose__, and __why__), and if I explicitly define them, I won't find any exceptions.  The only possible way around this is to search for all words that begin with __wh__.  But if I do so, most of the hits will be false positives, because most words that start with __wh-__ are outside of this set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Whale', 'What', 'When', 'Whenever', 'Where', 'Whether', 'Whiff', 'While', 'Whirling', 'White', 'Who', 'Whose', 'Why', 'what', 'whatever', 'wheat', 'wheelbarrow', 'wheeled', 'when', 'whence', 'whenever', 'where', 'wherein', 'wherever', 'whether', 'which', 'while', 'whimpering', 'whin', 'whinny', 'whipped', 'whirlpool', 'whiruled', 'whisk', 'whisked', 'whisper', 'whisper_', 'whispered', 'whispering', 'whispers', 'whistle', 'whistled', 'white', 'white-haired', 'white-robed', 'whither', 'who', 'whole', 'wholly', 'whom', 'whose', 'why']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in tokens if re.search('^[Ww]h', w)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As expected, most of the results are false positives.  There are also  versions of the __wh-__ words starting with both upper- and lowercase letters; as well as 'whom', the accusative form of 'who'.  But there are also a number of words that could be considered __wh-__ words that I wouldn't have thought of searching for, such as words ending with '-ever' (e.g., 'whatever', 'whenever', etc...); variant forms of 'where' (i.e., 'whence' and 'whither'), as well as 'whether' and 'wherein'.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. \n",
    "\n",
    "◑ Create a file consisting of words and (made up) frequencies, where each line consists of a word, the space character, and a positive integer, e.g. `fuzzy 53`. Read the file into a Python list using `open(filename).readlines()`. Next, break each line into its two fields using `split()`, and convert the number into an integer using `int()`. The result should be a list of the form: `[['fuzzy', 53], ...]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy = open('fuzzy.txt', encoding = \"utf-8\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fuzzy', 53],\n",
       " ['wuzzy', 92],\n",
       " ['was', 128],\n",
       " ['a', 4897],\n",
       " ['bear', 23],\n",
       " ['had', 47],\n",
       " ['no', 93],\n",
       " ['hair', 23],\n",
       " [\"wasn't\", 78],\n",
       " ['he', 77]]"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[word, int(value)] for word, value in (f.split() for f in fuzzy)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20. \n",
    "\n",
    "◑ Write code to access a favorite webpage and extract some text from it. For example, access a weather site and extract the forecast top temperature for your town or city today.\n",
    "\n",
    "*This is not so straightforward.  Maybe need to use API.  Registered with openweathermap, try again later.*\n",
    "\n",
    "Cf. this discussion: https://stackoverflow.com/questions/1474489/python-weather-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cod': 401,\n",
      " 'message': 'Invalid API key. Please see '\n",
      "            'http://openweathermap.org/faq#error401 for more info.'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import requests\n",
    "\n",
    "r = requests.get('http://api.openweathermap.org/data/2.5/weather?q=London&APPID=10f1049e5862ee388ad748981accb4c1')\n",
    "pprint(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21.\n",
    "\n",
    "◑ Write a function `unknown()` that takes a URL as its argument, and returns a list of unknown words that occur on that webpage. In order to do this, extract all substrings consisting of lowercase letters (using `re.findall()`) and remove any items from this set that occur in the Words Corpus (`nltk.corpus.words`). Try to categorize these words manually and discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = [w.lower() for w in nltk.corpus.words.words('en')]\n",
    "\n",
    "def unknown(url):\n",
    "    \n",
    "    # get text\n",
    "    raw = return_URL_contents(url)\n",
    "    \n",
    "    # get lower-case words\n",
    "    raw_lower = re.findall(r'\\b[a-z]+\\b', raw)\n",
    "    \n",
    "    # find unknown words and eliminate duplicates\n",
    "    return sorted(set([w for w in raw_lower if w not in wordlist]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['agencies', 'answers', 'app', 'attacking', 'attempts', 'automation', 'banned', 'became', 'biohacker', 'boils', 'bringing', 'businesses', 'called', 'candidates', 'changing', 'companies', 'compares', 'comparing', 'competitors', 'couldn', 'criticized', 'debates', 'details', 'diagnosing', 'didn', 'discussed', 'doesn', 'doors', 'employees', 'enemies', 'enjoyed', 'executives', 'explodes', 'explores', 'frontrunner', 'fundraising', 'giants', 'harshest', 'has', 'hasn', 'helped', 'hosted', 'ignored', 'ignoring', 'including', 'indicated', 'infused', 'issued', 'laws', 'lists', 'lives', 'mailing', 'mainstream', 'members', 'millennials', 'minutes', 'missed', 'monopolies', 'offered', 'okay', 'options', 'playing', 'podcast', 'politicians', 'practices', 'pressed', 'proposals', 'pushed', 'represents', 'required', 'rights', 'rules', 'sectors', 'sees', 'sharing', 'shifted', 'showcased', 'signing', 'simmering', 'specifics', 'stories', 'techlash', 'teddyschleifer', 'themes', 'thinks', 'topics', 'users', 'using', 'verboten', 'voters', 'wants', 'whacks', 'years']\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.vox.com/recode/2019/10/16/20916712/cnn-democratic-presidential-debate-big-tech-silicon-valley-warren-harris'\n",
    "\n",
    "print(unknown(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Many of the results seem to be common words that haven't been stemmed.  I.e., nouns with a plural \"-s\" or verbs with \"-ing\" or \"-ed\" endings.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TO DO: modifying `unknown` so that it can optionally get rid of words with common endings.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
